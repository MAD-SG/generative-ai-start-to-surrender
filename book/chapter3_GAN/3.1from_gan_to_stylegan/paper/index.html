
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.49">
    
    
      
        <title>From GAN to PGGAN: Some Base GAN model Introduction - Generative AI: From Start to Surrender</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.6f8fc17f.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#from-gan-to-pggan-some-base-gan-model-introduction" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="Generative AI: From Start to Surrender" class="md-header__button md-logo" aria-label="Generative AI: From Start to Surrender" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Generative AI: From Start to Surrender
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              From GAN to PGGAN: Some Base GAN model Introduction
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../chapter1_Introduction/1.1terminology/" class="md-tabs__link">
          
  
  Introduction

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../chapter2_VAE/2.1introduction/" class="md-tabs__link">
          
  
  VAE

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../3.2pggan/paper/" class="md-tabs__link">
          
  
  GANs

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../chapter4_diffusion/4.1introduction/" class="md-tabs__link">
          
  
  Diffusion

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../chapter5_generation_theory/5.1MLE/" class="md-tabs__link">
          
  
  Generation Theory

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="Generative AI: From Start to Surrender" class="md-nav__button md-logo" aria-label="Generative AI: From Start to Surrender" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Generative AI: From Start to Surrender
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Introduction
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../chapter1_Introduction/1.1terminology/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Terms
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../chapter1_Introduction/1.2fourier_transform/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fourier Transform
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../chapter1_Introduction/1.3signal_processing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Signal Processing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../chapter1_Introduction/1.4statistics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Statistics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../chapter1_Introduction/1.5SDE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SDE
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    VAE
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            VAE
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../chapter2_VAE/2.1introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VAE Introduction
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    GANs
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            GANs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../3.2pggan/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PGGAN
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../3.3stylegan/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StyleGAN
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../3.4stylegan2/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StyleGAN2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../3.5stylegan3/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StyleGAN3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../3.6styleganT/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StyleGAN T
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../3.7R3Gan/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    R3Gan
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Diffusion
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Diffusion
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../chapter4_diffusion/4.1introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../chapter4_diffusion/4.2DDPM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DDPM
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Generation Theory
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Generation Theory
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../chapter5_generation_theory/5.1MLE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Maximal Likelihood
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="from-gan-to-pggan-some-base-gan-model-introduction">From GAN to PGGAN: Some Base GAN model Introduction<a class="headerlink" href="#from-gan-to-pggan-some-base-gan-model-introduction" title="Permanent link">&para;</a></h1>
<h1 id="2014gan-generative-adversarial-networks">[2014]GAN : <a href="https://arxiv.org/abs/1406.2661">Generative Adversarial Networks</a><a class="headerlink" href="#2014gan-generative-adversarial-networks" title="Permanent link">&para;</a></h1>
<p>GANs (Generative Adversarial Networks) are based on an adversarial process framework. In this framework, there are two networks that work in opposition: a generator and a discriminator. This can be compared to a scenario where one team (generator) tries to produce counterfeit items without being detected, while the other team (discriminator) acts like law enforcement trying to detect the fakes.
Overall Introduction</p>
<ul>
<li>
<p>Base model structure
  <img alt="image" src="https://hackmd.io/_uploads/BkDsNVoDyx.png" />
<em>Image Source: <a href="https://semiengineering.com/knowledge_centers/artificial-intelligence/neural-networks/generative-adversarial-network-gan/">Semi Engineering - GAN Knowledge Center</a></em></p>
</li>
<li>
<p>The model works through two independent learning models: a Generative Model and a Discriminative Model, which learn through adversarial training to produce high-quality outputs.</p>
<ul>
<li>Generative Model,short as <span class="arithmatex">\(G\)</span>：</li>
<li>Takes random noise <span class="arithmatex">\(z\)</span> as input -&gt; Generates images <span class="arithmatex">\(G(z)\)</span></li>
<li>Aims to create images that look real enough to fool <span class="arithmatex">\(D\)</span></li>
<li>Discriminative Model, short as <span class="arithmatex">\(D\)</span>,is a binary classifier:</li>
<li>Takes an image x as input -&gt; Output <span class="arithmatex">\(D(x)\)</span>, representing the probability that <span class="arithmatex">\(x\)</span> is a real image<ul>
<li><span class="arithmatex">\(D(x)=1\)</span> means 100% confidence it's real</li>
<li><span class="arithmatex">\(D(x)=0\)</span> means it's definitely fake</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Adversarial training: The optimization process is a minimax game, with the goal of reaching Nash equilibrium</p>
</li>
<li>
<p>The generator tries to minimize the probability of the discriminator detecting fake samples
    The discriminator tries to maximize its ability to distinguish between real and fake samples</p>
</li>
</ul>
<h2 id="loss-function"><strong>Loss function</strong>:<a class="headerlink" href="#loss-function" title="Permanent link">&para;</a></h2>
<p><span class="arithmatex">\(<span class="arithmatex">\(V(D,G) = \underset{D}{\max} {\underbrace{\mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1-D(G(z)))]}_{\text{Discriminator Loss}:L_D}} +\underset{G}{\min} {\underbrace{\mathbb{E}_{z \sim p_z(z)}[\log(1-D(G(z)))]}_{\text{Generator Loss: }L_G}}\)</span>\)</span></p>
<p>Where:
  - <span class="arithmatex">\(p_{data}(x)\)</span> is the real data distribution
  - <span class="arithmatex">\(p_z(z)\)</span> is the noise distribution
  - <span class="arithmatex">\(G(z)\)</span> is generator mapping from noise to synthetic data
  - <span class="arithmatex">\(D(x)\)</span> is the discriminator's estimate of the probability that <span class="arithmatex">\(x\)</span> is real</p>
<p>overall training process:
  <img alt="image" src="https://hackmd.io/_uploads/S1ROHEiwke.png" />
<em>Image Source: Goodfellow et al., "Generative Adversarial Networks" (2014) arXiv:1406.2661</em></p>
<ul>
<li>
<p>In GAN training, we iterate the Discriminator (<span class="arithmatex">\(D\)</span>) <span class="arithmatex">\(K\)</span> times before updating the Generator (<span class="arithmatex">\(G\)</span>) once：</p>
</li>
<li>
<p>because we need <span class="arithmatex">\(D\)</span> to be powerful enough to provide accurate feedback for <span class="arithmatex">\(G's\)</span> improvement</p>
</li>
<li>This iterative strategy helps maintain training stability and prevent mode collapse, although K needs to be carefully balanced <ul>
<li>too large and G can't learn effectively, too small and D's feedback becomes unreliable.</li>
</ul>
</li>
</ul>
<h2 id="base-model-structure">Base model structure<a class="headerlink" href="#base-model-structure" title="Permanent link">&para;</a></h2>
<p>Code reference: https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py</p>
<pre><code class="language-Python">class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()

        def block(in_feat, out_feat, normalize=True):
            layers = [nn.Linear(in_feat, out_feat)]
            if normalize:
                layers.append(nn.BatchNorm1d(out_feat, 0.8))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers

        self.model = nn.Sequential(
            *block(opt.latent_dim, 128, normalize=False),
            *block(128, 256),
            *block(256, 512),
            *block(512, 1024),
            nn.Linear(1024, int(np.prod(img_shape))),
            nn.Tanh()
        )
    def forward(self, z):
        img = self.model(z)
        img = img.view(img.size(0), *img_shape)
        return img
</code></pre>
<pre><code class="language-Python">class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(int(np.prod(img_shape)), 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 1),
            nn.Sigmoid(),
        )
    def forward(self, img):
        img_flat = img.view(img.size(0), -1)
        validity = self.model(img_flat)
        return validity
</code></pre>
<p><strong>Training process and images generated:</strong></p>
<p><img alt="image" src="https://hackmd.io/_uploads/HJX7I4jDyl.png" />
<em>Image Source: Goodfellow et al., "Generative Adversarial Networks" (2014) arXiv:1406.2661</em></p>
<ul>
<li>Real data distribution <span class="arithmatex">\(p_{data}\)</span>(Black dotted line)</li>
<li>Generator distribution <span class="arithmatex">\(p_g\)</span>(Green solid line )</li>
<li>Discriminator output <span class="arithmatex">\(D\)</span>(Blue dashed line)</li>
<li>Noise space where <span class="arithmatex">\(z\)</span> is sampled (Lower horizontal line)</li>
<li>Data space <span class="arithmatex">\(x\)</span>(Upper horizontal line)</li>
<li>Generator G's mapping from  <span class="arithmatex">\(z\)</span> to  <span class="arithmatex">\(x\)</span> (Arrows connecting lines)</li>
</ul>
<p>From the picture, we can see the training Evolution (from a to d):
- Initial Stage:
  - The generated distribution  <span class="arithmatex">\(p_g\)</span> (green) differs significantly from the real distribution <span class="arithmatex">\(p_{data}\)</span> (black)
  - Discriminator <span class="arithmatex">\(D\)</span>  (blue) attempts to distinguish samples, but performs unstably
- Discriminator Training: <span class="arithmatex">\(D\)</span> is trained to reach optimal solution: <span class="arithmatex">\(<span class="arithmatex">\(D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}\)</span>\)</span>
- Generator Update:  <span class="arithmatex">\(G\)</span> updates based on gradients from <span class="arithmatex">\(D\)</span>
- Final Convergence: When <span class="arithmatex">\(G\)</span> and <span class="arithmatex">\(D\)</span> have sufficient capacity（ <span class="arithmatex">\(p_{data} =p_g\)</span>), they reach Nash equilibrium</p>
<p><img alt="image" src="https://hackmd.io/_uploads/HJn4UVswye.png" />
<em>Image Source: Goodfellow et al., "Generative Adversarial Networks" (2014) arXiv:1406.2661</em></p>
<h1 id="2014cgan-conditional-generative-adversarial-nets">[2014]<a href="https://arxiv.org/abs/1411.1784">cGAN: Conditional Generative Adversarial Nets</a><a class="headerlink" href="#2014cgan-conditional-generative-adversarial-nets" title="Permanent link">&para;</a></h1>
<h2 id="overall-introduction">Overall Introduction:<a class="headerlink" href="#overall-introduction" title="Permanent link">&para;</a></h2>
<p>conditional generation
Traditional GANs produce samples from random noise but <strong>can't control the output features</strong>, as they are <strong>unsupervised learning</strong>.</p>
<p>While conditional GANs (cGANs) <strong>incorporate conditional information into both the generator and discriminator, enabling control over the output properties</strong>. This is achieved through a semi-supervised approach.</p>
<p>The cGAN paper only shows its generated results on the MNIST dataset, where simply concatenating label embeddings might have limited impact. However, the core idea of "guiding the generation process with conditional information" proposed by cGAN has significantly influenced subsequent generative models. </p>
<ul>
<li>For example, models like DALL-E and Stable Diffusion, although utilizing different architectures like Diffusion, have adopted the principle of conditional generation: they use text embeddings as conditional information to control image generation.</li>
</ul>
<h2 id="base-model-structure_1">Base model structure<a class="headerlink" href="#base-model-structure_1" title="Permanent link">&para;</a></h2>
<p><img alt="image" src="https://hackmd.io/_uploads/Hy6cONjwkx.png" />
<em>Source: Mirza et al., "Conditional Generative Adversarial Nets" (2014) arXiv:1411.1784</em></p>
<p>How to combine the condition into input:
- First convert categorical labels into continuous vector representations using nn.Embedding
  - <code>self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)</code>
- Then concatenates torch.cat label embeddings(y) with noise vectors(z) along dimension -1: 
  - <code>gen_input = torch.cat((self.label_emb(labels), noise), -1)</code>
- Uses this concatenated vector as input to generate images through multiple network layers</p>
<pre><code class="language-python">class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)
        def block(in_feat, out_feat, normalize=True):
            layers = [nn.Linear(in_feat, out_feat)]
            if normalize:
                layers.append(nn.BatchNorm1d(out_feat, 0.8))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers

        self.model = nn.Sequential(
            *block(opt.latent_dim + opt.n_classes, 128, normalize=False),
            *block(128, 256),
            *block(256, 512),
            *block(512, 1024),
            nn.Linear(1024, int(np.prod(img_shape))),
            nn.Tanh()
        )

    def forward(self, noise, labels):
        # Concatenate label embedding and image to produce input
        gen_input = torch.cat((self.label_emb(labels), noise), -1)
        img = self.model(gen_input)
        img = img.view(img.size(0), *img_shape)
        return img
</code></pre>
<p>IN Discriminator:
- Flattens input images: 
    - <code>img.view(img.size(0), -1)</code>
- Similarly, processes labels through embedding: <code>self.label_embedding(labels)</code>
- Concatenates flattened images and label embeddings:
    - <code>d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)</code>
- Passes concatenated vector through discriminator network for real/fake classification</p>
<pre><code class="language-python">class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()

        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)

        self.model = nn.Sequential(
            nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 512),
            nn.Dropout(0.4),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 512),
            nn.Dropout(0.4),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 1),
        )

    def forward(self, img, labels):
        # Concatenate label embedding and image to produce input
        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)
        validity = self.model(d_in)
        return validity
</code></pre>
<p>Actually, it's still a Binary Classification Task. The output of cGAN discriminator still maintains GAN's binary output (real/fake). </p>
<ul>
<li>Doesn't explicitly verify condition-image matching</li>
<li>Output is a single scalar through Sigmoid/BCELoss or MSELoss</li>
</ul>
<p>For example, an input condition: number "7". If the generator generates an image that looks like "3". Although the discriminator will not directly point out "this is not 7", because the label of "7" in the training data has never been paired with the image of "3". So this wrong match will be identified as "generated" by the discriminator.</p>
<h2 id="loss-function_1">Loss function<a class="headerlink" href="#loss-function_1" title="Permanent link">&para;</a></h2>
<div class="arithmatex">\[\min_{G} \max_{D} V(D,G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x|y)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z|y)))]\]</div>
<p>Where: <span class="arithmatex">\(\mathbb{E}\)</span>: Expected value (expectation) <span class="arithmatex">\(\mathbb{E}_{x \sim p_{data}(x)}[\log D(x|y)]\)</span> : 
- <span class="arithmatex">\(x \sim p_{data(x)}\)</span>:  <span class="arithmatex">\(x\)</span>sampled from real data distribution
- <span class="arithmatex">\(D(x|y)\)</span>: Discriminator's output for real data  <span class="arithmatex">\(x\)</span> given condition  <span class="arithmatex">\(y\)</span>
- <span class="arithmatex">\(E[\log D(x|y)]\)</span> - Discriminator's ability to identify real samples</p>
<p><span class="arithmatex">\(\mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z|y)))]\)</span> : 
- <span class="arithmatex">\(z \sim p_z(z)\)</span> z sampled from noise distribution
- <span class="arithmatex">\(G(z|y)\)</span>: Generator's output from noise z given condition y
- <span class="arithmatex">\(E[log(1 - D(G(z|y)))]\)</span> - Discriminator's ability to identify fake samples</p>
<pre><code class="language-python">for epoch in range(opt.n_epochs):
    for i, (imgs, labels) in enumerate(dataloader):
        batch_size = imgs.shape[0]
        # Adversarial ground truths
        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)
        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)
        # Configure input
        real_imgs = Variable(imgs.type(FloatTensor))
        labels = Variable(labels.type(LongTensor))
        # -----------------
        #  Train Generator
        # -----------------
        optimizer_G.zero_grad()
        # Sample noise and labels as generator input
        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))
        gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size)))
        # Generate a batch of images
        gen_imgs = generator(z, gen_labels)
        # Loss measures generator's ability to fool the discriminator
        validity = discriminator(gen_imgs, gen_labels)
        g_loss = adversarial_loss(validity, valid)
        g_loss.backward()
        optimizer_G.step()
        # ---------------------
        #  Train Discriminator
        # ---------------------
        optimizer_D.zero_grad()
        # Loss for real images
        validity_real = discriminator(real_imgs, labels)
        d_real_loss = adversarial_loss(validity_real, valid)
        # Loss for fake images
        validity_fake = discriminator(gen_imgs.detach(), gen_labels)
        d_fake_loss = adversarial_loss(validity_fake, fake)
        # Total discriminator loss
        d_loss = (d_real_loss + d_fake_loss) / 2
        d_loss.backward()
        optimizer_D.step()

</code></pre>
<p><img alt="image" src="https://hackmd.io/_uploads/SJkkKEjvkl.png" />
<em>Source: Mirza et al., "Conditional Generative Adversarial Nets" (2014) arXiv:1411.1784</em></p>
<hr />
<h1 id="2015dcgandeep-convolutional-ganunsupervised-representation-learning-with-deep-convolutional-generative-adversarial-networks">[2015]DCGAN(Deep Convolutional GAN）：<a href="https://arxiv.org/abs/1511.06434">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a><a class="headerlink" href="#2015dcgandeep-convolutional-ganunsupervised-representation-learning-with-deep-convolutional-generative-adversarial-networks" title="Permanent link">&para;</a></h1>
<p>Overall Introduction:  Deep Convolutional GAN
DCGAN integrates the strengths of Convolutional Neural Networks into GANs with key innovations:
1. Convolutional Layers: Transposed convolutions in the generator and strided convolutions in the discriminator enhance spatial information retention.
2. Batch Normalization: Used extensively in both parts to improve stability and prevent mode collapse.
3. Activation Functions: The generator uses ReLU with a Tanh final layer, and the discriminator employs LeakyReLU.</p>
<ul>
<li>Starts from 100-dimensional noise z, gradually generating 64×64 images through multiple convolution layers</li>
<li>Feature map progression(C<em>H</em>W): 
  <span class="arithmatex">\(100\times1\times1\)</span>-&gt;<span class="arithmatex">\(1024\times4\times4\)</span> -&gt; <span class="arithmatex">\(512\times8\times8\)</span>-&gt;<span class="arithmatex">\(256\times16\times16\)</span>-&gt;<span class="arithmatex">\(128\times32\times32\)</span>-&gt;<span class="arithmatex">\(3\times64\times64\)</span></li>
</ul>
<h2 id="base-model-structure_2">Base model structure<a class="headerlink" href="#base-model-structure_2" title="Permanent link">&para;</a></h2>
<p><img alt="image" src="https://hackmd.io/_uploads/SytbYEsPkl.png" />
<em>Source: Radford et al., "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks" (2016) arXiv:1511.06434</em></p>
<p>About Transposed convolutions:
- Input Matrix Expansion: Initially, the input feature map undergoes an expansion by inserting zeros between each element. The number of zeros inserted depends on the stride parameter. 
- Application of the Convolution Kernel: Next, the convolution kernel is applied to the expanded feature map. This process is similar to traditional convolution operations, where the kernel slides over the expanded feature map, computing the dot product with local regions. Unlike regular convolution, this operation results in a larger output feature map because the input has been expanded.
- Adjustment of Output Size: Finally, the output feature map might be cropped or padded to adjust its dimensions to the desired size. This adjustment depends on the padding parameter, which can either reduce or increase the spatial dimensions of the output</p>
<h2 id="loss-function_2">Loss function<a class="headerlink" href="#loss-function_2" title="Permanent link">&para;</a></h2>
<p>Same as GAN objective : <span class="arithmatex">\(\min_G \max_D V(D,G) = \mathbb{E}_{x\sim p_{data}}[\log D(x)] + \mathbb{E}_{z\sim p_z}[\log(1-D(G(z)))]\)</span></p>
<hr />
<h1 id="2017-wgan-wasserstein-gan">[2017] WGAN:<a href="https://arxiv.org/abs/1701.07875"> Wasserstein GAN</a><a class="headerlink" href="#2017-wgan-wasserstein-gan" title="Permanent link">&para;</a></h1>
<h2 id="overall-introduction_1">Overall Introduction:<a class="headerlink" href="#overall-introduction_1" title="Permanent link">&para;</a></h2>
<p>WGAN introduces Wasserstein distance and Lipschitz constraint  in loss function to "improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches". WGAN replaces the JSD with the Wasserstein distance to measure the distribution distance in the original GAN.</p>
<ul>
<li>
<p>If the discriminator is trained too well, the generator's gradients vanish, and the generator's loss cannot decrease </p>
</li>
<li>
<p>If the discriminator is not trained well enough, the generator's gradients become inaccurate, causing it to move erratically. </p>
</li>
</ul>
<p>The discriminator needs to be trained to just the right degree - neither too well nor too poorly - but this balance is very difficult to achieve. Moreover, this optimal balance might even vary at different stages within the same training epoch, which is why GANs are so difficult to train.</p>
<h2 id="drawbacks-as-jsd">Drawbacks as JSD：<a class="headerlink" href="#drawbacks-as-jsd" title="Permanent link">&para;</a></h2>
<h3 id="gradient-vanishing-problem-training-instability">Gradient Vanishing Problem &amp; training instability<a class="headerlink" href="#gradient-vanishing-problem-training-instability" title="Permanent link">&para;</a></h3>
<p>We have introduced above, under an (approximately) optimal discriminator, minimizing the generator's loss is equivalent to minimizing the JS divergence between <span class="arithmatex">\(P_r\)</span> and <span class="arithmatex">\(P_g\)</span>. Since <span class="arithmatex">\(P_r\)</span> and <span class="arithmatex">\(P_g\)</span> almost inevitably have negligible overlap, their JS divergence will always be the constant <span class="arithmatex">\(\log 2\)</span>, regardless of how far apart they are. This ultimately leads to the generator's gradient (approximately) becoming 0, resulting in gradient vanishing.
<img alt="image" src="https://hackmd.io/_uploads/SyN95VovJl.png" />
<em>Source: Arjovsky et al., "Wasserstein GAN" (2017) arXiv:1701.07875</em></p>
<h3 id="model-collapse">Model collapse<a class="headerlink" href="#model-collapse" title="Permanent link">&para;</a></h3>
<p>Secondly, even the previously mentioned standard KL divergence term has flaws. Because KL divergence is not a symmetric measure, <span class="arithmatex">\(KL(P_g\|P_r)\)</span> and <span class="arithmatex">\(KL(P_r\|P_g)\)</span> are different. Taking the former as an example:
<span class="arithmatex">\(KL(P_g||P_r) = \int_x P_g(x)\log(\frac{P_g(x)}{P_r(x)})dx\)</span></p>
<ul>
<li>When <span class="arithmatex">\(P_g(x) \to 0\)</span> and <span class="arithmatex">\(P_{r}(x) \to 1\)</span>, <span class="arithmatex">\(P_g(x)\log\frac{P_g(x)}{P_{r}(x)} \to 0\)</span>, contributing nearly 0 to <span class="arithmatex">\(KL(P_g||P_r)\)</span></li>
<li>When <span class="arithmatex">\(P_g(x) \to 1\)</span> and <span class="arithmatex">\(P_{r}(x) \to 0\)</span>, <span class="arithmatex">\(P_g(x)\log\frac{P_g(x)}{P_{r}(x)} \to +\infty\)</span>, contributing positively infinite to <span class="arithmatex">\(KL(P_g||P_r)\)</span></li>
<li>In other words, <span class="arithmatex">\(KL(P_g||P_r)\)</span> penalizes these two types of errors differently. </li>
<li>The first type of error corresponds to <strong>"generator failing to generate real samples"</strong> with small penalty.</li>
<li>The second type corresponds to <strong>"generator generating unrealistic samples"</strong> with large penalty. </li>
</ul>
<p>The first type of error represents a lack of diversity, while the second type represents a lack of accuracy. As a result, the generator would rather generate some repetitive but "safe" samples, and is reluctant to generate diverse samples, because one small mistake could lead to the second type of error, resulting in an unacceptable loss. This phenomenon is commonly referred to as mode collapse.</p>
<p><img alt="image" src="https://hackmd.io/_uploads/SJAaYViwJg.png" />
<em>Source: Arjovsky et al., "Wasserstein GAN" (2017) arXiv:1701.07875</em></p>
<p>Why Wasserstein distance?
The superiority of the Wasserstein distance compared to KL divergence and JS divergence lies in its ability to reflect the proximity between two distributions even when they don't overlap. While KL divergence and JS divergence are discontinuous , being either maximum or minimum. The Wasserstein distance is smooth and offers a more natural way to measure distances between distributions.
1. Training Stability: Provides meaningful gradients even when distributions do not overlap, significantly improving the stability of GAN training.
2. Reduced Mode Collapse: Encourages diversity in generated samples by considering the overall differences between distributions, reducing mode collapse.
3. Intuitive Loss Function: Serves as a loss metric, where a smaller Wasserstein distance indicates closer alignment with the target distribution's statistical properties.
4. Effective GAN Training: WGANs use Wasserstein distance to offer a more stable and effective training process, enhancing the quality and diversity of generated samples.</p>
<h2 id="loss-function_3">Loss function<a class="headerlink" href="#loss-function_3" title="Permanent link">&para;</a></h2>
<ol>
<li>About Wasserstein Distance (measure distance by estimating the difference between expectation):</li>
<li>Mathematical Definition ：<span class="arithmatex">\(W(P,Q) = \inf_{\gamma \in \Pi(P,Q)} \mathbb{E}_{(x,y)\sim \gamma}[||x-y||]\)</span><ul>
<li><span class="arithmatex">\(\Pi(P_r, P_g)\)</span> is the set of all possible joint distributions whose marginal distributions are <span class="arithmatex">\(P_r\)</span> and <span class="arithmatex">\(P_g\)</span>. </li>
<li>In other words, for each distribution in <span class="arithmatex">\(\Pi(P_r, P_g)\)</span>, its marginal distributions are <span class="arithmatex">\(P_r\)</span> and <span class="arithmatex">\(P_g\)</span>. </li>
<li>For each possible joint distribution <span class="arithmatex">\(\gamma\)</span>,sample <span class="arithmatex">\((x,y) \sim \gamma\)</span> to get a real sample <span class="arithmatex">\(x\)</span> and a generated sample <span class="arithmatex">\(y\)</span>, and calculate the distance between these samples <span class="arithmatex">\(\|x-y\|\)</span>. </li>
<li><span class="arithmatex">\(\mathbb{E}_{(x,y)\sim\gamma}[\|x-y\|]\)</span>calculates the expected value of the sample distance under this joint distribution. The infimum of this expected value among all possible joint distributions <span class="arithmatex">\(\inf_{\gamma\sim\Pi(P_r,P_g)} \mathbb{E}_{(x,y)\sim\gamma}[\|x-y\|]\)</span> is defined as the Wasserstein distance.</li>
<li>Intuitively, <span class="arithmatex">\(\mathbb{E}_{(x,y)\sim\gamma}[\|x-y\|]\)</span> can be understood as the "cost" of moving "<span class="arithmatex">\(P_r\)</span> pile of earth" to " <span class="arithmatex">\(P_g\)</span> location" under this "transport plan", and <span class="arithmatex">\(W(P_r, P_g)\)</span> is the "minimum cost" under the "optimal transport plan", which is why it's called the Earth-Mover distance.</li>
</ul>
</li>
<li>Lipschitz Constraint</li>
<li>A function f is called Lipschitz continuous if it satisfies:<span class="arithmatex">(<span class="arithmatex">\(|f(x) - f(y)| \leq C|x - y|\)</span>\)</span><ul>
<li>where <span class="arithmatex">\(C\)</span> is the Lipschitz constant.</li>
<li>When inputs x and y are close to each other, their corresponding outputs <span class="arithmatex">\(f(x)\)</span> and <span class="arithmatex">\(f(y)\)</span> must also be close. This property ensures smoothness and continuity in the function</li>
</ul>
</li>
<li>As for a discriminator in WGAN, D is constrained to be 1-Lipschitz functions: <span class="arithmatex">\(|D(x) - D(y)| \leq |x - y|\)</span></li>
<li>Wasserstein Distance in WGAN:</li>
<li>GAN objective function :<span class="arithmatex">\(<span class="arithmatex">\(\min_{G} \max_{D} V(D,G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]\)</span>\)</span></li>
<li>In WGAN, the objective function can be written as:<span class="arithmatex">(<span class="arithmatex">\(W(P_r, P_g) = \sup_{||f||_L \leq 1} \mathbb{E}_{x \sim \mathbb{P}_r}[D(x)] - \mathbb{E}_{x \sim \mathbb{P}_g}[D(x)]\)</span>\)</span>
  where ：<ul>
<li>D is constrained to be 1-Lipschitz functions.</li>
<li><span class="arithmatex">\(\mathbb{P}_r\)</span> represents the distribution of real data (real probability distribution)</li>
<li><span class="arithmatex">\(x\sim\mathbb{P}_r\)</span> means x is sampled from the real data distribution</li>
<li><span class="arithmatex">\(\mathbb{P}_g\)</span> represents the distribution of generated data (generated probability distribution)</li>
<li><span class="arithmatex">\({x}\sim\mathbb{P}_g\)</span> means x is sampled from the generator's distribution</li>
<li>WGAN tries to minimize the Wasserstein distance between <span class="arithmatex">\(\mathbb{P}_r\)</span> and <span class="arithmatex">\(\mathbb{P}_g\)</span> </li>
<li><span class="arithmatex">\(||f||_L \leq 1\)</span> means that the discriminator D must satisfy the Lipschitz condition.</li>
</ul>
</li>
</ol>
<hr />
<p>how do we get the loss function:
* Primal Form (Original Wasserstein Distance):
  <span class="arithmatex">\(<span class="arithmatex">\(W(P,Q) = \inf_{\gamma \in \Pi(P,Q)} \mathbb{E}_{(x,y)\sim \gamma}[||x-y||]\)</span>\)</span>
  where <span class="arithmatex">\(<span class="arithmatex">\(\Pi(P,Q)\)</span>\)</span> is the set of all joint distributions (couplings) whose margins are P and Q.
* Kantorovich Duality Theorem:According to the duality Theorem, this problem is equivalent to:
<span class="arithmatex">\(<span class="arithmatex">\(W(P,Q) = \sup_{f\in Lip_1} \left(\int f\,dP - \int f\,dQ\right)\)</span>\)</span>
where <span class="arithmatex">\(Lip_1\)</span> is the set of 1-Lipschitz functions.
*  Expectation Form:Converting the integrals to expectations:
<span class="arithmatex">\(<span class="arithmatex">\(W(P,Q) = \sup_{||f||_L \leq 1} [\mathbb{E}_{x\sim P}[f(x)] - \mathbb{E}_{x\sim Q}[f(x)]]\)</span>\)</span>
*  Application to GAN: When P = Pr (real distribution) and Q = Pg (generated distribution):
<span class="arithmatex">\(<span class="arithmatex">\(W(P_r,P_g) = \sup_{||f||_L \leq 1} [\mathbb{E}_{x\sim P_r}[f(x)] - \mathbb{E}_{x\sim P_g}[f(x)]]\)</span>\)</span></p>
<hr />
<ol>
<li>Weight Clipping : -&gt; implementation of Lipschitz Constraint in WGAN</li>
<li>WGAN forces the discriminator to satisfy the Lipschitz constraint through weight clipping or gradient penalty.</li>
<li>After each gradient update, the weights of the critic (discriminator) are clipped to a fixed range [-c, c] </li>
<li>In the paper, c = 0.01
 ```python</li>
</ol>
<h1 id="_1">在每次参数更新后执行<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<p>for param in discriminator.parameters():
    param.data.clamp_(-c, c)  # c通常设为0.01
Weight Clipping -&gt; all weight values are forced to be limited to the range of [-0.01, 0.01].</p>
<pre><code>- Any value outside this range will be &quot;clipped&quot; to the boundary value. 
- This ensures the Lipschitz constraint of the network, but may also lead to limitations in expressiveness.

**&quot;Weight clipping is a clearly terrible way to enforce a Lipschitz constraint.&quot;** -- M. Arjovsky, S. Chintala and L. Bottou, &quot;Wasserstein Generative Adversarial Networks,&quot; in International Conference on Machine Learning, 2017, pp. 214-223.

## Training process -&gt; Wasserstein &amp; Lipschitz  Gradient

![image](https://hackmd.io/_uploads/SkSZ5NovJl.png)
*Source: Arjovsky et al., &quot;Wasserstein GAN&quot; (2017) arXiv:1701.07875*



1. Discriminator  Gradient:
The Wasserstein loss for the Discriminator is:$$L(w) = \mathbb{E}_{x \sim \mathbb{P}_r}[f_w(x)] - \mathbb{E}_{z \sim p(z)}[f_w(g_\theta(z))]$$
where:
* $f_w(x)$ Discriminator evaluates real data samples
* $x\sim\mathbb{P}_r$ means x is sampled from the real data distribution
* $f_w(g_θ(z))$ Discriminator evaluates generated data samples
    * $z$ is random noise transformed by generator $g_θ$ 
    * $g_θ(z)$ represents the Generator generated samples

For a batch of size m, the empirical version becomes:$$L(w) = \frac{1}{m}\sum_{i=1}^m f_w(x^{(i)}) - \frac{1}{m}\sum_{i=1}^m f_w(g_\theta(z^{(i)}))
$$
Therefore, the gradient with respect to Discriminator parameters w is:$$\nabla_w L = \frac{1}{m}\sum_{i=1}^m \nabla_w f_w(x^{(i)}) - \frac{1}{m}\sum_{i=1}^m \nabla_w f_w(g_\theta(z^{(i)}))
$$

2. Generator Gradient:
The generator's objective is to minimize:$$L(θ) = -\mathbb{E}_{z \sim p(z)}[f_w(g_\theta(z))]$$
For a batch of size m, this becomes:$$L(θ) = -\frac{1}{m}\sum_{i=1}^m f_w(g_\theta(z^{(i)}))
$$
The gradient with respect to generator parameters θ is:$$\nabla_\theta L = -\frac{1}{m}\sum_{i=1}^m \nabla_\theta f_w(g_\theta(z^{(i)}))
$$
- Line 5: Discriminator gradient computation$$g_w ← \nabla_w [\frac{1}{m}\sum_{i=1}^m f_w(x^{(i)}) - \frac{1}{m}\sum_{i=1}^m f_w(g_\theta(z^{(i)}))]$$
- Line 10: Generator gradient computation$$g_\theta ← -\nabla_\theta \frac{1}{m}\sum_{i=1}^m f_w(g_\theta(z^{(i)}))$$
```python
for i, (imgs, _) in enumerate(dataloader):
        # Configure input
        real_imgs = Variable(imgs.type(Tensor))
        # ---------------------
        #  Train Discriminator
        # ---------------------
        optimizer_D.zero_grad()
        # Sample noise as generator input
        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))
        # Generate a batch of images
        fake_imgs = generator(z).detach()
        # Adversarial loss
        loss_D = -torch.mean(discriminator(real_imgs)) + torch.mean(discriminator(fake_imgs))
        loss_D.backward()
        optimizer_D.step()
        # Clip weights of discriminator
        for p in discriminator.parameters():
            p.data.clamp_(-opt.clip_value, opt.clip_value)
        # Train the generator every n_critic iterations
        if i % opt.n_critic == 0:
            # -----------------
            #  Train Generator
            # -----------------
            optimizer_G.zero_grad()
            # Generate a batch of images
            gen_imgs = generator(z)
            # Adversarial loss
            loss_G = -torch.mean(discriminator(gen_imgs))
            loss_G.backward()
            optimizer_G.step()
</code></pre>
<p><img alt="image" src="https://hackmd.io/_uploads/r1Q694jv1e.png" />
<em>Source: Arjovsky et al., "Wasserstein GAN" (2017) arXiv:1701.07875</em></p>
<hr />
<h1 id="2017-wgan-gp-improved-training-of-wasserstein-gans">[2017] WGAN-GP: <a href="https://arxiv.org/abs/1704.00028">Improved Training of Wasserstein GANs</a><a class="headerlink" href="#2017-wgan-gp-improved-training-of-wasserstein-gans" title="Permanent link">&para;</a></h1>
<h2 id="overall-introduction-gradient-penalty">Overall Introduction:  gradient penalty<a class="headerlink" href="#overall-introduction-gradient-penalty" title="Permanent link">&para;</a></h2>
<p>WGAN-GP replaces the weight clipping in the original WGAN by adding a gradient penalty term at the random interpolation points between the real and generated data, thereby achieving a more stable training process and better generation effects.
Loss function in WGAN: $$L 
 = \sup_{||f||<em>L \leq 1} \mathbb{E}</em>{x \sim \mathbb{P}<em>r}[D(x)] - \mathbb{E}</em>{x \sim \mathbb{P}_g}[D(x)]
 $$
Loss function in WGAN-GP: <span class="arithmatex">\(<span class="arithmatex">\(L = \mathbb{E}_{x\sim P_r}[D(x)] - \mathbb{E}_{x\sim P_g}[D(x)] + \lambda \mathbb{E}_{\hat{x}\sim P_{\hat{x}}}[(||\nabla_{\hat{x}}D(\hat{x})||_2 - 1)^2]\)</span>\)</span>
where:
- Wasserstein Distance Term :<span class="arithmatex">\(\mathbb{E}_{x\sim P_r}[D(x)] - \mathbb{E}_{x\sim P_g}[D(x)]\)</span>
  - measure distance between real and generated distributions
- Gradient Penalty Term: <span class="arithmatex">\(\lambda \mathbb{E}_{\hat{x}\sim P_{\hat{x}}}[(||\nabla_{\hat{x}}D(\hat{x})||_2 - 1)^2]\)</span>
  - λ is penalty coefficient (typically 10)
  - Ensures gradient norm is close to 1
  - <span class="arithmatex">\(\hat{x}\)</span> is a random interpolation between real samples and generated samples:</p>
<h3 id="drawbacks-as-weight-clipping">Drawbacks as weight-clipping<a class="headerlink" href="#drawbacks-as-weight-clipping" title="Permanent link">&para;</a></h3>
<p><img alt="image" src="https://hackmd.io/_uploads/SJUHiEiD1g.png" />
<em>Source: Gulrajani et al., "Improved Training of Wasserstein GANs" (2017) arXiv:1704.00028</em></p>
<p>Capacity underuse
- Main Issues:
  - Theoretically, this critic should maintain unit gradient magnitudes everywhere, but when using weight clipping constraints, the critic in WGAN tends to learn overly simplistic functions. 
- Experimental Validation:
  - To verify this, we conducted experiments using the real distribution plus random noise as the generator's output. 
  - The results showed that critics with weight clipping indeed overlook the complex features of the data, learning only simple approximations. 
Exploding and vanishing gradients
- Main Issues:
  - WGAN faces optimization challenges during training, caused by the interaction between weight constraints and the loss function. If the clipping threshold <span class="arithmatex">\(c\)</span> is not carefully adjusted, it may lead to either vanishing or exploding gradients.
- Experimental Validation:
  - Researchers conducted experiments on the Swiss Roll toy dataset using three different clipping thresholds: 0.1, 0.01, and 0.001. With weight clipping:
    - At <span class="arithmatex">\(c=0.1\)</span>, gradients exhibited exponential growth (red line going up).
    - At <span class="arithmatex">\(c=0.01\)</span> and <span class="arithmatex">\(c=0.001\)</span>, gradients exhibited exponential decay (purple and green lines going down).
The two smaller graphs on the right show differences in weight distribution:
- The upper graph: Weight clipping pushes weights toward two extreme values.
- The lower graph: Gradient penalty results in a more normal distribution of weights.</p>
<h2 id="loss-function_4">Loss function<a class="headerlink" href="#loss-function_4" title="Permanent link">&para;</a></h2>
<p>Training process:
<img alt="image" src="https://hackmd.io/_uploads/H1KOo4sw1l.png" />
<em>Source: Gulrajani et al., "Improved Training of Wasserstein GANs" (2017) arXiv:1704.00028</em></p>
<p><strong>How does the Gradient Penalty term work in WGAN-GP？</strong></p>
<ol>
<li>The loss function of WGAN-GP:  <span class="arithmatex">\(<span class="arithmatex">\(\min_G \max_D \mathbb{E}_{x\sim P_r}[D(x)] - \mathbb{E}_{z\sim P_z}[D(G(z))] + \lambda \mathbb{E}_{\hat{x}\sim P_{\hat{x}}}[(||\nabla_{\hat{x}}D(\hat{x})||_2 - 1)^2]\)</span>\)</span></li>
<li>The core idea of the Gradient Penalty term is to enforce the 1-Lipschitz constraint on discriminator D across the sample space： <span class="arithmatex">\(|D(x_1) - D(x_2)| \leq |x_1 - x_2|\)</span></li>
<li>The 1-Lipschitz constraint above is equivalent to having the gradient norm of the discriminator not exceeding 1 at any point:  <span class="arithmatex">\(||\nabla_x D(x)||_2 \leq 1\)</span></li>
<li>WGAN-GP enforces the gradient norm to be equal to 1, rather than less than or equal to 1, through the penalty term:<span class="arithmatex">\(\mathcal{L}_{GP} = \lambda \mathbb{E}_{\hat{x}\sim P_{\hat{x}}}[(||\nabla_{\hat{x}}D(\hat{x})||_2 - 1)^2]\)</span></li>
<li><span class="arithmatex">\(\hat{x} = \epsilon x + (1-\epsilon)G(z), \epsilon \sim U[0,1]\)</span><ul>
<li>So <span class="arithmatex">\(\hat{x}\)</span> is a linear interpolation between data points of the real data distribution <span class="arithmatex">\(P_r\)</span> and the generated data distribution <span class="arithmatex">\(P_g\)</span>.</li>
<li>Why sampling?</li>
<li>According to <span class="arithmatex">\(||\nabla_x D(x)||_2 \leq 1\)</span>, the optimal critic forms a line between the paired points of the real and generated distributions with a gradient norm of 1. Therefore, as a compromise, the constraint is only enforced along these sampled lines.</li>
<li>Easy to implement and worked out in experiments.</li>
</ul>
</li>
</ol>
<p><span class="arithmatex">\(\hat{x}\)</span>:</p>
<pre><code class="language-python">alpha = torch.rand(real_samples.size(0), 1, 1, 1)
interpolates = alpha * real_samples + (1 - alpha) * fake_samples
</code></pre>
<p><span class="arithmatex">\(\nabla_{\hat{x}}D(\hat{x})\)</span>:</p>
<pre><code class="language-python">#d(x)
d_interpolates = D(interpolates)  
# \nabla_{\hat{x}}D(\hat{x}):
gradients = autograd.grad(
        outputs=d_interpolates,
        inputs=interpolates,
        grad_outputs=torch.ones_like(d_interpolates),
        create_graph=True
    )[0]
</code></pre>
<p><span class="arithmatex">\((||\nabla_{\hat{x}}D(\hat{x})||_2 - 1)^2\)</span>:</p>
<pre><code class="language-python">gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
</code></pre>
<ol>
<li>The regulatory effect of this penalty term is manifested in:</li>
<li>When <span class="arithmatex">\(||\nabla_{\hat{x}}D(\hat{x})||_2 &gt; 1\)</span>, showing:<ul>
<li>Too Steep Gradients: Discriminators tend to be "aggressive" in judging real/fake samples and change too rapidly </li>
<li>May lead to training instability:</li>
<li>Likely to cause discriminator overfitting</li>
<li>Provides too strong gradient signals to the generator</li>
</ul>
</li>
<li>When <span class="arithmatex">\(||\nabla_{\hat{x}}D(\hat{x})||_2 &lt; 1\)</span>, showing:<ul>
<li>Too Flat Gradients: The Discriminator tends to be  insensitive to input changes </li>
<li>Insufficient Discrimination :  The Discriminator cannot effectively distinguish real/fake samples</li>
<li>Vanishing Gradients: Generator might not receive effective training signals</li>
</ul>
</li>
<li>Only when gradient norm  <span class="arithmatex">\(||\nabla_{\hat{x}}D(\hat{x})||_2 = 1\)</span>, the penalty term becomes zero</li>
</ol>
<pre><code class="language-python">def compute_gradient_penalty(D, real_samples, fake_samples):
    # Random interpolation coefficient
    alpha = torch.rand(real_samples.size(0), 1, 1, 1)
    # Generate interpolated samples
    interpolates = alpha * real_samples + (1 - alpha) * fake_samples
    interpolates.requires_grad_(True)
    # Compute discriminator output
    d_interpolates = D(interpolates)  
    # Compute gradients
    gradients = autograd.grad(
        outputs=d_interpolates,
        inputs=interpolates,
        grad_outputs=torch.ones_like(d_interpolates),
        create_graph=True
    )[0]
    # Compute gradient penalty
    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
    return gradient_penalty

# Training loop
for epoch in epochs:
    for real_data in dataloader:
        # Compute gradient penalty
        gradient_penalty = compute_gradient_penalty(D, real_data, fake_data)
        # Discriminator loss
        d_loss = -torch.mean(D(real_data)) + torch.mean(D(fake_data)) + lambda_gp * gradient_penalty
</code></pre>
<p><img alt="image" src="https://hackmd.io/_uploads/S1MnAVowJl.png" />
<em>Source: Gulrajani et al., "Improved Training of Wasserstein GANs" (2017) arXiv:1704.00028</em></p>
<p><img alt="image" src="https://hackmd.io/_uploads/rygjCViDyx.png" />
<em>Source: Gulrajani et al., "Improved Training of Wasserstein GANs" (2017) arXiv:1704.00028</em></p>
<hr />
<h1 id="2018-pggan-progressive-growing-of-gans-for-improved-quality-stability-and-variation">[2018] PGGAN: <a href="https://arxiv.org/abs/1710.10196">Progressive Growing of GANs for Improved Quality, Stability, and Variation</a><a class="headerlink" href="#2018-pggan-progressive-growing-of-gans-for-improved-quality-stability-and-variation" title="Permanent link">&para;</a></h1>
<h2 id="overall-introduction_2">Overall Introduction:<a class="headerlink" href="#overall-introduction_2" title="Permanent link">&para;</a></h2>
<ol>
<li>Progressive Growing</li>
<li>Core Idea: Start at low resolution and progressively increase to higher resolutions.</li>
<li>Advantages:  More stable training &amp; Higher computational efficiency &amp; Better memory utilization</li>
<li>Implementation: Smoothly fade in new layers and synchronous growth of the generator and discriminator</li>
<li>Minibatch Standard Deviation </li>
<li>Purpose: Increase the diversity of generated images &amp; prevents mode collapse.</li>
<li>Implementation: <ul>
<li>Introduce a statistical layer late in the discriminator</li>
<li>Calculate the standard deviation within a minibatch of samples</li>
<li>Concatenate statistical features with the original features</li>
</ul>
</li>
<li>Normalization Strategies</li>
<li>Purpose: strategies ensure underlying training stability.</li>
<li>Implementation: <ul>
<li>Generator: Uses PixelNorm 
The structure of PGGAN laid an important foundation for subsequent work (such as StyleGAN).</li>
</ul>
</li>
</ol>
<h2 id="progressive-growing-of-gans">PROGRESSIVE GROWING OF GANS<a class="headerlink" href="#progressive-growing-of-gans" title="Permanent link">&para;</a></h2>
<p><img alt="image" src="https://hackmd.io/_uploads/H1g1JBjvJx.png" />
<em>Source: Karras et al., "Progressive Growing of GANs for Improved Quality, Stability, and Variation" (2018) arXiv:1710.10196</em></p>
<p><img alt="image" src="https://hackmd.io/_uploads/SJvW1HsDye.png" />
<em>Source: Karras et al., "Progressive Growing of GANs for Improved Quality, Stability, and Variation" (2018) arXiv:1710.10196</em></p>
<ul>
<li>Each resolution stage has two phases:</li>
<li>Fade-in Phase:<ul>
<li>The new layer is gradually blended in using alpha parameter</li>
<li>Alpha increases linearly from 0 to 1</li>
<li>In PGGAN, the growth of the α parameter is linear and is controlled by the number of training iterations. This is achieved as follows:</li>
</ul>
</li>
</ul>
<pre><code class="language-python"># 假设fade_in_iters是fade-in阶段的总迭代次数
fade_in_iters = 600000  # 600k images
# 当前迭代次数current_iter
alpha = min(current_iter / fade_in_iters, 1.0)

def fade_in(self, alpha, upscaled, generated):
    return torch.tanh(alpha * generated + (1-alpha) * upscaled)
......
final_upscaled = self.rgb_layers[steps-1](upscaled)
final_out = self.rgb_layers[steps](out)
return self.fade_in(alpha, final_upscaled, final_out)
</code></pre>
<ol>
<li>Stabilization Phase: <ul>
<li>Train network with new layers fully active</li>
<li>Old paths are removed</li>
<li>Network stabilizes at new resolution
Time Allocation:</li>
</ul>
</li>
<li>Fade-in Phase: 600k images</li>
<li>Stabilization Phase: 600k images</li>
<li>Total per resolution: 1.2M images (600k + 600k)</li>
</ol>
<pre><code>Complete Training Process Example (from 4×4 to 1024×1024):
4×4:   Only Stabilization     600k images
8×8:   Fade-in + Stabilization 1.2M images
16×16: Fade-in + Stabilization 1.2M images
32×32: Fade-in + Stabilization 1.2M images
64×64: Fade-in + Stabilization 1.2M images
128×128: Fade-in + Stabilization 1.2M images
256×256: Fade-in + Stabilization 1.2M images
512×512: Fade-in + Stabilization 1.2M images
1024×1024: Fade-in + Stabilization 1.2M images
</code></pre>
<p>Smooth Layer Transitions:</p>
<pre><code class="language-python">def forward(self, x, alpha):
    # Old path (lower resolution)
    old_rgb = self.from_rgb_old(x)
    old_rgb = self.upsample(old_rgb)

    # New path (higher resolution)
    new_x = self.upsample(x)
    new_x = self.conv(new_x)
    new_rgb = self.to_rgb_new(new_x)

    # Smooth blending
    return (1 - alpha) * old_rgb + alpha * new_rgb
</code></pre>
<ul>
<li>toRGB: 1×1 convolution to convert features to RGB</li>
<li>fromRGB: 1×1 convolution to convert RGB to features</li>
<li>2×: Upsampling (nearest neighbor)</li>
<li>0.5×: Downsampling (average pooling)</li>
</ul>
<h2 id="increasing-variation-using-minibatch-standard-deviation">INCREASING VARIATION USING MINIBATCH STANDARD DEVIATION<a class="headerlink" href="#increasing-variation-using-minibatch-standard-deviation" title="Permanent link">&para;</a></h2>
<p><img alt="image" src="https://hackmd.io/_uploads/S1bEyBjwkx.png" />
<em>Source: Wang et al., "Citrus Disease Image Generation and Classification Based on Improved FastGAN and EfficientNet-B5" (2023) Electronics, 12(5), 1232</em></p>
<ol>
<li>For each feature and spatial location i, compute standard deviation across the batch:
  <span class="arithmatex">\(<span class="arithmatex">\(\sigma_i(x) = \sqrt{\frac{1}{N}\sum_{k=1}^{N}(x_{ik} - \mu_i)^2}\)</span>\)</span>
where:</li>
<li><span class="arithmatex">\(x_{ik}\)</span> is the feature value for sample k at position i</li>
<li><span class="arithmatex">\(\mu_i = \frac{1}{N}\sum_{k=1}^{N}x_{ik}\)</span> is the mean across the batch</li>
<li>N is the batch size</li>
<li>Average the standard deviations across features and spatial dimensions:
  <span class="arithmatex">\(<span class="arithmatex">\(\sigma = \frac{1}{C \times H \times W}\sum_{i}\sigma_i(x)\)</span>\)</span>
where:</li>
<li><span class="arithmatex">\(C\)</span> : channels. <span class="arithmatex">\(H\)</span>: height. <span class="arithmatex">\(W\)</span> :width
These statistics are then:</li>
<li>Replicated into a <span class="arithmatex">\([1×1×H×W]\)</span> tensor</li>
<li>Further replicated N times to match batch size: <span class="arithmatex">\([N×1×H×W]\)</span></li>
<li>Concatenated with original input along channel dimension to get final output of shape <span class="arithmatex">\([N×(C+1)×H×W]\)</span></li>
</ol>
<pre><code class="language-python">def minibatch_stddev_layer(x, group_size=4):
    N, C, H, W = x.shape
    G = min(group_size, N)  # 分组大小

    # [NCHW] -&gt; [GMCHW] 分成G组
    y = x.reshape(G, -1, C, H, W)

    # 计算标准差
    y = torch.sqrt(y.var(0) + 1e-8)  # [-MCHW]

    # 取平均得到单个值
    y = y.mean(dim=[0,1,2,3])  # []

    # 广播回原始形状
    y = y.reshape(1, 1, 1, 1)
    y = y.repeat(N, 1, H, W)  # [N1HW]

    # 连接到输入特征图
    return torch.cat([x, y], 1)  # [N(C+1)HW]
</code></pre>
<p>The main advantages of this technique are:</p>
<ol>
<li>Helps the discriminator identify the statistical characteristics of the generated images</li>
<li>Encourages the generator to produce more diverse outputs</li>
<li>Helps avoid mode collapse</li>
</ol>
<h2 id="normalization-in-generator-and-discriminator">NORMALIZATION IN GENERATOR AND DISCRIMINATOR<a class="headerlink" href="#normalization-in-generator-and-discriminator" title="Permanent link">&para;</a></h2>
<h3 id="normalization-in-passed-gan-related-model">Normalization in passed GAN-related-model<a class="headerlink" href="#normalization-in-passed-gan-related-model" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>GAN model</th>
<th>Normalization applied</th>
<th>Implementation detail</th>
</tr>
</thead>
<tbody>
<tr>
<td>GAN</td>
<td>batch normalization</td>
<td>Typically employs basic batch normalization in both the generator and discriminator.</td>
</tr>
<tr>
<td>cGAN</td>
<td>batch normalization</td>
<td>Typically employs basic batch normalization in both the generator and discriminator.</td>
</tr>
<tr>
<td>DCGAN</td>
<td>batch normalization</td>
<td>Generator: BN is used in all layers except the output layer. Discriminator: BN is used in all layers except the input and output layers.</td>
</tr>
<tr>
<td>WGAN</td>
<td>Remove BN in the discriminator</td>
<td>Advises against using batch normalization due to its impact on the Lipschitz constraint. Completely removes BN in the discriminator. The generator may use BN, but it is often omitted in practice.</td>
</tr>
<tr>
<td>WGAN-GP</td>
<td>Remove BN in the discriminator</td>
<td>Discriminator: Recommends using layer normalization or instance normalization instead of batch normalization. This is because BN introduces correlations between samples, affecting the calculation of the gradient penalty.</td>
</tr>
<tr>
<td>PGGAN</td>
<td>Completely Remove BN and Pixel-wise normalization in the generator and new weight initialization</td>
<td>Generator: BN is used in all layers except the output layer. Discriminator: BN is used in all layers except the input and output layers.</td>
</tr>
</tbody>
</table>
<h3 id="pixelwise-feature-vector-normalization-in-generator">PIXELWISE FEATURE VECTOR NORMALIZATION IN GENERATOR<a class="headerlink" href="#pixelwise-feature-vector-normalization-in-generator" title="Permanent link">&para;</a></h3>
<p>Applied after each convolutional layer in the generator at each pixel position independently:
<span class="arithmatex">\(<span class="arithmatex">\(b_{x,y} = \frac{a_{x,y}}{\sqrt{\frac{1}{N}\sum_{j=0}^{N-1}(a_{x,y}^j)^2 + \epsilon}}\)</span>\)</span>, 
Where:
- <span class="arithmatex">\(\epsilon = 10^{-8}\)</span></p>
<ul>
<li><span class="arithmatex">\(a_{x,y} \text{ is the original feature vector at pixel position } (x,y)\)</span></li>
<li><span class="arithmatex">\(b_{x,y} \text{ is the normalized feature vector at pixel position } (x,y)\)</span></li>
<li><span class="arithmatex">\(N \text{ is the number of feature maps (channels)}\)</span></li>
<li><span class="arithmatex">\(\epsilon = 10^{-8} \text{ is a small constant to prevent division by zero}\)</span></li>
<li><span class="arithmatex">\(\text{The sum } \sum_{j=0}^{N-1} \text{ is taken over all } N \text{ feature maps for that pixel position}\)</span></li>
</ul>
<pre><code class="language-python">class PixelNorm(nn.Module):
    def forward(self, x):
        norm = torch.mean(x ** 2, dim=1, keepdim=True)
        norm = torch.sqrt(norm + self.epsilon)
        return x / norm

</code></pre>
<ul>
<li><span class="arithmatex">\(x ** 2\)</span>:  Calculate the squared values of all features at each pixel <span class="arithmatex">\((a_{x,y}^j)^2\)</span></li>
<li><code>torch.mean(..., dim=1)</code>:  Average these squares across all feature maps <span class="arithmatex">\({\frac{1}{N}\sum_{j=0}^{N-1}(a_{x,y}^j)^2 }\)</span></li>
<li>torch.sqrt(... + epsilon): Take the square root of the average (plus ε) <span class="arithmatex">\({\sqrt{\frac{1}{N}\sum_{j=0}^{N-1}(a_{x,y}^j)^2 + \epsilon}}\)</span></li>
<li><span class="arithmatex">\(x / norm\)</span>:  normalization <span class="arithmatex">\(\frac{a_{x,y}}{\sqrt{\frac{1}{N}\sum_{j=0}^{N-1}(a_{x,y}^j)^2 + \epsilon}}\)</span>$</li>
</ul>
<h3 id="qualized-learning-rate">QUALIZED LEARNING RATE<a class="headerlink" href="#qualized-learning-rate" title="Permanent link">&para;</a></h3>
<p><strong>Problem</strong>: In traditional neural network training, parameters of different layers may have different dynamic ranges. When using adaptive optimizers like RMSProp or Adam, they normalize gradient updates based on the standard deviation of parameters. This results in parameters with larger dynamic ranges requiring more time to adjust properly.
Specific Implementation:
1. Traditional Weight Initialization 
  - In standard neural networks, weights are typically initialized using methods 
    - like He initialization N(0, sqrt(2/n)), Better suited for ReLU</p>
<pre><code class="language-python"># Standard He initialization (for comparison)
weight_shape = (out_channels, in_channels, kernel, kernel)
std = np.sqrt(2.0 / (in_channels * kernel * kernel))
weights = np.random.normal(0, std, weight_shape)
</code></pre>
<ul>
<li>This can lead to different layers learning at different rates, causing training instability</li>
<li>The variance of the gradients can differ significantly between layers</li>
<li>The Equalized Learning Rate Solution: </li>
<li>initialization (happens only once when the model is created):<ul>
<li>Instead of using usual standard initialization, weights are initialized from N(0,1)</li>
</ul>
</li>
</ul>
<h2 id="equalized-learning-rate-approach">Equalized learning rate approach<a class="headerlink" href="#equalized-learning-rate-approach" title="Permanent link">&para;</a></h2>
<p>weights = np.random.normal(0, 1.0, weight_shape)
runtime_coef = std  # Applied during forward pass</p>
<ul>
<li>During training (happens every forward pass): scaling process</li>
<li>Each layer's weights are explicitly scaled during runtime by a layer-specific constant </li>
<li>The scaling factor is the per-layer normalization constant from He initialization</li>
</ul>
<h3 id="in-equalizedconv2d">In EqualizedConv2d<a class="headerlink" href="#in-equalizedconv2d" title="Permanent link">&para;</a></h3>
<pre><code class="language-python">self.scale = np.sqrt(2) / np.sqrt(fan_in)  # Calculate He scaling factor
scaled_weight = self.weight * self.scale    # Apply scaling at runtime
</code></pre>
<ul>
<li>This ensures that the dynamic range and learning speed are similar for all weights</li>
<li>Standard layers: Apply scaling during initialization<ul>
<li>Equalized layers: Apply scaling during each forward pass</li>
<li>This ensures the gradient updates remain properly scaled throughout training</li>
</ul>
</li>
</ul>
<p>Why It's Useful:
1. Ensures that all weights have a similar dynamic range.
2. Makes the learning process more balanced, avoiding slow learning for some parameters due to large ranges.
3. Better adapts to c
- In standard neural networks, weights are typically initialized using methods 
  - like He initialization <span class="arithmatex">\(N(0, sqrt(2/n))\)</span>, Better suited for ReLU</p>
<hr />
<p>RMSProp or Adam, they normalize gradient updates based on the standard deviation of parameters：
RMSprop:<span class="arithmatex">\(<span class="arithmatex">\(\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{v_t + \epsilon}} g_t\)</span>\)</span> Adam:<span class="arithmatex">\(<span class="arithmatex">\(\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t\)</span>\)</span>
1. Standard Deviation Estimation:
- <span class="arithmatex">\(v_t\)</span> actually estimates the exponential moving average of squared gradients:<span class="arithmatex">\(v_t = \beta v_{t-1} + (1-\beta)g_t^2\)</span>
- This accumulated squared gradient <span class="arithmatex">\(v_t\)</span> is essentially estimating the second moment of the gradient, and its square root <span class="arithmatex">\(\sqrt v_t\)</span> approximates the standard deviation of the gradient
2. Normalization Effect:
- In the update formula, the gradient term (<span class="arithmatex">\(g_t\)</span> or <span class="arithmatex">\(ĥ_t\)</span>) is divided by <span class="arithmatex">\(\sqrt v_t\)</span>
- This is equivalent to normalizing the gradient update by the gradient's standard deviation
- Mathematically equivalent to:<span class="arithmatex">\(<span class="arithmatex">\(\text{normalized update} = \frac{g_t}{\sqrt{v_t + \epsilon}}\)</span>\)</span>
3. Why This Is Standard Deviation Normalization:
- If a parameter has large gradient variations (high standard deviation), <span class="arithmatex">\(\sqrt v_t\)</span> will become larger
- This will make the actual update step smaller
- Conversely, if gradient variations are small (low standard deviation), the update step will become larger accordingly
- This achieves adaptive standard deviation normalization
    - This is also why EQUALIZED LEARNING RATE solves this problem by explicitly controlling the dynamic range of parameters (<span class="arithmatex">\(ŵᵢ = wᵢ/c\)</span>).</p>
<p>Main Advantages:
1. Keeps the learning speed consistent for all weights.
2. Avoids the issue of having both too high and too low learning rates at the same time.
3. By dynamically scaling during runtime rather than statically at initialization, it makes the training process more stable.</p>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": ["navigation.tabs", "toc.integrate"], "search": "../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.88dd0f4e.min.js"></script>
      
        <script src="../../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>