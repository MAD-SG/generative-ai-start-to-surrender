
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../cd/">
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.50">
    
    
      
        <title>Score Matching - Generative AI: From Start to Surrender</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.a40c8224.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#score-matching" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Generative AI: From Start to Surrender" class="md-header__button md-logo" aria-label="Generative AI: From Start to Surrender" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Generative AI: From Start to Surrender
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Score Matching
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter1_Introduction/1.1terminology/" class="md-tabs__link">
          
  
  Introduction

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter2_VAE/2.1introduction/" class="md-tabs__link">
          
  
  VAE

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter3_GAN/3.1from_gan_to_stylegan/paper/" class="md-tabs__link">
          
  
  GANs

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter4_diffusion/4.1introduction/" class="md-tabs__link">
          
  
  Diffusion

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter5_generation_theory/5.1MLE/" class="md-tabs__link">
          
  
  Generation Theory

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../introduction/" class="md-tabs__link">
          
  
  Energy Based Models

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Generative AI: From Start to Surrender" class="md-nav__button md-logo" aria-label="Generative AI: From Start to Surrender" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Generative AI: From Start to Surrender
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Introduction
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter1_Introduction/1.1terminology/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Terms
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter1_Introduction/1.2fourier_transform/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fourier Transform
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter1_Introduction/1.3signal_processing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Signal Processing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter1_Introduction/1.4statistics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Statistics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter1_Introduction/1.5SDE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SDE
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter1_Introduction/tutorials/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tutorials
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    VAE
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            VAE
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter2_VAE/2.1introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    GANs
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            GANs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter3_GAN/3.1from_gan_to_stylegan/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    From GAN to PGGAN
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter3_GAN/3.3stylegan/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StyleGAN
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter3_GAN/3.4stylegan2/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StyleGAN2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter3_GAN/3.5stylegan3/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StyleGAN3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter3_GAN/3.6styleganT/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StyleGAN T
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter3_GAN/3.7R3Gan/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    R3GAN
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Diffusion
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Diffusion
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter4_diffusion/4.1introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter4_diffusion/4.2DDPM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DDPM
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Generation Theory
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Generation Theory
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter5_generation_theory/5.1MLE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Maximal Likelihood
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter5_generation_theory/manifold_hypothesis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Manifold Hypothesis
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" checked>
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Energy Based Models
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Energy Based Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../score_function/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Score Function
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sampling/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sampling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cd/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contrastive Divergence
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Score Matching
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Score Matching
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#explicit-score-matching" class="md-nav__link">
    <span class="md-ellipsis">
      Explicit Score-Matching
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#implicit-score-matching" class="md-nav__link">
    <span class="md-ellipsis">
      Implicit Score Matching
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#siced-score-matching" class="md-nav__link">
    <span class="md-ellipsis">
      Siced score matching
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#denoising-score-matching" class="md-nav__link">
    <span class="md-ellipsis">
      Denoising Score Matching
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="score-matching">Score matching<a class="headerlink" href="#score-matching" title="Permanent link">&para;</a></h1>
<p>Score Matching 是一种估计概率分布的密度模型的方法，最早由 Aapo Hyvärinen 在 2005 年提出。它的主要目标是通过直接拟合数据的概率密度梯度（也称为 score function）来避免对归一化因子（partition function）的显式计算。以下是对 Score Matching 方法的总结：</p>
<p><strong>核心思想</strong>
- 设 <span class="arithmatex">\(p(x)\)</span> 是数据的真实分布，模型分布为 <span class="arithmatex">\(q_\theta(x)\)</span>。</p>
<ul>
<li>
<p>Score Matching 的目标是使模型的 score function <span class="arithmatex">\(\nabla_x \log q_\theta(x)\)</span> 尽可能接近数据分布的 score function <span class="arithmatex">\(\nabla_x \log p(x)\)</span></p>
</li>
<li>
<p>换句话说，优化目标是最小化以下目标函数：</p>
</li>
</ul>
<div class="arithmatex">\[
 J(\theta) = \frac{1}{2} \mathbb{E}_{p(x)} \left[ \| \nabla_x \log q_\theta(x) - \nabla_x \log p(x) \|^2 \right].
\]</div>
<p>总体来说，在EBM和SBM (energy based model and score based model) 中,score matching 都可以使用。在EBM中，函数是一个标量，表示能量，能量低的地方表示概率高，稳定区域。在SBM中，函数是一个向量，表示 score function，score 表示能量或者密度变化的剧烈程度和方向，随着score 的负方向，会到达能量低的区域。score matching 在这两种模型中都有自然的扩展，即可以拟合非常复杂的density function或者其score function。</p>
<p>对于score matching, 它也有不同的实现方法。</p>
<ul>
<li>explicite score matching</li>
<li>sliced score matching</li>
<li>denosing score matching</li>
</ul>
<h2 id="explicit-score-matching">Explicit Score-Matching<a class="headerlink" href="#explicit-score-matching" title="Permanent link">&para;</a></h2>
<h3 id="esm">显式分数匹配（ESM）的直观思想<a class="headerlink" href="#esm" title="Permanent link">&para;</a></h3>
<p>ESM 的核心目标是训练一个模型 $ s_\theta(x) $，使其预测的梯度尽可能接近真实的 score function。为此，ESM 直接定义一个损失函数：</p>
<div class="arithmatex">\[
\mathcal{L}(\theta) = \mathbb{E}_{x \sim p(x)} \left[ \| s_\theta(x) - \nabla_x \log p(x) \|^2 \right]
\]</div>
<p>但问题在于：<strong>真实分布 $ p(x) $ 未知</strong>，因此无法直接计算真实的梯度。
<strong>解决方案</strong>：先用 <strong>核密度估计（KDE）</strong> 近似 $ p(x) $，再用 KDE 的结果训练模型。</p>
<hr />
<h3 id="step-1kde"><strong>Step 1：用核密度估计（KDE）近似数据分布</strong><a class="headerlink" href="#step-1kde" title="Permanent link">&para;</a></h3>
<p>KDE 是一种非参数方法，通过数据点周围的“平滑小山丘”（核函数）叠加来估计分布。例如，对于数据点 $ {x_1, x_2, ..., x_N} $，KDE 的公式为：</p>
<div class="arithmatex">\[
\hat{p}(x) = \frac{1}{N} \sum_{i=1}^N K_h(x - x_i)
\]</div>
<p>其中 $ K_h $ 是核函数（如高斯核），$ h $ 是控制平滑度的带宽参数。</p>
<p><strong>高斯核的直观解释</strong>：每个数据点 $ x_i $ 周围生成一个钟形曲线，所有曲线叠加形成整体分布。</p>
<hr />
<h3 id="step-2-kde"><strong>Step 2：计算 KDE 的梯度</strong><a class="headerlink" href="#step-2-kde" title="Permanent link">&para;</a></h3>
<p>为了得到 score function，需计算 $ \nabla_x \log \hat{p}(x) $。以高斯核为例：
1. 计算梯度的分子部分：</p>
<p>$$
   \nabla_x \hat{p}(x) = \frac{1}{N} \sum_{i=1}^N K_h(x - x_i) \cdot \left(-\frac{x - x_i}{h^2}\right)
   $$</p>
<ol>
<li>计算梯度：</li>
</ol>
<p>$$
   \nabla_x \log \hat{p}(x) = \frac{\nabla_x \hat{p}(x)}{\hat{p}(x)}
   $$</p>
<p><strong>物理意义</strong>：梯度方向指向周围数据点的加权平均位置。</p>
<h3 id="step-3"><strong>Step 3：训练模型拟合梯度</strong><a class="headerlink" href="#step-3" title="Permanent link">&para;</a></h3>
<p>定义一个模型（如神经网络） $ s_\theta(x) $，输入数据点 $ x $，输出预测的梯度。损失函数为：</p>
<div class="arithmatex">\[
\mathcal{L}(\theta) \approx \frac{1}{N} \sum_{i=1}^N \| s_\theta(x_i) - \nabla_x \log \hat{p}(x_i) \|^2
\]</div>
<p><strong>训练过程</strong>：
1. 对每个数据点 $ x_i $，用 KDE 计算其梯度 $ \nabla_x \log \hat{p}(x_i) $。
2. 用梯度下降法优化模型参数 $ \theta $，使预测值逼近 KDE 的梯度。</p>
<h3 id="_1"><strong>为什么需要改进？</strong><a class="headerlink" href="#_1" title="Permanent link">&para;</a></h3>
<p>尽管方法直观，但存在两个主要问题：
1. <strong>维度灾难</strong>：KDE 在高维数据中效果差（计算量大且估计不准）。
2. <strong>计算效率</strong>：每个点的梯度计算需遍历所有数据，复杂度为 $ O(N) $。</p>
<h3 id="_2"><strong>改进方案</strong><a class="headerlink" href="#_2" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>去噪分数匹配（DSM）</strong>：
   直接向数据添加噪声（如高斯噪声），利用噪声分布的已知梯度间接训练模型，避免显式计算 $ p(x) $。</p>
</li>
<li>
<p><strong>降维预处理</strong>：
   对高维数据使用 PCA 或自编码器降维，再在低维空间应用 KDE。</p>
</li>
<li>
<p><strong>Mini-batch 优化</strong>：
   每次随机采样部分数据计算梯度，减少计算量。</p>
</li>
</ol>
<h3 id="_3"><strong>总结与适用场景</strong><a class="headerlink" href="#_3" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>适用场景</strong>：低维数据分布估计、生成模型预训练、小规模数据分析。</li>
<li><strong>优势</strong>：无需假设数据分布形式，直接通过数据学习梯度。</li>
<li><strong>局限性</strong>：高维数据效果受限，需结合 DSM 或降维技术。</li>
</ul>
<p>通过 KDE 与显式分数匹配的结合，我们能够从有限的数据中“感知”概率分布的变化方向，为后续生成模型或异常检测任务奠定基础。尽管存在挑战，这一方法在低维场景中仍是一个简洁而强大的工具。</p>
<h3 id="_4">实验<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h3>
<p>我们用一个二维的混合高斯分布进行实验。</p>
<pre><code class="language-python">def generate_data(num_samples=1000):
    # 生成两个高斯分布的混合数据
    mean1 = [2, 2]
    cov1 = [[1, 0.5], [0.5, 1]]
    data1 = np.random.multivariate_normal(mean1, cov1, num_samples//2)

    mean2 = [-2, -2]
    cov2 = [[1, -0.5], [-0.5, 1]]
    data2 = np.random.multivariate_normal(mean2, cov2, num_samples//2)

    return np.vstack([data1, data2])
</code></pre>
<p>根据以上的混合高斯分布生成的数据。
<img alt="alt text" src="../../../images/image-25.png" />
然后我们进行KDE，然后利用KDE去估计梯度。最后得到预测结果和真实梯度的可视化。</p>
<p><img alt="alt text" src="../../../images/image-27.png" /></p>
<p>可以看到最终在高概率区域，分布是比较相似的，但是在低概率区域，分布相差较大。这个原因主要是因为低概率区域的数据量较少，KDE 估计的梯度可能会受到大量噪声的影响。 同时模型学出来的分布比KDE和原始分布更加接近。</p>
<h2 id="implicit-score-matching">Implicit Score Matching<a class="headerlink" href="#implicit-score-matching" title="Permanent link">&para;</a></h2>
<h3 id="fisher">从Fisher散度到隐式分数匹配的推导<a class="headerlink" href="#fisher" title="Permanent link">&para;</a></h3>
<h3 id="1-fisher">1. 定义Fisher散度<a class="headerlink" href="#1-fisher" title="Permanent link">&para;</a></h3>
<p>Fisher散度衡量两个概率分布 $ p(\mathbf{x}) $ 和 $ q(\mathbf{x}; \theta) $ 的分数差异：</p>
<div class="arithmatex">\[
D_F(p \parallel q) = \mathbb{E}_{p(\mathbf{x})} \left[ \| \nabla_{\mathbf{x}} \log p(\mathbf{x}) - \nabla_{\mathbf{x}} \log q(\mathbf{x}; \theta) \|^2 \right].
\]</div>
<h3 id="2">2. 展开平方项<a class="headerlink" href="#2" title="Permanent link">&para;</a></h3>
<p>将平方项展开为三部分：</p>
<div class="arithmatex">\[
D_F = \underbrace{\mathbb{E}_{p} \left[ \| \nabla \log p \|^2 \right]}_{T_1} - 2 \underbrace{\mathbb{E}_{p} \left[ (\nabla \log p)^\top (\nabla \log q) \right]}_{T_2} + \underbrace{\mathbb{E}_{p} \left[ \| \nabla \log q \|^2 \right]}_{T_3}.
\]</div>
<h3 id="3-t_2">3. 处理交叉项 $ T_2 $<a class="headerlink" href="#3-t_2" title="Permanent link">&para;</a></h3>
<p>交叉项 $ T_2 = -2 \mathbb{E}_{p} \left[ (\nabla \log p)^\top (\nabla \log q) \right] $ 包含未知的 $ \nabla \log p $，需通过分部积分消除依赖。</p>
<h4 id="_5">分量为标量的推导<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h4>
<p>对每个维度 $ x_i $ 单独处理：</p>
<div class="arithmatex">\[
T_2 = -2 \sum_{i=1}^d \mathbb{E}_{p} \left[ \partial_i \log p \cdot \partial_i \log q \right].
\]</div>
<p>应用分部积分公式：</p>
<div class="arithmatex">\[
\int p(\mathbf{x}) \partial_i \log p \cdot \partial_i \log q \, d\mathbf{x} = -\int p(\mathbf{x}) \partial_i^2 \log q \, d\mathbf{x}.
\]</div>
<p><strong>关键假设</strong>：概率密度在边界处衰减至零，即：</p>
<div class="arithmatex">\[
\left. p(\mathbf{x}) \partial_i \log q \right|_{x_i \to \pm\infty} = 0.
\]</div>
<h4 id="_6">合并所有维度<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h4>
<p>对每个分量积分后求和：</p>
<div class="arithmatex">\[
T_2 = 2 \sum_{i=1}^d \mathbb{E}_{p} \left[ \partial_i^2 \log q \right] = 2 \mathbb{E}_{p} \left[ \text{tr}(\nabla_{\mathbf{x}}^2 \log q) \right],
\]</div>
<p>其中 $ \text{tr}(\nabla_{\mathbf{x}}^2 \log q) $ 表示Hessian矩阵的迹。</p>
<h3 id="4-fisher">4. 重组Fisher散度<a class="headerlink" href="#4-fisher" title="Permanent link">&para;</a></h3>
<p>将 $ T_1, T_2, T_3 $ 代入原式：</p>
<div class="arithmatex">\[
D_F = \mathbb{E}_{p} \left[ \| \nabla \log p \|^2 \right] + \mathbb{E}_{p} \left[ \| \nabla \log q \|^2 + 2 \, \text{tr}(\nabla^2 \log q) \right].
\]</div>
<p>忽略常数项 $ \mathbb{E}_{p} \left[ | \nabla \log p |^2 \right] $，优化目标简化为：</p>
<div class="arithmatex">\[
\min_{\theta} \mathbb{E}_{p} \left[ \| \nabla \log q \|^2 + 2 \, \text{tr}(\nabla^2 \log q) \right].
\]</div>
<h3 id="5">5. 标准化目标函数<a class="headerlink" href="#5" title="Permanent link">&para;</a></h3>
<p>引入缩放因子 $ \frac{1}{2} $，得到隐式分数匹配目标函数：</p>
<div class="arithmatex">\[
J(\theta) = \mathbb{E}_{p(\mathbf{x})} \left[ \frac{1}{2} \| \nabla_{\mathbf{x}} \log q(\mathbf{x}; \theta) \|^2 + \text{tr}(\nabla_{\mathbf{x}}^2 \log q(\mathbf{x}; \theta)) \right].
\]</div>
<h3 id="6">6. 隐式性分析<a class="headerlink" href="#6" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>显式匹配项</strong>：$ \frac{1}{2} | \nabla \log q |^2 $ 直接约束分数模长。</li>
<li><strong>隐式正则项</strong>：$ \text{tr}(\nabla^2 \log q) $ 通过二阶导数隐式约束分数方向，避免依赖 $ \nabla \log p $。</li>
</ul>
<hr />
<h4 id="7">7. 最终形式<a class="headerlink" href="#7" title="Permanent link">&para;</a></h4>
<p>隐式分数匹配的目标函数为：</p>
<div class="arithmatex">\[
\boxed{J(\theta) = \mathbb{E}_{p(\mathbf{x})} \left[ \frac{1}{2} \| \nabla_{\mathbf{x}} \log q(\mathbf{x}; \theta) \|^2 + \text{tr}(\nabla_{\mathbf{x}}^2 \log q(\mathbf{x}; \theta)) \right]}.
\]</div>
<p>当然如果我们只关注score function 本身，而不需要知道原始的density function 或者说energy function, 那么问题就简化了，就不涉及到二阶导了。</p>
<p>Let <span class="arithmatex">\(s_\theta(x) = \nabla_{\mathbf{x}} \log q(\mathbf{x}; \theta)\)</span> be score function.</p>
<p>The implicit score matching loss can be approximated by Monte Carlo:</p>
<div class="arithmatex">\[
J_{\text{ISM}}(\theta) \approx \frac{1}{M} \sum_{m=1}^{M} \sum_{i} \left( \partial_i \mathbf{s}_{\theta}(\mathbf{x}^{(m)}) + \frac{1}{2} \| \mathbf{s}_{\theta}(\mathbf{x}^{(m)}) \|^2 \right),
\]</div>
<p>where <span class="arithmatex">\(\partial_i \mathbf{s}_{\theta}(\mathbf{x}^{(m)}) = \frac{\partial}{\partial x_i} [\mathbf{s}_{\theta}(\mathbf{x})]_i = \frac{\partial^2}{\partial x_i^2} \log p(\mathbf{x})\)</span>. If the model for the score function is realized by a deep neural network, the trace operator can be difficult to compute, hence making the implicit score matching not scalable [40].</p>
<p>** Refenrence**
- Aapo Hyv¨arinen. Estimation of non-normalized statistical models by score matching. Journal
of Machine Learning Research (JMLR), 6(24):695–709, 2005. https://jmlr.org/papers/volume6/
hyvarinen05a/hyvarinen05a.pdf</p>
<h3 id="experiment">experiment<a class="headerlink" href="#experiment" title="Permanent link">&para;</a></h3>
<p>We use a mixture of guassian distribution for testing. But the results is not good.</p>
<p><img alt="alt text" src="../../../images/image-29.png" />
<img alt="pred" src="../../../images/image-28.png" /></p>
<p>It turns out that the derivative maganitude is near 0, but the trace is almost - 0.0004.</p>
<ul>
<li>optimizatioin 1
add regularization of <span class="arithmatex">\(E_x p_\theta(x)\)</span>.
Still not work, almost constant
<img alt="alt text" src="../../../images/image-30.png" /></li>
<li>optimization 2
output energy function not the <span class="arithmatex">\(p_\theta\)</span>, that is without a exponention.</li>
</ul>
<h2 id="siced-score-matching">Siced score matching<a class="headerlink" href="#siced-score-matching" title="Permanent link">&para;</a></h2>
<ul>
<li>optimization3
Finally, it is because that the network used ReLU, which is not enough for modeling the complex density function. Here we
change to Swish function instead. Now the iteration is much more stable and output the correct estimation of the ground truth density function (un-normalized)</li>
</ul>
<p><img alt="alt text" src="../../../images/image-31.png" /></p>
<h4 id="_7"><strong>核心思想</strong><a class="headerlink" href="#_7" title="Permanent link">&para;</a></h4>
<p><strong>Sliced Score Matching（切片分数匹配）</strong> 是一种用于高效估计数据分布梯度（score function）的方法，主要针对高维数据场景设计。其核心创新在于通过<strong>随机投影技术</strong>降低计算复杂度，避免直接计算高维Jacobian矩阵的迹（Trace），从而解决传统Implicit Score Matching的可扩展性问题。</p>
<p>直观的说，就是在任何的投影空间，两个function 能够近似，那么原本的两个function 也必然近似。这是一种比较重要的思想，可以在其他的问题中借鉴。同样的还有，如果两个function 的梯度处处相等，那么这两个function 也相等（差一个constant), 这些都是一种转化问题的技巧，值得学习。</p>
<h4 id="_8"><strong>数学定义</strong><a class="headerlink" href="#_8" title="Permanent link">&para;</a></h4>
<p><strong>1. Fisher 散度的定义</strong>
Fisher 散度衡量真实分布 $ p(x) $ 与模型分布 $ q_\theta(x) $ 的 score function 之间的差异：
$$
\mathcal{D}<em>{\text{Fisher}}(p | q</em>\theta) = \mathbb{E}<em>{p(x)} \left[ | \nabla_x \log p(x) - \nabla_x \log q</em>\theta(x) |^2 \right].
$$
最小化 Fisher 散度等价于让模型 score $ \nabla_x \log q_\theta(x) $ 逼近真实 score $ \nabla_x \log p(x) $。</p>
<p>通过 Stein 恒等式，ISM 将 Fisher 散度转化为以下目标函数：
$$
J_{\text{ISM}}(\theta) = \mathbb{E}<em>{p(x)} \left[ \operatorname{Tr}(\nabla_x \mathbf{s}</em>\theta(x)) + \frac{1}{2} | \mathbf{s}<em>\theta(x) |^2 \right],
$$
其中 $ \mathbf{s}</em>\theta(x) $ 是模型预测的 score function。
<strong>关键等式</strong>：当 $ \mathbf{s}<em>\theta(x) = \nabla_x \log p(x) $ 时，ISM 损失达到最小值，此时 $ \mathcal{D}</em>{\text{Fisher}} = 0 $。</p>
<p><strong>3. Sliced Score Matching 的动机</strong>
ISM 的瓶颈在于计算 Jacobian 矩阵的迹 $ \operatorname{Tr}(\nabla_x \mathbf{s}_\theta(x)) $，其复杂度为 $ O(d^2) <span class="arithmatex">\(（\)</span> d $ 为数据维度）。
<strong>核心思想</strong>：利用随机投影技术将迹的计算复杂度降至 $ O(d) $。</p>
<hr />
<p><strong>4. 随机投影近似迹</strong></p>
<p>根据 Hutchinson 迹估计器，任意矩阵 $ A \in \mathbb{R}^{d \times d} $ 的迹可表示为：</p>
<div class="arithmatex">\[
\operatorname{Tr}(A) = \mathbb{E}_{\mathbf{v} \sim \mathcal{N}(0, I)} \left[ \mathbf{v}^\top A \mathbf{v} \right],
\]</div>
<p>其中 $ \mathbf{v} $ 为标准正态分布的随机向量。
将此技术应用于 ISM 的迹项：</p>
<div class="arithmatex">\[
\operatorname{Tr}(\nabla_x \mathbf{s}_\theta(x)) = \mathbb{E}_{\mathbf{v}} \left[ \mathbf{v}^\top \nabla_x \mathbf{s}_\theta(x) \mathbf{v} \right].
\]</div>
<p><strong>5. Sliced Score Matching 的目标函数</strong></p>
<p>将 ISM 的迹项替换为随机投影近似，得到 SSM 的目标函数：</p>
<div class="arithmatex">\[
J_{\text{SSM}}(\theta) = \mathbb{E}_{p(x)} \mathbb{E}_{\mathbf{v}} \left[ \mathbf{v}^\top \nabla_x \mathbf{s}_\theta(x) \mathbf{v} + \frac{1}{2} \| \mathbf{s}_\theta(x) \|^2 \right].
\]</div>
<p><strong>蒙特卡洛近似</strong>：通过采样少量投影方向 $ {\mathbf{v}<em>k}</em>{k=1}^K $ 估计期望值：</p>
<div class="arithmatex">\[
J_{\text{SSM}}(\theta) \approx \frac{1}{N} \sum_{i=1}^N \left[ \frac{1}{K} \sum_{k=1}^K \mathbf{v}_k^\top \nabla_x \mathbf{s}_\theta(x_i) \mathbf{v}_k + \frac{1}{2} \| \mathbf{s}_\theta(x_i) \|^2 \right].
\]</div>
<hr />
<p><strong>6. 等价性证明</strong></p>
<p>当投影方向数量 $ K \to \infty $ 时，SSM 与 ISM 的目标函数等价：
$$
\lim_{K \to \infty} J_{\text{SSM}}(\theta) = J_{\text{ISM}}(\theta).
$$
因此，SSM 是 ISM 的高效近似，且在优化过程中等价于最小化 Fisher 散度。</p>
<hr />
<p><strong>7. 总结</strong></p>
<ul>
<li><strong>从 Fisher 散度到 SSM</strong> 的推导路径：
  $$
  \mathcal{D}<em>{\text{Fisher}} \xrightarrow{\text{Stein 恒等式}} J</em>{\text{ISM}} \xrightarrow{\text{随机投影近似}} J_{\text{SSM}}.
  $$</li>
<li><strong>SSM 的优势</strong>：
  将计算复杂度从 $ O(d^2) $ 降低至 $ O(Kd) $，适用于高维数据（如图像、文本）。</li>
<li>
<p><strong>适用场景</strong>：
  生成模型（如扩散模型）、无需显式概率密度的梯度估计。</p>
</li>
<li>
<p><strong>基础公式</strong>
   与传统Implicit Score Matching的损失函数类似，但引入随机投影向量 $ \mathbf{v} $：
   $$
   J_{\text{SSM}}(\theta) = \mathbb{E}<em>{p(\mathbf{x})} \mathbb{E}</em>{\mathbf{v} \sim \mathcal{N}(0,I)} \left[ \mathbf{v}^\top \nabla_{\mathbf{x}} \mathbf{s}<em>\theta(\mathbf{x}) \mathbf{v} + \frac{1}{2} | \mathbf{s}</em>\theta(\mathbf{x}) |^2 \right],
   $$
   其中：</p>
</li>
<li>$ \mathbf{v} $ 是服从标准正态分布的随机向量。</li>
<li>
<p>$ \mathbf{v}^\top \nabla_{\mathbf{x}} \mathbf{s}_\theta(\mathbf{x}) \mathbf{v} $ 表示Jacobian矩阵在随机方向 $ \mathbf{v} $ 上的投影。</p>
</li>
<li>
<p><strong>关键简化</strong>
   通过随机投影，将计算全Jacobian矩阵的迹 $ \operatorname{Tr}(\nabla_{\mathbf{x}}\mathbf{s}_\theta(\mathbf{x})) $ 转化为：</p>
</li>
</ul>
<p>$$
   \operatorname{Tr}(\nabla_{\mathbf{x}}\mathbf{s}<em>\theta(\mathbf{x})) = \mathbb{E}</em>{\mathbf{v}} \left[ \mathbf{v}^\top \nabla_{\mathbf{x}} \mathbf{s}_\theta(\mathbf{x}) \mathbf{v} \right].
   $$</p>
<p>这一技巧将复杂度从 $ O(d^2) $（直接计算Jacobian）降低到 $ O(d) $（仅需计算向量-矩阵-向量乘积）。</p>
<hr />
<h4 id="_9"><strong>优势与意义</strong><a class="headerlink" href="#_9" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>计算高效性</strong></li>
<li>避免显式计算高维Jacobian矩阵，尤其适合深度神经网络等参数化模型。</li>
<li>
<p>通过蒙特卡洛采样少量投影方向 $ \mathbf{v} $，即可近似真实迹值。</p>
</li>
<li>
<p><strong>理论保证</strong></p>
</li>
<li>在投影方向足够多时，损失函数与Implicit Score Matching等价。</li>
<li>
<p>保留了梯度匹配的一致性，即最小化损失等价于匹配真实score function。</p>
</li>
<li>
<p><strong>适用场景</strong></p>
</li>
<li>高维数据（如图像、文本）。</li>
<li>生成模型（如扩散模型）、密度估计任务。</li>
</ol>
<hr />
<h4 id="_10"><strong>示例说明</strong><a class="headerlink" href="#_10" title="Permanent link">&para;</a></h4>
<p>假设数据维度 $ d=1000 $，直接计算Jacobian矩阵的迹需要 $ 10^6 $ 次操作，而Sliced Score Matching仅需：
1. 采样随机向量 $ \mathbf{v} \in \mathbb{R}^{1000} $。
2. 计算 $ \mathbf{v}^\top \nabla_{\mathbf{x}} \mathbf{s}_\theta(\mathbf{x}) \mathbf{v} $，仅需 $ O(1000) $ 次操作。
3. 对多个 $ \mathbf{v} $ 取平均，近似迹值。</p>
<hr />
<h4 id="_11"><strong>对比其他方法</strong><a class="headerlink" href="#_11" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>计算复杂度</strong></th>
<th><strong>主要挑战</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Explicit Score Matching</td>
<td>$ O(d) $</td>
<td>依赖真实score function，通常未知</td>
</tr>
<tr>
<td>Implicit Score Matching</td>
<td>$ O(d^2) $</td>
<td>计算Jacobian迹的高复杂度</td>
</tr>
<tr>
<td>Sliced Score Matching</td>
<td>$ O(d) $</td>
<td>需平衡投影方向数量与估计精度</td>
</tr>
</tbody>
</table>
<hr />
<h4 id="_12"><strong>代码实现（伪代码）</strong><a class="headerlink" href="#_12" title="Permanent link">&para;</a></h4>
<pre><code class="language-python">import torch

def sliced_score_matching_loss(model, data, num_projections=10):
    &quot;&quot;&quot;
    model: 预测score function的神经网络
    data: 输入数据样本
    num_projections: 随机投影方向的数量
    &quot;&quot;&quot;
    scores = model(data)  # 模型预测的score [batch_size, d]
    loss = 0.5 * torch.mean(torch.sum(scores**2, dim=1))  # 1/2 ||s_θ(x)||^2

    for _ in range(num_projections):
        v = torch.randn_like(data)  # 随机投影向量 [batch_size, d]
        v.requires_grad_(True)

        # 计算 v^T ∇s_θ(x) v
        v_scores = torch.sum(v * scores, dim=1)  # [batch_size]
        jvp = torch.autograd.grad(
            outputs=v_scores, inputs=data,
            grad_outputs=torch.ones_like(v_scores),
            create_graph=True
        )[0]  # ∇(v^T s_θ(x)) = v^T ∇s_θ(x)
        trace_estimate = torch.sum(v * jvp, dim=1)  # v^T ∇s_θ(x) v

        loss += torch.mean(trace_estimate) / num_projections

    return loss
</code></pre>
<h3 id="experiment_1">experiment<a class="headerlink" href="#experiment_1" title="Permanent link">&para;</a></h3>
<p>We also tried with the mixture og gaussian distribution, it performs well.</p>
<p><img alt="alt text" src="../../../images/image-32.png" /></p>
<p>See notebook on  <strong>experiment/implicit_score_matching.ipynb</strong>.</p>
<h2 id="denoising-score-matching">Denoising Score Matching<a class="headerlink" href="#denoising-score-matching" title="Permanent link">&para;</a></h2>
<p>Generative models aim to learn the underlying distribution of data, allowing us to generate new samples. <strong>Denoising Score Matching (DSM)</strong> is a powerful technique in this domain, leveraging noise to simplify training. In this blog, we'll break down DSM's intuition, math, training, and sampling, and extend it to multi-scale and continuous noise settings. We’ll emphasize <em>why</em> each step matters and <em>how</em> the pieces connect.</p>
<h3 id="1-intuition-why-add-noise">1. Intuition: Why Add Noise?<a class="headerlink" href="#1-intuition-why-add-noise" title="Permanent link">&para;</a></h3>
<h4 id="what-is-a-score">What is a "Score"?<a class="headerlink" href="#what-is-a-score" title="Permanent link">&para;</a></h4>
<p>The <strong>score</strong> of a probability distribution $ p(x) $ is its gradient of the log-density:
$$ \nabla_x \log p(x) $$
Imagine you’re hiking on a landscape where valleys represent regions of high data density (e.g., realistic images). The score tells you the direction to move <em>uphill</em> toward higher density (i.e., toward realistic data).</p>
<h4 id="why-traditional-score-matching-fails">Why Traditional Score Matching Fails<a class="headerlink" href="#why-traditional-score-matching-fails" title="Permanent link">&para;</a></h4>
<p>For high-dimensional data (e.g., images), most of the space is empty—data lies on a thin "manifold." Traditional score matching struggles because:
1. <strong>Computational cost</strong>: Estimating gradients in high dimensions is expensive.
2. <strong>Sparse signals</strong>: The score is undefined or noisy in empty regions far from the data manifold.</p>
<h4 id="the-noise-solution">The Noise Solution<a class="headerlink" href="#the-noise-solution" title="Permanent link">&para;</a></h4>
<p>By adding Gaussian noise to data points, we "smooth" the distribution, filling empty regions with a blurry haze of noisy data. This makes the score easier to estimate everywhere. Think of it like turning a spiky mountain range into rolling hills—easier to navigate!</p>
<p><strong>Key Insight</strong>:
Instead of learning $ \nabla_x \log p(x) $ directly (hard!), learn to <em>denoise</em> perturbed data. The denoising direction aligns with the score of the <em>noise-augmented</em> distribution.</p>
<h3 id="2-derivation-connecting-noise-to-the-score">2. Derivation: Connecting Noise to the Score<a class="headerlink" href="#2-derivation-connecting-noise-to-the-score" title="Permanent link">&para;</a></h3>
<h4 id="step-1-define-the-noisy-distribution">Step 1: Define the Noisy Distribution<a class="headerlink" href="#step-1-define-the-noisy-distribution" title="Permanent link">&para;</a></h4>
<p>Corrupt a data point $ x $ with Gaussian noise:</p>
<div class="arithmatex">\[ \tilde{x} = x + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I) \]</div>
<p>The conditional distribution is:</p>
<div class="arithmatex">\[ q(\tilde{x}|x) = \mathcal{N}(x, \sigma^2 I) \]</div>
<p>thus,</p>
<div class="arithmatex">\[
q_\sigma(\tilde{\mathbf{x}}) = \int q(\tilde{\mathbf{x}} \mid \mathbf{x}) p_{\text{data}}(\mathbf{x}) \, d\mathbf{x},
\]</div>
<p>where $ q(\tilde{\mathbf{x}} \mid \mathbf{x}) = \mathcal{N}(\tilde{\mathbf{x}}; \mathbf{x}, \sigma^2 \mathbf{I}) $ is a Gaussian distribution centered at $ \mathbf{x} $ with variance $ \sigma^2 $. This represents a <strong>convolution</strong> of $ p_{\text{data}}(\mathbf{x}) $ with a Gaussian kernel.</p>
<h4 id="1-small-noise-sigma-to-0">1. Small Noise $ \sigma \to 0 $<a class="headerlink" href="#1-small-noise-sigma-to-0" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Behavior of $ q(\tilde{\mathbf{x}} \mid \mathbf{x}) $:</strong></li>
</ul>
<p>The Gaussian becomes a Dirac delta function:</p>
<p>$$
  q_\sigma(\tilde{\mathbf{x}} \mid \mathbf{x}) \to \delta(\tilde{\mathbf{x}} - \mathbf{x}).
  $$</p>
<ul>
<li><strong>Effect on $q(\tilde{\mathbf{x}}) $:</strong></li>
</ul>
<p>The integral simplifies to $p_{\text{data}}(\tilde{\mathbf{x}}) $, preserving the original distribution:</p>
<p>$$
  q_\sigma(\tilde{\mathbf{x}}) \approx p_{\text{data}}(\tilde{\mathbf{x}}).
  $$
- <strong>Interpretation:</strong> Minimal blurring; the perturbed distribution matches the original data distribution.</p>
<h4 id="2-moderate-noise-sigma-0">2. Moderate Noise $ \sigma &gt; 0 $<a class="headerlink" href="#2-moderate-noise-sigma-0" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>Behavior of $q(\tilde{\mathbf{x}} \mid \mathbf{x}) $:</strong>
  The Gaussian acts as a smoothing kernel with width proportional to $ \sigma $.</p>
</li>
<li>
<p><strong>Effect on $q(\tilde{\mathbf{x}}) $:</strong></p>
</li>
</ul>
<p>The convolution introduces controlled blurring, creating a smoothed version of $ p_{\text{data}}(\mathbf{x}) $. Fine details are averaged, but the global structure remains recognizable.</p>
<ul>
<li><strong>Interpretation:</strong> Useful for regularization or generating "softened" data samples.</li>
</ul>
<h4 id="3-large-noise-sigma-to-infty">3. Large Noise $\sigma \to \infty $**<a class="headerlink" href="#3-large-noise-sigma-to-infty" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>Behavior of $ q_\sigma(\tilde{\mathbf{x}} \mid \mathbf{x}) $:</strong>
  The Gaussian becomes extremely wide and flat, approximating a uniform distribution over the domain.</p>
</li>
<li>
<p><strong>Effect on $ q_\sigma(\tilde{\mathbf{x}}) $:</strong></p>
</li>
</ul>
<p>The integral averages $ p_{\text{data}}(\mathbf{x}) $ over a large region, erasing fine structure. If $ p_{\text{data}}(\mathbf{x}) $ is bounded, $ q(\tilde{\mathbf{x}}) $ approaches a uniform distribution; otherwise, it becomes a broad Gaussian.</p>
<ul>
<li><strong>Interpretation:</strong> Severe distortion; the original distribution is lost.</li>
</ul>
<h4 id="step-2-score-of-the-noisy-distribution">Step 2: Score of the Noisy Distribution<a class="headerlink" href="#step-2-score-of-the-noisy-distribution" title="Permanent link">&para;</a></h4>
<p>The score of $ q(\tilde{x}|x) $ is:</p>
<div class="arithmatex">\[ \nabla_{\tilde{x}} \log q(\tilde{x}|x) = \frac{x - \tilde{x}}{\sigma^2} \]</div>
<p><strong>Why?</strong>
For a Gaussian $ \mathcal{N}(x, \sigma^2 I) $, the gradient of the log-density with respect to $ \tilde{x} $ points toward the mean $ x $. The term $ (x - \tilde{x})/\sigma^2 $ is the "denoising direction" that corrects $ \tilde{x} $ back to $ x $.</p>
<h4 id="step-3-the-dsm-objective">Step 3: The DSM Objective<a class="headerlink" href="#step-3-the-dsm-objective" title="Permanent link">&para;</a></h4>
<p>Train a model $ s_\theta(\tilde{x}) $ to match this score:</p>
<div class="arithmatex">\[ J(\theta) = \mathbb{E}_{q(\tilde{x},x)}\left[ \| s_\theta(\tilde{x}) - \frac{x - \tilde{x}}{\sigma^2} \|^2 \right] \]</div>
<p><strong>Why This Works</strong>:</p>
<p>Minimizing this loss forces $ s_\theta(\tilde{x}) $ to approximate $ \nabla_{\tilde{x}} \log q(\tilde{x}) $, the score of the <em>marginal</em> noisy distribution $ q(\tilde{x}) = \int q(\tilde{x}|x)p_{\text{data}}(x)dx $,  which has been proved in the above section.
As illustrated above, this is equivalent to learning the score of the <em>true</em> data distribution $ p_{\text{data}}(x) $ as $ \sigma \to 0 $.</p>
<h3 id="3-training-process-step-by-step">3. Training Process: Step-by-Step<a class="headerlink" href="#3-training-process-step-by-step" title="Permanent link">&para;</a></h3>
<h4 id="step-1-add-noise-to-data">Step 1: Add Noise to Data<a class="headerlink" href="#step-1-add-noise-to-data" title="Permanent link">&para;</a></h4>
<p>For each clean data point $ x $, generate a noisy version:</p>
<div class="arithmatex">\[ \tilde{x} = x + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2 I) \]</div>
<p><strong>Why Add Noise?</strong>
- Creates "easier" training examples by spreading data into empty regions.
- Teaches the model to handle perturbations, improving robustness.</p>
<h4 id="step-2-compute-the-loss">Step 2: Compute the Loss<a class="headerlink" href="#step-2-compute-the-loss" title="Permanent link">&para;</a></h4>
<p>The DSM loss simplifies to:</p>
<div class="arithmatex">\[ \mathcal{L} = \frac{1}{N} \sum_{i=1}^N \| s_\theta(\tilde{x}_i) - \frac{x_i - \tilde{x}_i}{\sigma^2} \|^2 \]</div>
<p><strong>Interpretation</strong>:</p>
<p>The model learns to predict the vector $ (x_i - \tilde{x}_i)/\sigma^2 $, which points from the noisy sample $ \tilde{x}_i $ back to the clean $ x_i $. This is equivalent to estimating the score of the noisy distribution.</p>
<h4 id="step-3-gradient-descent">Step 3: Gradient Descent<a class="headerlink" href="#step-3-gradient-descent" title="Permanent link">&para;</a></h4>
<p>Update model parameters $ \theta $ to minimize $ \mathcal{L} $.</p>
<p><img alt="progress of denoising" src="../../../images/image-33.png" /></p>
<p><strong>Practical Tip</strong>:</p>
<p>Scale the loss by $ \sigma^2 $ to balance learning across noise levels (critical for multi-scale training).</p>
<p><strong>多尺度 sigmas (σ schedules)</strong></p>
<ul>
<li>
<p>为了增强模型的鲁棒性，可以为噪声设置一系列值，从较小的噪声到较大的噪声，形成一个<strong>噪声尺度</strong> 。</p>
</li>
<li>
<p><strong>公式</strong> : 通常构造一个对数间隔的序列，例如：</p>
</li>
</ul>
<div class="arithmatex">\[
\sigma_i = \sigma_{\text{min}} \times \left(\frac{\sigma_{\text{max}}}{\sigma_{\text{min}}}\right)^{i / (N-1)}
\]</div>
<pre><code>- `σ_min`: 最小噪声强度。

- `σ_max`: 最大噪声强度。

- `N`: 噪声尺度的数量。
</code></pre>
<ul>
<li>
<p><strong>常用设置</strong> :</p>
<ul>
<li>
<p>对于图像数据，<code>σ_min</code> 通常是 <code>0.01</code> 或 <code>0.05</code>。</p>
</li>
<li>
<p><code>σ_max</code> 通常取 <code>0.5</code> 或 <code>1.0</code>。</p>
</li>
<li>
<p><code>N</code> 的典型值为 <code>10</code> 到 <code>50</code>。</p>
</li>
</ul>
</li>
<li>
<p><strong>优点</strong> : 多尺度噪声更适合复杂分布，可以建模从低噪声到高噪声的各种情况。</p>
</li>
<li>
<p><strong>缺点</strong> : 训练和实现更复杂，需要动态调整噪声水平。</p>
</li>
</ul>
<h3 id="4-sampling-with-langevin-dynamics">4. Sampling with Langevin Dynamics<a class="headerlink" href="#4-sampling-with-langevin-dynamics" title="Permanent link">&para;</a></h3>
<p>Once trained, we use <strong>Langevin dynamics</strong> to generate samples by "walking" along the learned score.</p>
<h4 id="the-update-rule">The Update Rule:<a class="headerlink" href="#the-update-rule" title="Permanent link">&para;</a></h4>
<div class="arithmatex">\[ x_{t+1} = x_t + \epsilon \cdot s_\theta(x_t) + \sqrt{2\epsilon} \cdot z_t, \quad z_t \sim \mathcal{N}(0, I) \]</div>
<p><strong>Breaking It Down</strong>:</p>
<ol>
<li><strong>Score term $ \epsilon \cdot s_\theta(x_t) $</strong>: Guides $ x_t $ toward high-density regions (denoising).</li>
<li><strong>Noise term $ \sqrt{2\epsilon} \cdot z_t $</strong>: Adds randomness to escape local minima and explore the distribution.</li>
</ol>
<p><strong>Why This Works</strong>:
Langevin dynamics is a Markov Chain Monte Carlo (MCMC) method that uses the score to perform gradient ascent on $ \log p(x) $. The noise ensures ergodicity, allowing the chain to converge to the true distribution.</p>
<h4 id="analogy">Analogy:<a class="headerlink" href="#analogy" title="Permanent link">&para;</a></h4>
<p>Imagine rolling a marble on a bumpy surface (the data landscape). The score tilts the surface to guide the marble toward valleys (data points), while the noise gives it occasional kicks to explore new areas.</p>
<h3 id="5-multi-scale-noise-training">5. Multi-Scale Noise Training<a class="headerlink" href="#5-multi-scale-noise-training" title="Permanent link">&para;</a></h3>
<h4 id="why-multiple-noise-scales">Why Multiple Noise Scales?<a class="headerlink" href="#why-multiple-noise-scales" title="Permanent link">&para;</a></h4>
<p><strong>Key Challenges in Training SBMs/EBMs</strong></p>
<ul>
<li><strong>Undefined Score Function Off the Data Manifold</strong> : Under the manifold hypothesis, data resides on a low-dimensional manifold embedded in a high-dimensional ambient space. The score function $ \nabla_x \log p(x) $, which requires gradients to be defined everywhere in the ambient space, becomes ill-defined outside the manifold.</li>
</ul>
<p><strong>Problem</strong>: Score estimation fails in regions irrelevant to the data, destabilizing training and generation.</p>
<p><strong>Implication</strong>: Score estimation fails in regions irrelevant to the data, destabilizing training and generation.</p>
<ul>
<li><strong>Sparse Data in Low-Density Regions</strong></li>
</ul>
<p><strong>Problem</strong>: Real-world datasets often lack sufficient samples in low-density areas (e.g., transitions between classes or rare features). This sparsity makes it difficult to reliably estimate the score function in these regions.</p>
<p><strong>Implication</strong>: Poor score approximation leads to artifacts, mode collapse, or unrealistic interpolations.</p>
<ul>
<li><strong>Degradation of Mixing Distribution Coefficients</strong></li>
</ul>
<p><strong>Problem</strong>: In near-zero density regions (e.g., far from the manifold), the coefficients (weights) of the mixing distribution—used to model complex data—vanish or become negligible.</p>
<p><strong>Implication</strong>: The model loses expressive power in these regions, exacerbating mode collapse and limiting diversity in generated samples.</p>
<p>We use multi-scale noise pertubation could help address these challenges.</p>
<p>Real-world data (e.g., images) has structure at multiple resolutions:
- <strong>Low noise (small $ \sigma $)</strong>: Captures fine details (e.g., textures).
- <strong>High noise (large $ \sigma $)</strong>: Captures coarse structure (e.g., shapes).</p>
<p>Training with a single $ \sigma $ limits the model’s ability to generalize across scales.</p>
<h4 id="training-process">Training Process:<a class="headerlink" href="#training-process" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>Noise Sampling</strong>: For each batch, randomly pick $ \sigma_i $ from a set $ {\sigma_1, ..., \sigma_L} $.</li>
<li><strong>Loss Adjustment</strong>: Scale the loss by $ \sigma_i^2 $ to prevent larger $ \sigma $ from dominating:</li>
</ol>
<p>$$ \mathcal{L} = \frac{1}{L} \sum_{i=1}^L \mathbb{E}\left[ \sigma_i^2 | s_\theta(\tilde{x}, \sigma_i) - \frac{x - \tilde{x}}{\sigma_i^2} |^2 \right] $$</p>
<h4 id="sampling">Sampling:<a class="headerlink" href="#sampling" title="Permanent link">&para;</a></h4>
<p>Use a decreasing sequence $ \sigma_1 &gt; \sigma_2 &gt; ... &gt; \sigma_L $ during Langevin dynamics:</p>
<ol>
<li>Start with high noise to capture coarse structure.</li>
<li>Gradually reduce noise to refine details.</li>
</ol>
<p><strong>Analogy</strong>:
Like sketching a painting—first outline shapes (high noise), then add details (low noise).</p>
<h3 id="6-continuous-noise-levels">6. Continuous Noise Levels<a class="headerlink" href="#6-continuous-noise-levels" title="Permanent link">&para;</a></h3>
<h4 id="why-go-continuous">Why Go Continuous?<a class="headerlink" href="#why-go-continuous" title="Permanent link">&para;</a></h4>
<p>Discrete noise scales are rigid and computationally costly for large $ L $. A continuous approach:
- Smoothly interpolates between noise levels.
- Connects to differential equations for efficient sampling.</p>
<h4 id="training">Training:<a class="headerlink" href="#training" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>Noise Sampling</strong>: Sample $ t \sim \mathcal{U}(0,1) $, compute $ \sigma(t) $ (e.g., $ \sigma(t) = \sigma_{\text{min}} + t(\sigma_{\text{max}} - \sigma_{\text{min}}) $).</li>
<li><strong>Condition the Model</strong>: Feed $ t $ to $ s_\theta $ via time embeddings (e.g., sinusoidal features).</li>
</ol>
<h4 id="sampling-with-stochastic-differential-equations-sdes">Sampling with Stochastic Differential Equations (SDEs)<a class="headerlink" href="#sampling-with-stochastic-differential-equations-sdes" title="Permanent link">&para;</a></h4>
<p>The continuous noise process can be described as an SDE:</p>
<div class="arithmatex">\[ dx = s_\theta(x, t) dt + \sqrt{2\sigma(t)} dw \]</div>
<p><strong>Intuition</strong>:
This generalizes Langevin dynamics to infinitesimal steps. The term $ s_\theta(x,t)dt $ is the deterministic drift (denoising), and $ \sqrt{2\sigma(t)}dw $ is stochastic diffusion (noise).</p>
<p><strong>Solving the SDE</strong>:
Use numerical solvers like Euler-Maruyama:</p>
<div class="arithmatex">\[ x_{t+1} = x_t + s_\theta(x_t, t) \Delta t + \sqrt{2\sigma(t) \Delta t} \, z_t \]</div>
<p>This is equivalent to Langevin dynamics with time-dependent noise.</p>
<h3 id="7-why-does-dsm-work-a-unified-view">7. Why Does DSM Work? A Unified View<a class="headerlink" href="#7-why-does-dsm-work-a-unified-view" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Avoids Density Estimation</strong>: Models gradients instead of $ p(x) $, bypassing normalization constraints.</li>
<li><strong>Exploits Noise</strong>: Smoothens the data manifold, making score estimation tractable.</li>
<li><strong>Connects to Diffusion Models</strong>: DSM is the backbone of diffusion models (e.g., DDPM, Score-SDE), where noise addition/removal is formalized across timesteps.</li>
</ol>
<p><strong>Comparison to GANs/VAEs</strong>:
- <strong>No adversarial training</strong>: More stable than GANs.
- <strong>No latent space bottlenecks</strong>: Richer expressivity than VAEs.</p>
<h3 id="conclusion">Conclusion<a class="headerlink" href="#conclusion" title="Permanent link">&para;</a></h3>
<p>Denoising Score Matching elegantly bridges noise and geometry to learn data distributions. By progressively corrupting and denoising data, it captures multi-scale structure and enables efficient sampling via SDEs. This framework powers cutting-edge generative models, offering flexibility, stability, and scalability.</p>
<p><strong>Further Reading</strong>:
- <a href="https://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf">Score Matching</a> (Hyvärinen, 2005)
- <a href="https://www.iro.umontreal.ca/~vincentp/Publications/denoising_score_matching_AISTATS2011.pdf">Denoising Score Matching</a> (Vincent, 2011)
- <a href="https://arxiv.org/abs/2011.13456">Score-Based SDEs</a> (Song et al., 2021)
- <a href="https://arxiv.org/pdf/1907.05600">Generative Modeling by Estimating Gradients of the Data Distribution</a> (YangSong, 2019)</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "toc.integrate"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.60a45f97.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>