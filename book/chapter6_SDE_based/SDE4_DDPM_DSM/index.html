
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.50">
    
    
      
        <title>Stochastic Differential Equation for DDPM and DSM - Generative AI: From Start to Surrender</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.a40c8224.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#stochastic-differential-equation-for-ddpm-and-dsm" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Generative AI: From Start to Surrender" class="md-header__button md-logo" aria-label="Generative AI: From Start to Surrender" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Generative AI: From Start to Surrender
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Stochastic Differential Equation for DDPM and DSM
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter1_Introduction/1.1terminology/" class="md-tabs__link">
          
  
  Introduction

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter2_VAE/2.1introduction/" class="md-tabs__link">
          
  
  VAE

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter3_GAN/3.1from_gan_to_stylegan/paper/" class="md-tabs__link">
          
  
  GANs

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter4_diffusion/4.1introduction/" class="md-tabs__link">
          
  
  Diffusion

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter5_generation_theory/5.1MLE/" class="md-tabs__link">
          
  
  Generation Theory

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter6_ebm/introduction/" class="md-tabs__link">
          
  
  Energy Based Models

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Generative AI: From Start to Surrender" class="md-nav__button md-logo" aria-label="Generative AI: From Start to Surrender" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Generative AI: From Start to Surrender
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Introduction
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter1_Introduction/1.1terminology/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Terms
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter1_Introduction/1.2fourier_transform/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fourier Transform
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter1_Introduction/1.3signal_processing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Signal Processing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter1_Introduction/1.4statistics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Statistics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter1_Introduction/1.5SDE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SDE
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter1_Introduction/tutorials/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tutorials
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    VAE
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            VAE
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter2_VAE/2.1introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    GANs
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            GANs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter3_GAN/3.1from_gan_to_stylegan/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    From GAN to PGGAN
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter3_GAN/3.3stylegan/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StyleGAN
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter3_GAN/3.4stylegan2/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StyleGAN2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter3_GAN/3.5stylegan3/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StyleGAN3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter3_GAN/3.6styleganT/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StyleGAN T
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter3_GAN/3.7R3Gan/paper/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    R3GAN
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Diffusion
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Diffusion
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter4_diffusion/4.1introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter4_diffusion/4.2DDPM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DDPM
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Generation Theory
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Generation Theory
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter5_generation_theory/5.1MLE/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Maximal Likelihood
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter5_generation_theory/manifold_hypothesis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Manifold Hypothesis
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Energy Based Models
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Energy Based Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter6_ebm/introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter6_ebm/score_function/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Score Function
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter6_ebm/sampling/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sampling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter6_ebm/cd/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contrastive Divergence
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter6_ebm/score_matching/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Score Matching
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="stochastic-differential-equation-for-ddpm-and-dsm">Stochastic Differential Equation for DDPM and DSM<a class="headerlink" href="#stochastic-differential-equation-for-ddpm-and-dsm" title="Permanent link">&para;</a></h1>
<p>In this section, we show how connect SDE with the denoising defussion probabilistic model and
the denoising score matching.</p>
<h2 id="1-autocorrelation">1. Autocorrelation<a class="headerlink" href="#1-autocorrelation" title="Permanent link">&para;</a></h2>
<p>In the context of the excerpt, the term “autocorrelation function” refers to the covariance</p>
<div class="arithmatex">\[
 \mathbb{E}[\xi(t)\,\xi(t')]\quad
\]</div>
<p>of the noise process <span class="arithmatex">\(\xi(\cdot)\)</span> evaluated at two different times <span class="arithmatex">\(t\)</span> and <span class="arithmatex">\(t'\)</span>. For an
idealized <strong>white‐noise</strong>  process (as in standard Brownian motion), this autocorrelation is
taken to be a <strong>Dirac delta function</strong> :</p>
<div class="arithmatex">\[
 \mathbb{E}[\xi(t)\,\xi(t')] \;=\;\delta(t - t').
\]</div>
<p>Intuitively, this means that <span class="arithmatex">\(\xi(t)\)</span> is “uncorrelated” with itself at any times <span class="arithmatex">\(t \neq t'\)</span>,
and all of its correlation is concentrated at the single point <span class="arithmatex">\(t = t'\)</span>. In more precise terms,
the Dirac delta is not a function in the usual sense but a distribution, capturing the idea that
white noise has no memory or persistence in time—any two distinct instants are independent,
but at the same instant the variance (or “intensity”) is infinite in such a way that it integrates
to a finite value over an infinitesimal interval.</p>
<h2 id="2-forward-diffusion">2. Forward Diffusion<a class="headerlink" href="#2-forward-diffusion" title="Permanent link">&para;</a></h2>
<p>Suppose we have the <em>forward</em> SDE(1)</p>
<div class="arithmatex">\[
 \tag{1}
dx \;=\; f(x,t)\,dt \;+\; g(t)\,dW_t,
\]</div>
<p>where <span class="arithmatex">\(W_t\)</span> is a standard Wiener process, and <span class="arithmatex">\(g(t)\)</span> is (for simplicity) taken to be spatially
constant but possibly time‐dependent. Let <span class="arithmatex">\(p_t(x)\)</span> be the probability density of <span class="arithmatex">\(x_t\)</span> at
time <span class="arithmatex">\(t\)</span>.</p>
<h2 id="3-reverse-diffusion">3. Reverse Diffusion<a class="headerlink" href="#3-reverse-diffusion" title="Permanent link">&para;</a></h2>
<p>Suppose <span class="arithmatex">\(p_t(x)\)</span> is the density metric for <span class="arithmatex">\(x\)</span> at time <span class="arithmatex">\(t\)</span>, then by the Fokker-Planck Equation, we have</p>
<div class="arithmatex">\[
\tag{2}
 dx \;=\;
\bigl[f(x,t)\;-\;g(t)^2\,\nabla_x \log p_{t}(x)\bigr]\;dt \;+\; g(t)\,d\overline{W}_t,
\]</div>
<p>In the following, we give more details why this backward diffusion holds for the density function
<span class="arithmatex">\(p_t(x)\)</span>.</p>
<h3 id="fokkerplanck-forward-kolmogorov-equation">Fokker–Planck (Forward Kolmogorov) Equation<a class="headerlink" href="#fokkerplanck-forward-kolmogorov-equation" title="Permanent link">&para;</a></h3>
<p>The density <span class="arithmatex">\(p_t(x)\)</span> satisfies the Fokker–Planck (or forward‐Kolmogorov) PDE</p>
<p>$$\tag{3}
 \frac{\partial p_t(x)}{\partial t}
\;=\;
-\nabla!\cdot!\bigl[f(x,t)\,p_t(x)\bigr]
\;+\;
\frac{1}{2}\,g(t)^2\,\Delta p_t(x),
$$
where <span class="arithmatex">\(\Delta\)</span> is the Laplacian in <span class="arithmatex">\(x\)</span>.</p>
<p><strong>NOTE</strong></p>
<blockquote>
<p>In <strong>one dimension</strong> , the Laplacian simplifies to:</p>
<p>$$
 \Delta p_t(x) = \frac{\partial^2 p_t(x)}{\partial x^2}
 $$</p>
<p>In <strong>higher dimensions</strong> , the Laplacian is given by:</p>
<p>$$
 \Delta p_t(x) = \sum_{i=1}^{d} \frac{\partial^2 p_t(x)}{\partial x_i^2}
 $$</p>
<p>where <span class="arithmatex">\(d\)</span> is the dimension of the space. Thus, in the context of the <strong>Fokker-Planck equation</strong>, the Laplacian term governs the <strong>diffusive spreading</strong>  of the probability density due to the noise term <span class="arithmatex">\(g(t) dW_t\)</span> in the associated SDE.</p>
</blockquote>
<p>Now we prove that the reverse diffusion formula</p>
<h3 id="timereversal-strategy">Time‐Reversal Strategy<a class="headerlink" href="#timereversal-strategy" title="Permanent link">&para;</a></h3>
<p>We want an SDE whose sample paths “run backward” in time with the <em>same</em> distributions as <span class="arithmatex">\((x_t)\)</span>
 running forward. Concretely, define</p>
<div class="arithmatex">\[
 \hat{X}_s \;=\; x_{T - s}
\quad
\text{for } 0 \le s \le T.
\]</div>
<p>That is, <span class="arithmatex">\(\hat{X}_0 = x_{T}\)</span>, <span class="arithmatex">\(\hat{X}_T = x_0\)</span>. Our goal is to find a stochastic differential
equation of the form</p>
<div class="arithmatex">\[
 \tag{4}
d\hat{X}_s
\;=\; \hat{f}\bigl(\hat{X}_s, s\bigr)\,ds
\;+\;
g(T-s)\,d\overline{W}_s
\]</div>
<p>(where <span class="arithmatex">\(\overline{W}_s\)</span> is another Wiener process) such that <span class="arithmatex">\(\hat{X}_s\)</span> has the <em>same</em> distribution as <span class="arithmatex">\(x_{T-s}\)</span>.In other words, if <span class="arithmatex">\(q_s(x)\)</span> denotes the density of <span class="arithmatex">\(\hat{X}_s\)</span>, we want <span class="arithmatex">\(q_s(x) = p_{T-s}(x)\)</span>. We must determine the drift <span class="arithmatex">\(\hat{f}(x,s)\)</span>.</p>
<h3 id="matching-probability-densities-via-fokkerplanck">Matching Probability Densities via Fokker–Planck<a class="headerlink" href="#matching-probability-densities-via-fokkerplanck" title="Permanent link">&para;</a></h3>
<h4 id="pde-for-the-reverse-process">PDE for the reverse process<a class="headerlink" href="#pde-for-the-reverse-process" title="Permanent link">&para;</a></h4>
<p>If <span class="arithmatex">\(\hat{X}_s\)</span> satisfies</p>
<div class="arithmatex">\[
 d\hat{X}_s
= \hat{f}(\hat{X}_s, s)\,ds
\;+\;
g(T-s)\,d\overline{W}_s,
\]</div>
<p>then its density <span class="arithmatex">\(q_s(x)\)</span> obeys</p>
<div class="arithmatex">\[
 \frac{\partial q_s(x)}{\partial s}
\;=\;
-\nabla \cdot \bigl[\hat{f}(x, s)\;q_s(x)\bigr]
\;+\;
\frac{1}{2}\,g(T-s)^2\;\Delta\,q_s(x).
\]</div>
<p>We require <span class="arithmatex">\(q_s(x) = p_{T-s}(x)\)</span>. Substitute <span class="arithmatex">\(q_s(x) = p_{T-s}(x)\)</span> into the PDE. Note that</p>
<div class="arithmatex">\[
 \frac{\partial}{\partial s}\,p_{T-s}(x)
\;=\;
-\,\frac{\partial}{\partial t}\,p_{t}(x)\Bigr|_{\,t=T-s}.
\]</div>
<p>Hence</p>
<div class="arithmatex">\[
 \frac{\partial q_s}{\partial s}(x)
\;=\;
-\,\frac{\partial}{\partial t}\,p_{t}(x)\Bigr|_{t=T-s}.
\]</div>
<p>But from the <em>forward</em> Fokker–Planck equation, we know</p>
<div class="arithmatex">\[
 \frac{\partial}{\partial t}\,p_{t}(x)
\;=\;
-\nabla\!\cdot\!\bigl[f(x,t)\,p_t(x)\bigr]
+\tfrac{1}{2}\,g(t)^2\,\Delta\,p_t(x).
\]</div>
<p>Putting <span class="arithmatex">\(t = T-s\)</span> and changing the sign, we get</p>
<div class="arithmatex">\[
 \frac{\partial q_s}{\partial s}(x)
\;=\;
+\nabla\!\cdot\!\bigl[f(x,T-s)\,p_{T-s}(x)\bigr]
\;-\;
\tfrac{1}{2}\,g(T-s)^2\,\Delta\,p_{T-s}(x).
\]</div>
<p>Identifying the reversed drift <span class="arithmatex">\(\hat{f}\)</span> Meanwhile, from the PDE for <span class="arithmatex">\(q_s\)</span> we also have</p>
<div class="arithmatex">\[
\frac{\partial q_s}{\partial s}(x)
\;=\;
-\,\nabla\!\cdot\!\bigl[\hat{f}(x,s)\,q_s(x)\bigr]
\;+\;
\tfrac12\,g(T-s)^2\,\Delta\,q_s(x).
\]</div>
<p>Because <span class="arithmatex">\(q_s(x) = p_{T-s}(x)\)</span>, we can equate the two expressions. Matching terms gives</p>
<div class="arithmatex">\[
\nabla\!\cdot\!\bigl[\hat{f}(x,s)\,p_{T-s}(x)\bigr]
\;=\;
-\,\nabla\!\cdot\!\bigl[f(x,T-s)\,p_{T-s}(x)\bigr]
\;+\;
g(T-s)^2 \,\nabla \!\cdot\!\bigl[\tfrac12\,p_{T-s}(x)\bigr],
\]</div>
<p>where we used that</p>
<div class="arithmatex">\[
\Delta\,p_{T-s}(x) = \nabla \cdot (\nabla\,p_{T-s}(x))
\]</div>
<p>Rearrange:</p>
<div class="arithmatex">\[
 \nabla\!\cdot\!\Bigl[\hat{f}(x,s)\,p_{T-s}(x)\Bigr]
\;=\;
-\nabla\!\cdot\!\Bigl[f(x,T-s)\,p_{T-s}(x)\Bigr]
\;+\;
\tfrac12\,g(T-s)^2\,\nabla \cdot\!\Bigl[\nabla\,p_{T-s}(x)\Bigr].
\]</div>
<p>We can factor out <span class="arithmatex">\(p_{T-s}(x)\)</span> to write</p>
<div class="arithmatex">\[
\hat{f}(x,s)\,p_{T-s}(x)
\;=\;
f(x,T-s)\,p_{T-s}(x)
\;-\;
\tfrac12\,g(T-s)^2\,\nabla\,p_{T-s}(x)
\;+\;
\text{(gradient‐free constant in } x\text{)},
\]</div>
<p>but the divergence‐free “constant” in <span class="arithmatex">\(x\)</span> must be zero if we want the correct boundary conditions at infinity and a well‐defined velocity field. Dividing both sides by <span class="arithmatex">\(p_{T-s}(x)\)</span> then yields</p>
<div class="arithmatex">\[
\hat{f}(x,s)
\;=\;
f(x,T-s)
\;-\;
\tfrac12\,g(T-s)^2\,\nabla \log p_{T-s}(x).
\]</div>
<p>This is precisely the extra “score‐function” correction term discovered by Nelson and Anderson.</p>
<h4 id="final-reverse-sde">Final Reverse SDE<a class="headerlink" href="#final-reverse-sde" title="Permanent link">&para;</a></h4>
<p>Hence the <em>reversed</em> SDE can be written as</p>
<div class="arithmatex">\[
\boxed{
d\hat{X}_s
\;=\;
\Bigl[
f(\hat{X}_s,\,T-s)
\;-\;
\tfrac12\,g(T-s)^2\,\nabla \log p_{T-s}(\hat{X}_s)
\Bigr]\;ds
\;+\;
g(T-s)\,d\overline{W}_s.
}
\]</div>
<p>Often one writes the time variable in the same forward direction and just says
“the reverse‐time SDE from <span class="arithmatex">\(t\)</span> down to 0 is</p>
<div class="arithmatex">\[
dx \;=\;
\bigl[f(x,t)\;-\;g(t)^2\,\nabla_x \log p_{t}(x)\bigr]\;dt \;+\; g(t)\,d\overline{W}_t,
\]</div>
<p>where <span class="arithmatex">\(\overline{W}_t\)</span> is a standard Wiener process <em>when viewed backward</em> in the original clock. In either notation, the key extra piece is <span class="arithmatex">\(\,-\,g^{2}\nabla\log p_{t}(\cdot)\)</span>, which ensures the <em>distributions</em> truly run in reverse.</p>
<ul>
<li>
<p>The drift in the reverse process must “compensate” for how diffusion was spreading mass forward in time.</p>
</li>
<li>
<p>This compensation appears as <span class="arithmatex">\(\,-\,\tfrac12\,g^2 \nabla \log p_t(x)\)</span>.</p>
</li>
<li>
<p>Equivalently, one can say that in the backward direction, the random walk “knows” how to concentrate probability back into regions from which it had been dispersing forward.</p>
</li>
</ul>
<p>This completes the derivation (sometimes known as <em>Anderson’s theorem</em>).</p>
<p><strong>Understanding the Fokker–Planck Equation: A Gentle Introduction</strong> In many physical, biological, and financial systems, we are often interested not only in the behavior of individual particles or agents but also in how <em>probability distributions</em> evolve over time. If you have ever encountered a situation where randomness (noise) and drift (directed motion) both play roles in the dynamics, you have likely brushed up against the <em>Fokker–Planck equation (FP equation)</em>.
In this blog, we will explore:</p>
<ol>
<li>
<p><strong>What the Fokker–Planck equation is.</strong></p>
</li>
<li>
<p><strong>When and why the probability density follows it.</strong></p>
</li>
<li>
<p><strong>An intuitive explanation to help you understand it deeply.</strong></p>
</li>
</ol>
<h2 id="4-stochastic-differential-equation-for-ddpm">4. Stochastic Differential Equation for DDPM<a class="headerlink" href="#4-stochastic-differential-equation-for-ddpm" title="Permanent link">&para;</a></h2>
<p>Denoising Diffusion Probabilistic Models (DDPMs) are generative models that progressively add noise to data in a forward process, and then learn to invert this process to generate new data samples from noise. In the original DDPM (Ho et al., 2020), the forward diffusion is discrete, applying noise in <span class="arithmatex">\(N\)</span> steps until data become (approximately) Gaussian. The reverse process is also defined discretely, denoising step by step.
Recent perspectives (Song et al., 2021) have shown that we can view diffusion models in <em>continuous time</em>, describing them via SDEs. This text shows how the discrete forward process in DDPM converges to a continuous SDE when we let the number of steps <span class="arithmatex">\(N \to \infty\)</span>, and how the corresponding reverse SDE aligns with the DDPM reverse noising formula.</p>
<h3 id="41-discrete-forward-process-in-ddpm">4.1 Discrete Forward Process in DDPM<a class="headerlink" href="#41-discrete-forward-process-in-ddpm" title="Permanent link">&para;</a></h3>
<p>In the standard (discrete) DDPM, each forward step is:</p>
<div class="arithmatex">\[
 x_i \;=\; \sqrt{1 - \beta_i}\, x_{i-1} \;+\; \sqrt{\beta_i}\,\epsilon_i,
\quad
\epsilon_i \sim \mathcal{N}(0, I),
\quad i=1,\dots,N.
\]</div>
<ul>
<li>
<p><span class="arithmatex">\(\beta_i \in [0,1]\)</span> is the noise intensity at step <span class="arithmatex">\(i\)</span>.</p>
</li>
<li>
<p><span class="arithmatex">\(x_0\)</span> is sampled from the real data distribution <span class="arithmatex">\(p_{\mathrm{data}}\)</span>.</p>
</li>
<li>
<p>After <span class="arithmatex">\(N\)</span> steps, <span class="arithmatex">\(x_N\)</span> is (approximately) Gaussian.
Often, <span class="arithmatex">\(\beta_i\)</span> is chosen according to a schedule (e.g. linear, cosine) from a small value to a moderately larger one. Even if some <span class="arithmatex">\(\beta_i\)</span> are not <em>very</em> small, we can view each step as adding Gaussian noise to the current sample.</p>
</li>
</ul>
<h3 id="42-splitting-large-steps-via-gaussian-additivity">4.2 Splitting Large Steps via Gaussian Additivity<a class="headerlink" href="#42-splitting-large-steps-via-gaussian-additivity" title="Permanent link">&para;</a></h3>
<p>A key observation is that a single “large step” of Gaussian noise can be <em>split</em> into many smaller Gaussian increments without changing the <strong>final distribution</strong> . Formally, if</p>
<p>$$
 x_{i}
\;=\; \sqrt{1-\beta_i}\,x_{i-1} \;+\; \sqrt{\beta_i}\,\epsilon_i,
$$
we can represent <span class="arithmatex">\(\epsilon_i\)</span> as the sum of <span class="arithmatex">\(M\)</span> smaller Gaussians of variance <span class="arithmatex">\(\beta_i/M\)</span>. In other words, we can rewrite that same update with <span class="arithmatex">\(M\)</span> micro-steps, each of which has variance <span class="arithmatex">\(\beta_i/M\)</span>. Because Gaussians are closed under convolution, <span class="arithmatex">\(M\)</span> small steps or <span class="arithmatex">\(1\)</span> big step produce the <em>same</em> marginal distribution for <span class="arithmatex">\(x_i\)</span>.
When <span class="arithmatex">\(M \to \infty\)</span>, each micro-step becomes infinitesimally small. Hence, we can imagine the entire forward diffusion as a limit of infinitely many tiny Gaussian increments. This viewpoint paves the way to interpret the forward process as a stochastic differential equation.</p>
<hr />
<h2 id="4-from-discrete-updates-to-a-continuous-sde">4. From Discrete Updates to a Continuous SDE<a class="headerlink" href="#4-from-discrete-updates-to-a-continuous-sde" title="Permanent link">&para;</a></h2>
<h3 id="41-mapping-steps-to-time">4.1 Mapping Steps to Time<a class="headerlink" href="#41-mapping-steps-to-time" title="Permanent link">&para;</a></h3>
<p>Let the total diffusion run over time <span class="arithmatex">\(t \in [0,1]\)</span>, with discrete steps <span class="arithmatex">\(t_i = i/N\)</span>. Denote <span class="arithmatex">\(\Delta t = 1/N\)</span>. Suppose we set
$$
 \beta_i \;\approx\; \beta(t_i)\,\Delta t,
$$</p>
<p>where <span class="arithmatex">\(\beta(\cdot)\)</span> is some bounded nonnegative function on <span class="arithmatex">\([0,1]\)</span>.</p>
<h3 id="42-taylor-expansion-argument">4.2 Taylor Expansion Argument<a class="headerlink" href="#42-taylor-expansion-argument" title="Permanent link">&para;</a></h3>
<p>Consider one discrete step:</p>
<div class="arithmatex">\[
x_{i}=\sqrt{1-\beta_i}\,x_{i-1} \;+\; \sqrt{\beta_i}\,\epsilon_i.
\]</div>
<p>If <span class="arithmatex">\(\beta_i\)</span> is small, we use a first-order expansion:</p>
<div class="arithmatex">\[
 \sqrt{1-\beta_i}
\;\approx\;
1 \;-\; \tfrac12\,\beta_i.
\]</div>
<p>Then</p>
<div class="arithmatex">\[
x_i - x_{i-1}\;\approx\;-\tfrac12\,\beta_i\,x_{i-1}\;+\; \sqrt{\beta_i}\,\epsilon_i.
\]</div>
<p>Suppose <span class="arithmatex">\(x_i = x_{t+\Delta t}\)</span> and <span class="arithmatex">\(x_{i-1}= x_{t}\)</span>,
Hence</p>
<div class="arithmatex">\[
\begin{aligned}
x_{t + \Delta t}  - x_{t} &amp;= -\frac{1}{2} \beta (t + \Delta t) \Delta t x_{t} + \sqrt{\beta(t+\Delta t)\Delta t}\epsilon_t\\
&amp;\approx  -\frac{1}{2} \beta (t ) \Delta t x_{t} + \sqrt{\beta(t)\Delta t}\epsilon_t \\
&amp; = -\frac{1}{2} \beta (t ) x_{t} \Delta t + \sqrt{\beta(t)} \Delta t \epsilon_t \\
&amp; = -\frac{1}{2} \beta (t ) x_{t} \Delta t + \sqrt{\beta(t)} \big(W(t+\Delta t)- W(t) \big)\\
&amp; = -\frac{1}{2} \beta (t ) x_{t} \Delta t + \sqrt{\beta(t)} dW
\end{aligned}
\]</div>
<p>Note that</p>
<div class="arithmatex">\[
W(t + \Delta t) - W(t) \sim \mathcal{N}(0, \Delta t).
\]</div>
<p>Thus in the limit we get the SDE:</p>
<div class="arithmatex">\[
d x = -\tfrac12\,\beta(t)\,x\,dt \;+\; \sqrt{\beta(t)}\,dw
\]</div>
<p>This is often referred to as the <strong>Variance Preserving SDE (VP-SDE)</strong>  in the score-based literature.</p>
<h3 id="43-splitting-steps-argument">4.3 Splitting Steps Argument<a class="headerlink" href="#43-splitting-steps-argument" title="Permanent link">&para;</a></h3>
<p>Alternatively, if <span class="arithmatex">\(\beta_i\)</span> is not initially small, we can split each “big step” into <span class="arithmatex">\(M\)</span> micro-steps of size <span class="arithmatex">\(\beta_i/M\)</span>. If we then let <span class="arithmatex">\(N \to \infty\)</span> and <span class="arithmatex">\(M \to \infty\)</span>, so that <span class="arithmatex">\(\beta_i/M \to 0\)</span>, the entire chain of <span class="arithmatex">\(N\times M\)</span> micro-steps again converges to the same SDE. Both methods lead to the conclusion:</p>
<div class="arithmatex">\[
 \boxed{
\text{Forward SDE: }
dx = -\tfrac12\,\beta(t)\,x\,dt \;+\; \sqrt{\beta(t)}\,dw.
}
\]</div>
<h3 id="44-the-reverse-sde">4.4. The Reverse SDE<a class="headerlink" href="#44-the-reverse-sde" title="Permanent link">&para;</a></h3>
<p>Given a forward SDE</p>
<div class="arithmatex">\[
 dx = f(x,t)\,dt \;+\; g(t)\,dw,
\]</div>
<p>the <em>reverse-time</em> SDE is given by (Anderson, 1982; Song et al., 2021):</p>
<div class="arithmatex">\[
d\tilde{x}=\bigl[f(\tilde{x}, t) \;-\; g(t)^2\,\nabla_{\tilde{x}}\!\log p_t(\tilde{x})\bigr]\;dt \;+\; g(t)\,d\tilde{w},
\]</div>
<p>where <span class="arithmatex">\(\tilde{w}\)</span> is a Brownian motion in reverse time, and <span class="arithmatex">\(\nabla_{\tilde{x}}!\log p_t(\tilde{x})\)</span> is the <em>score</em> function (gradient of the log density) at time <span class="arithmatex">\(t\)</span>.</p>
<p>When <span class="arithmatex">\(f(x,t) = -\tfrac12,\beta(t),x\)</span> and <span class="arithmatex">\(g(t)=\sqrt{\beta(t)}\)</span>, the reverse SDE becomes:</p>
<div class="arithmatex">\[
d\tilde{x}(t)= - \bigl[\tfrac12\,\beta(t)\,\tilde{x}+\beta(t)\,\nabla_{\tilde{x}}\!\log p_t(\tilde{x})\bigr]\;dt \;+\;\sqrt{\beta(t)}\,d\tilde{w}.
\]</div>
<h3 id="43-consistency-with-ddpms-reverse">4.3 Consistency with DDPM’s Reverse<a class="headerlink" href="#43-consistency-with-ddpms-reverse" title="Permanent link">&para;</a></h3>
<p>Review the reserve step in DDPM</p>
<div class="arithmatex">\[\tag{5}\boxed{
x_{t-1} = \frac{1}{\sqrt{1-\beta_t}}\left(x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\,\epsilon_\theta(x_t,t)\right) + \sqrt{\tilde{\beta}_t}\,z,\quad z\sim\mathcal{N}(0,I)}
\]</div>
<p>Recall that in the DDPM reverse process the update (when the network predicts the noise <span class="arithmatex">\(\epsilon_\theta\)</span>) is given by</p>
<div class="arithmatex">\[
 x_{t-1} = \frac{1}{\sqrt{1-\beta_t}}\left(x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\,\epsilon_\theta(x_t,t)\right) + \sqrt{\tilde{\beta}_t}\,z,\quad z\sim \mathcal{N}(0,I),
\]</div>
<p>with
$$
 \bar{\alpha}<em>t = \prod</em>{s=1}^t (1-\beta_s),\qquad \tilde{\beta}<em>t = \frac{1-\bar{\alpha}</em>{t-1}}{1-\bar{\alpha}_t}\,\beta_t.
$$</p>
<p>When the number of steps is large, one may view each update as occurring over a small time interval. In what follows we will reparameterize the discrete index in terms of a small time increment.</p>
<p>Suppose that in the original (forward) time variable <span class="arithmatex">\(t\)</span> we write the update from <span class="arithmatex">\(t\)</span> to <span class="arithmatex">\(t-\Delta t\)</span> as</p>
<div class="arithmatex">\[
 x(t-\Delta t) = \frac{1}{\sqrt{1-\beta(t)\,\Delta t}}\left(x(t) - \frac{\beta(t)\,\Delta t}{\sqrt{1-\bar{\alpha}(t)}}\,\epsilon_\theta(x(t),t)\right) + \sqrt{\tilde{\beta}(t)\,\Delta t}\,z.
\]</div>
<p>Here we assume that for a small time interval the noise variance scales aa <span class="arithmatex">\(\beta(t)\,\Delta t\)</span>,
and similarly <span class="arithmatex">\(\tilde{\beta}(t)\,\Delta t\)</span> is the variance used in the reverse update. (In the continuous limit one typically finds <span class="arithmatex">\(\tilde{\beta}(t)\to\beta(t)\)</span>.)</p>
<p>Expanding to first order in <span class="arithmatex">\(\Delta t\)</span> gives</p>
<div class="arithmatex">\[
\begin{aligned}
x(t-\Delta t) &amp;\approx \Bigl(1+\tfrac{1}{2}\beta(t)\Delta t\Bigr)\left(x(t) - \frac{\beta(t)\Delta t}{\sqrt{1-\bar{\alpha}(t)}}\,\epsilon_\theta(x(t),t)\right) + \sqrt{\tilde{\beta}(t)\Delta t}\,z\\
&amp;\approx x(t) - \frac{\beta(t)\,\Delta t}{\sqrt{1-\bar{\alpha}(t)}}\,\epsilon_\theta(x(t),t)+ \frac{1}{2}\beta(t)x(t)\,\Delta t+ \sqrt{\tilde{\beta}(t)\Delta t}\,z.
\end{aligned}
\]</div>
<p>That is, the increment in the original (forward) time is</p>
<div class="arithmatex">\[-\Delta x = x(t-\Delta t)-x(t) = \left[\frac{1}{2}\beta(t)x(t)-\frac{\beta(t)}{\sqrt{1-\bar{\alpha}(t)}}\,\epsilon_\theta(x(t),t)\right]\Delta t + \sqrt{\tilde{\beta}(t)\,\Delta t}\,z.
\]</div>
<p>In the continuous limit, <span class="arithmatex">\(\sqrt{\Delta t}\,z\)</span> becomes the differential <span class="arithmatex">\(dW\)</span> of a standard Wiener process.</p>
<div class="arithmatex">\[
 \text{d} x
=- \left[\frac{1}{2}\beta(t)\,x(t)
-\frac{\beta(t)}{\sqrt{1-\bar{\alpha}(t)}}\,\epsilon_\theta\Bigl(x(t),\,t)\Bigr)
\right]dt - \sqrt{\beta(t))}\,dW.
\]</div>
<p>Remember that we are treating the reverse process,</p>
<p>Let <span class="arithmatex">\(\tau = T- t\)</span> and <span class="arithmatex">\(\tilde{x}(\tau) = x(T-\tau)\)</span>,</p>
<p>We have <span class="arithmatex">\(d \tau = - dx\)</span> and <span class="arithmatex">\(d\tau = -dt\)</span>,
then,</p>
<div class="arithmatex">\[
 \text{d} \tilde{x}
=- \left[\frac{1}{2}\beta(T-\tau)\,\tilde{x}(\tau)
-\frac{\beta(T-\tau)}{\sqrt{1-\bar{\alpha}(T-\tau)}}\,\epsilon_\theta\Bigl(\tilde{x}(\tau),\,T-\tau\Bigr)
\right]\tau + \sqrt{\beta(T-\tau)}\,dW.
\]</div>
<p>This derivation shows how one may obtain a continuous-time (SDE) formulation of the DDPM reverse process with an explicit reversed time mapping.</p>
<p>Recall the formula from the reverse‐time dynamics,</p>
<div class="arithmatex">\[
 dx = \Bigl[-\frac{1}{2}\beta(t)x - \beta(t)\,\nabla_x\log p_t(x)\Bigr]dt + \sqrt{\beta(t)}\,d\bar{w}.
\]</div>
<p>Use the estimator of the gradient from learning</p>
<div class="arithmatex">\[
 \nabla_x\log p_t(x) \approx -\frac{1}{\sqrt{1-\bar{\alpha}(t)}}\,\epsilon_\theta(x,t),
\]</div>
<p>We got
$$
 dx = \left[-\frac{1}{2}\beta(t)x + \frac{\beta(t)}{\sqrt{1-\bar{\alpha}(t)}}\,\epsilon_\theta(x,t)\right]dt + \sqrt{\beta(t)}\,d\bar{w}.
$$</p>
<h2 id="6-network-output-noise-prediction-vs-score">6. Network Output: Noise Prediction vs. Score<a class="headerlink" href="#6-network-output-noise-prediction-vs-score" title="Permanent link">&para;</a></h2>
<p>In practice, DDPM commonly trains a network to predict the added noise <span class="arithmatex">\(\epsilon\)</span> rather than directly predicting <span class="arithmatex">\(\nabla_x \log p(x_t)\)</span>. However, there is a <strong>simple linear relation</strong>  between <span class="arithmatex">\(\epsilon\)</span> and the score. Therefore:
1. <strong>Predicting noise</strong>  and <strong>predicting the score</strong>  are <em>equivalent</em> in mathematical terms.</p>
<ol>
<li>
<p>Score-based SDE models often produce <span class="arithmatex">\(\mathbf{s}_\theta(x,t) \approx \nabla_x \log p_t(x)\)</span>.</p>
</li>
<li>
<p>DDPM-like models produce <span class="arithmatex">\(\epsilon_\theta(x_t,t)\)</span>, from which the score can be derived.</p>
</li>
</ol>
<p>Either way, we get a functional approximation to the same “denoising direction” that enables the reverse-time generative process.</p>
<h2 id="appendix">Appendix<a class="headerlink" href="#appendix" title="Permanent link">&para;</a></h2>
<h3 id="1-what-is-the-fokkerplanck-equation">1. What is the Fokker–Planck Equation?<a class="headerlink" href="#1-what-is-the-fokkerplanck-equation" title="Permanent link">&para;</a></h3>
<p>Put simply, the Fokker–Planck equation describes how the <em>probability density function</em> (PDF) of a stochastic (random) process evolves over time. More formally, if we let <span class="arithmatex">\(p(x,t)\)</span> be the probability density of finding the system in state <span class="arithmatex">\(x\)</span> at time <span class="arithmatex">\(t\)</span>, the Fokker–Planck equation reads:</p>
<div class="arithmatex">\[
\frac{\partial p(x,t)}{\partial t} =  \left\lbrace -\frac{\partial}{\partial x} \left[ \mu(x)p(x,t)\right] + \frac{1}{2}\,\frac{\partial^2}{\partial x^2}(\,D(x)\,p(x,t))\right\rbrace
\]</div>
<p>where</p>
<ul>
<li>
<p><span class="arithmatex">\(\mu(x)\)</span> represents the <em>drift</em> (systematic motion) of the process in state <span class="arithmatex">\(x\)</span>,</p>
</li>
<li>
<p><span class="arithmatex">\(D(x)\)</span> represents the <em>diffusion</em> (random fluctuations) at state <span class="arithmatex">\(x\)</span>.</p>
</li>
</ul>
<p>For simplicity, we often see the equation in the case of constant diffusion <span class="arithmatex">\(D\)</span> and constant drift <span class="arithmatex">\(v\)</span>:
$$
 \frac{\partial p(x,t)}{\partial t} = -v\,\frac{\partial p(x,t)}{\partial x} + \frac{D}{2}\,\frac{\partial^2 p(x,t)}{\partial x^2}.
$$</p>
<p>This partial differential equation tells us how the probability density changes because of both deterministic drift (<span class="arithmatex">\(v\)</span>) and random diffusion (<span class="arithmatex">\(D\)</span>).</p>
<hr />
<h4 id="when-does-the-probability-follow-the-fokkerplanck-equation">When Does the Probability Follow the Fokker–Planck Equation?<a class="headerlink" href="#when-does-the-probability-follow-the-fokkerplanck-equation" title="Permanent link">&para;</a></h4>
<p>The Fokker–Planck equation is typically valid under these conditions:</p>
<ol>
<li><strong>Continuous Markov processes in one dimension (or higher).</strong></li>
<li>
<p>The process must be <em>Markovian</em>: the future depends only on the present state, not on the history.</p>
</li>
<li>
<p><strong>Stochastic differential equations (SDEs) of the form</strong></p>
</li>
</ol>
<div class="arithmatex">\[
 dX_t = \mu(X_t)\,dt + \sqrt{D(X_t)}\,dW_t,
\]</div>
<ul>
<li>
<p>where <span class="arithmatex">\(W_t\)</span> is a standard Brownian motion (Wiener process).</p>
</li>
<li>
<p><strong>Small, Gaussian-like noise.</strong></p>
</li>
<li>
<p>The derivation of the Fokker–Planck relies on the assumption that increments of noise are small over short intervals (leading to the second-order derivative term in the FP equation).</p>
</li>
<li>
<p><strong>No long-range jumps (no Lévy flights).</strong></p>
</li>
<li>If jumps or large discontinuous moves exist, then we would use <em>generalized</em> kinetic equations (like fractional Fokker–Planck or Master equations) rather than the classical FP equation.
In essence, if your system follows an Itô-type (or Stratonovich-type) SDE with drift <span class="arithmatex">\(\mu\)</span> and diffusion coefficient <span class="arithmatex">\(D\)</span>, then the <em>probability density</em> of where that system might be at time <span class="arithmatex">\(t\)</span> will satisfy the Fokker–Planck equation.</li>
</ul>
<h4 id="intuitive-interpretation-of-drift-and-diffusion-in-the-fokkerplanck-equation">Intuitive Interpretation of Drift and Diffusion in the Fokker–Planck Equation<a class="headerlink" href="#intuitive-interpretation-of-drift-and-diffusion-in-the-fokkerplanck-equation" title="Permanent link">&para;</a></h4>
<p>The Fokker–Planck equation is a powerful tool for describing how probability densities evolve over time. In many fields—such as physics, biology, and finance—it is used to capture the interplay between deterministic forces and random fluctuations. This blog post provides an intuitive explanation of the two key components: <strong>drift</strong>  and <strong>diffusion</strong> .</p>
<h5 id="1-drift-the-deterministic-push">1. Drift: The Deterministic Push<a class="headerlink" href="#1-drift-the-deterministic-push" title="Permanent link">&para;</a></h5>
<h6 id="mathematical-form">Mathematical Form<a class="headerlink" href="#mathematical-form" title="Permanent link">&para;</a></h6>
<p>In the one-dimensional Fokker–Planck equation, the drift term is commonly written as:</p>
<div class="arithmatex">\[
 -\frac{\partial}{\partial x}\Big[A(x)P(x,t)\Big]
\]</div>
<p>Here,</p>
<ul>
<li>
<p><strong><span class="arithmatex">\(A(x)\)</span></strong>  represents the deterministic drift or “drift velocity”. It reflects the average effect of an external force acting on the system.</p>
</li>
<li>
<p><strong><span class="arithmatex">\(P(x,t)\)</span></strong>  is the probability density at position <span class="arithmatex">\(x\)</span> and time <span class="arithmatex">\(t\)</span>.</p>
</li>
<li>
<p>The product <strong>The product <span class="arithmatex">\(A(x)P(x,t)\)</span></strong>  represents the probability flux resulting from the deterministic motion.</p>
</li>
</ul>
<h6 id="intuitive-explanation">Intuitive Explanation<a class="headerlink" href="#intuitive-explanation" title="Permanent link">&para;</a></h6>
<p>Imagine particles moving under the influence of an external force:</p>
<ul>
<li>
<p><strong>Probability Flow:</strong>
The term <span class="arithmatex">\(A(x)P(x,t)\)</span> can be thought of as the flow of particles moving with velocity <span class="arithmatex">\(A(x)\)</span>. This is similar to how a group of people might move along a corridor when prompted by a guiding signal.</p>
</li>
<li>
<p><strong>Divergence and Local Density Change:</strong>
The spatial derivative <span class="arithmatex">\(\frac{\partial}{\partial x}[A(x)P(x,t)]\)</span> measures how the flow changes along the <span class="arithmatex">\(x\)</span>-direction (its divergence).</p>
</li>
<li>
<p>If the divergence is <strong>positive</strong>  at a point, more particles are leaving that region than entering it. Because of the negative sign in the equation, this results in a decrease in the probability density (<span class="arithmatex">\(\frac{\partial P}{\partial t} &lt; 0\)</span>).</p>
</li>
<li>
<p>Conversely, if the divergence is <strong>negative</strong> , particles are converging at that point, leading to an increase in the local density (<span class="arithmatex">\(\frac{\partial P}{\partial t} &gt; 0\)</span>).</p>
</li>
</ul>
<p>This formulation captures how deterministic forces cause a net flow of probability, thereby modifying the local density.</p>
<h5 id="2-diffusion-the-smoothing-effect-of-random-perturbations">2. Diffusion: The Smoothing Effect of Random Perturbations<a class="headerlink" href="#2-diffusion-the-smoothing-effect-of-random-perturbations" title="Permanent link">&para;</a></h5>
<h6 id="mathematical-form_1">Mathematical Form<a class="headerlink" href="#mathematical-form_1" title="Permanent link">&para;</a></h6>
<p>The diffusion term in the Fokker–Planck equation is typically expressed as:
$$
 \frac{1}{2}\frac{\partial^2}{\partial x^2}\Big[B(x)P(x,t)\Big]
$$</p>
<p>Here,</p>
<ul>
<li>
<p><strong><span class="arithmatex">\(B(x)\)</span></strong>  is the diffusion coefficient, indicating the strength of random fluctuations.</p>
</li>
<li>
<p>The second derivative <span class="arithmatex">\(\frac{\partial^2}{\partial x^2}\)</span> reflects the curvature of the probability density, indicating how "peaked" or "spread out" the distribution is locally.</p>
</li>
</ul>
<h6 id="intuitive-explanation_1">Intuitive Explanation<a class="headerlink" href="#intuitive-explanation_1" title="Permanent link">&para;</a></h6>
<p>Diffusion describes how random perturbations work to smooth out irregularities in the probability distribution:</p>
<ul>
<li>
<p><strong>High-Density Regions (Peaks):</strong>
At locations where <span class="arithmatex">\(P(x,t)\)</span> forms a peak, the distribution is concave down (negative curvature). The diffusion term acts to reduce the density at these peaks, causing the probability to spread out.</p>
</li>
<li>
<p><strong>Low-Density Regions (Valleys):</strong>
In contrast, in regions where the density is low and the distribution is convex (positive curvature), diffusion causes the density to increase by “filling in” these valleys with probability from neighboring regions.</p>
</li>
</ul>
<p>This effect is analogous to a drop of ink spreading on paper: the concentrated ink in the center disperses outward, leading to a more uniform concentration over time. Diffusion thus does not simply lower the density everywhere; rather, it redistributes it, diminishing local extremes to create a smoother overall profile.</p>
<hr />
<h5 id="3-drift-and-diffusion-combined-evolution-of-the-probability-density">3. Drift and Diffusion: Combined Evolution of the Probability Density<a class="headerlink" href="#3-drift-and-diffusion-combined-evolution-of-the-probability-density" title="Permanent link">&para;</a></h5>
<p>In the Fokker–Planck equation, drift and diffusion work together to dictate how the probability density evolves:</p>
<ul>
<li>
<p><strong>Drift</strong>  accounts for the systematic, directional movement driven by deterministic forces. It creates flows that can either concentrate or deplete the density depending on the spatial gradient (divergence) of the flux.</p>
</li>
<li>
<p><strong>Diffusion</strong>  represents the effect of random fluctuations, working to smooth out the probability distribution. It reduces sharp peaks and fills in dips, leading to a more even spread of the density.</p>
</li>
</ul>
<p>Together, these processes explain how the overall distribution changes over time, even if the motion of individual particles is unpredictable. By considering both the directional push of drift and the smoothing influence of diffusion, the Fokker–Planck equation provides a comprehensive framework for understanding the evolution of complex systems under uncertainty.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "toc.integrate"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.60a45f97.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>