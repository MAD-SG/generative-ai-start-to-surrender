<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="text/html; charset=utf-8" http-equiv="Content-Type">
<title>Basic - Generative AI: From Start to Surrender</title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<meta name="generator" content="mkdocs-1.6.1, mkdocs-gitbook-1.0.7">

<link rel="shortcut icon" href="../../../images/favicon.ico" type="image/x-icon">
<meta name="HandheldFriendly" content="true"/>
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta rel="next" href="" />
<link href="../../../css/style.min.css" rel="stylesheet"> 
</head>

<body>
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input type="text" placeholder="Type to search" />
</div> <!-- end of book-search-input -->

<nav role="navigation">
<ul class="summary">
<li>
<a href="../../.." target="_blank" class="custom-link">Generative AI: From Start to Surrender</a>
</li>
<li class="divider"></li>
<li class="chapter" data-path="">
<a href="../../..">Home</a>
<li class="header">Introduction</li>

<li>
<a href="../../chapter1_Introduction/1.1terminology/" class="">Terms</a>
</li>

<li>
<a href="../../chapter1_Introduction/1.2fourier_transform/" class="">Fourier Transform</a>
</li>

<li>
<a href="../../chapter1_Introduction/1.3signal_processing/" class="">Signal Processing</a>
</li>

<li>
<a href="../../chapter1_Introduction/1.4statistics/" class="">Statistics</a>
</li>

<li>
<a href="../../chapter1_Introduction/1.5SDE/" class="">SDE</a>
</li>

<li class="header">VAE</li>

<li>
<a href="../../chapter2_VAE/2.1introduction/" class="">VAE Introduction</a>
</li>

<li class="header">GANs</li>

<li>
<a href="../../chapter3_GAN/3.2pggan/paper/" class="">PGGAN</a>
</li>

<li>
<a href="../../chapter3_GAN/3.3stylegan/paper/" class="">StyleGAN</a>
</li>

<li>
<a href="../../chapter3_GAN/3.4stylegan2/paper/" class="">StyleGAN2</a>
</li>

<li>
<a href="../../chapter3_GAN/3.5stylegan3/paper/" class="">StyleGAN3</a>
</li>

<li>
<a href="../../chapter3_GAN/3.6styleganT/paper/" class="">StyleGAN T</a>
</li>

<li>
<a href="../../chapter3_GAN/3.7R3Gan/paper/" class="">R3Gan</a>
</li>

<li class="header">Diffusion</li>

<li>
<a href="../../chapter4_diffusion/4.1introduction/" class="">Introduction</a>
</li>

<li>
<a href="../../chapter4_diffusion/4.2DDPM/" class="">DDPM</a>
</li>

<li class="header">Generation Theory</li>

<li>
<a href="./" class="active">Basic</a>
</li>

<li class="divider"></li>



<li><a href="http://www.mkdocs.org">
Published with MkDocs
</a></li>

<li><a href="https://github.com/GitbookIO/theme-default">
Theme by GitBook
</a></li>
</ul>

</nav>

</div> <!-- end of book-summary -->

<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">

<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="." ></a>
</h1>

</div> <!-- end of book-header -->

<div class="page-wrapper" tabindex="-1" role="main">
<div class="page-inner">
<div id="book-search-results">
<div class="search-noresults">

<section class="normal markdown-section">



<h1 id="_1">基础理论</h1>
<h2 id="_2">生成模型可以根据其目标和方法分类：</h2>
<table>
<thead>
<tr>
<th>模型类别</th>
<th>目标</th>
<th>典型模型</th>
<th>应用领域</th>
</tr>
</thead>
<tbody>
<tr>
<td>GAN</td>
<td>最小化最小分布距离</td>
<td>GAN DCGAN、StyleGAN、BigGAN</td>
<td>图像生成</td>
</tr>
<tr>
<td>隐变量模型</td>
<td>最大化对数似然</td>
<td>VAE, PixelVAE</td>
<td>图像生成、序列生成</td>
</tr>
<tr>
<td>概率密度估计</td>
<td>最大化对数似然</td>
<td>Normalizing Flow, Energy-Based Models</td>
<td>密度估计、图像生成</td>
</tr>
<tr>
<td>逐步最大似然估计模型</td>
<td>逐步最大化对数似然</td>
<td>DDPM、Latent Diffusion Models (LDM)</td>
<td>图像生成、补全、超分辨率</td>
</tr>
<tr>
<td>自回归模型</td>
<td>最大化条件对数似然</td>
<td>自回归模型</td>
<td>图像生成、补全、超分辨率</td>
</tr>
<tr>
<td>几何/物理约束模型</td>
<td>最小化重建误差</td>
<td>NeRF, DeepSDF</td>
<td>三维建模、视点合成</td>
</tr>
<tr>
<td>规则/统计生成模型</td>
<td>基于规则或经验</td>
<td>Procedural Generation, SMOTE</td>
<td>数据增强、生成纹理</td>
</tr>
<tr>
<td>离散生成模型</td>
<td>离散最大似然估计</td>
<td>GPT, Transformer</td>
<td>文本生成、代码生成</td>
</tr>
<tr>
<td>稀疏/压缩生成模型</td>
<td>稀疏表示或压缩后重建</td>
<td>Sparse Coding, Autoencoders</td>
<td>特征提取、数据压缩</td>
</tr>
<tr>
<td>混合生成模型</td>
<td>结合多个生成目标</td>
<td>VAE-GAN, Diffusion-GAN</td>
<td>图像生成、高质量数据生成</td>
</tr>
</tbody>
</table>
<p>主要生成任务使用的方法</p>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>常用方法</th>
<th>特点</th>
<th>典型模型</th>
</tr>
</thead>
<tbody>
<tr>
<td>图像生成</td>
<td>GAN、扩散模型、VAE、自回归模型</td>
<td>生成质量高，适合单帧图像生成，多样性和控制性视模型而定</td>
<td>StyleGAN、DDPM、PixelCNN</td>
</tr>
<tr>
<td>语音生成</td>
<td>自回归模型、谱图生成、GAN、扩散模型</td>
<td>高保真语音生成，常结合声码器完成端到端生成</td>
<td>WaveNet、Tacotron、HiFi-GAN、DiffWave</td>
</tr>
<tr>
<td>视频生成</td>
<td>GAN、自回归模型、扩散模型、混合模型</td>
<td>视频生成需要考虑时间一致性，模型更复杂，生成质量依赖于时间和空间的建模能力</td>
<td>MoCoGAN、VideoGPT、Video Diffusion Models</td>
</tr>
</tbody>
</table>
<p>接下来我们主要介绍在最小距离分布和最大似然估计的框架下，怎么统一解释不同的生成模型。同时最大似然估计是最小分布距离的一种特例，我们其实可以在”最小距离分布“这个统一的框架下来解释生成方法的原理。</p>
<p>记住这个公式，最大似然估计的表达式</p>
<div class="arithmatex">\[
 \int_{x\sim p_{\text {data }}} p_{\text {data }}(x) \log p_{\theta}(x) d x
\]</div>
<p>从离散的角度，最大似然估计表示为</p>
<div class="arithmatex">\[
 \sum_{x\sim p_{\text {data }}}  \log p_{\theta}(x)
\]</div>
<hr />
<h2 id="1-mlp">定理1. MLP 是最小化分布差异的特定形式</h2>
<p><strong>证明</strong>:
先说明结论：两个分布间的“距离”可以用不同的指标来衡量（如 KL 散度、Jensen-Shannon 散度、Wasserstein 距离等）。MLE 的目标是最小化 KL 散度：
$$
D_{\text{KL}}(p_{\text{data}} || p_\theta)
$$
因此，MLE 可以被认为是以 <strong>KL 散度</strong>  作为距离衡量标准的特例。
下面我们只要证明MLP目标等价于优化KL散度就行。</p>
<p>其中</p>
<div class="arithmatex">\[D_{\mathrm{KL}}\left(p_{\text {data }} \| p_\theta\right)=\int p_{\text {data }}(x) \log \frac{p_{\text {data }}(x)}{p_\theta(x)} d x\]</div>
<p>展开为</p>
<div class="arithmatex">\[D_{\mathrm{KL}}\left(p_{\text {data }} \| p_\theta\right)=\int p_{\text {data }}(x) \log p_{\text {data }}(x) d x-\int p_{\text {data }}(x) \log p_\theta(x) d x\]</div>
<p>第一项和<span class="arithmatex">\(\theta\)</span> 也就是模型无关，因此可以忽略。第二项和<span class="arithmatex">\(\theta\)</span>有关，因此可以看成KL的目标。</p>
<p>另外MLE的原始定义为</p>
<div class="arithmatex">\[
\theta^*=\arg \max_{\theta} L_{\theta}  \\
== \arg\max_{\theta} E_{x \sim p_{\text {data }}}[\log p_{\theta}(x)]\\
= \arg\min_{\theta} - \int p_{\text {data }}(x) \log p_{\theta}(x) d x
\]</div>
<hr />
<p>从这个角度从新不同的生成模型，包括VAE, GAN, Diffusion等等，它们的目标都是最小化生成分布和原始数据分布的差异(距离)的最小化。</p>
<hr />
<h2 id="2gan">定理2：gan的优化目标等价最小化分布距离</h2>
<p>GAN的目的是最小化分布差异，其中vanila GAN的目的是最小化两个分布之间的JSD散度, WGAN的目的是最小化连个分布之前的Wasserstein距离</p>
<p><strong>证明</strong>：</p>
<hr />
<p><strong>1. Vanilla GAN 的优化目标与 Jensen-Shannon 散度</strong></p>
<p><strong>1.1 GAN 的优化目标</strong></p>
<p>GAN 的目标函数由生成器 <span class="arithmatex">\(G\)</span> 和判别器 <span class="arithmatex">\(D\)</span> 的对抗博弈组成：$
 \min_G \max_D \mathbb{E}<em>{x \sim p</em>{\text{data}}} [\log D(x)] + \mathbb{E}_{z \sim p(z)} [\log (1 - D(G(z)))]
$</p>
<p>其中：</p>
<ul>
<li>
<p><span class="arithmatex">\(p_{\text{data}}(x)\)</span>：真实数据分布。</p>
</li>
<li>
<p><span class="arithmatex">\(p_\theta(x) = G(z)\)</span>：生成分布。</p>
</li>
</ul>
<p><strong>1.2 判别器的优化</strong></p>
<p>对于固定的生成器 <span class="arithmatex">\(G\)</span>，判别器 <span class="arithmatex">\(D\)</span> 的目标是最大化：$
 \mathcal{L}(D) = \mathbb{E}<em>{x \sim p</em>{\text{data}}} [\log D(x)] + \mathbb{E}<em>{x \sim p</em>\theta} [\log (1 - D(x))]
$</p>
<p>优化 <span class="arithmatex">\(D(x)\)</span>：</p>
<p>假设 <span class="arithmatex">\(D(x)\)</span> 输出的值是 <span class="arithmatex">\(D(x) \in [0, 1]\)</span>，对其求导并找到最优解 <span class="arithmatex">\(D^*(x)\)</span>：$
 D^*(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_\theta(x)}
$
此时最优判别器 <span class="arithmatex">\(D^*(x)\)</span> 表示输入样本来自真实分布的概率。</p>
<p><strong>1.3 将最优判别器代入损失</strong></p>
<p>将 <span class="arithmatex">\(D^*(x)\)</span> 代入 GAN 的目标函数，得到生成器的优化目标：$
 \min_G \max_D \mathcal{L}(D) = \mathbb{E}<em>{x \sim p</em>{\text{data}}} \left[\log \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_\theta(x)} \right] + \mathbb{E}<em>{x \sim p</em>\theta} \left[\log \frac{p_\theta(x)}{p_{\text{data}}(x) + p_\theta(x)} \right]
$</p>
<p>化简：
$
 \mathcal{L}(G) = -\log(4) + 2 \cdot D_{\text{JS}}(p_{\text{data}} || p_\theta)
$
其中 <span class="arithmatex">\(D_{\text{JS}}\)</span> 是 <strong>Jensen-Shannon 散度</strong> ，定义为：$
 D_{\text{JS}}(p_{\text{data}} || p_\theta) = \frac{1}{2} D_{\text{KL}}(p_{\text{data}} || m) + \frac{1}{2} D_{\text{KL}}(p_\theta || m)
$
<span class="arithmatex">\(m = \frac{1}{2}(p_{\text{data}} + p_\theta)\)</span>。<strong>结论：</strong>
Vanilla GAN 的优化目标是最小化生成分布和数据分布之间的 Jensen-Shannon 散度。</p>
<hr />
<p><strong>2. WGAN 的优化目标与 Wasserstein 距离</strong> <strong>2.1 WGAN 的目标函数</strong></p>
<p>WGAN 的目标函数是：
$
 \mathcal{L}(G, D) = \min_G \max_{D \in \text{Lip-1}} \mathbb{E}<em>{x \sim p</em>{\text{data}}} [D(x)] - \mathbb{E}<em>{x \sim p</em>\theta} [D(x)]
$
<strong>2.1 约束条件：</strong>
- 判别器 <span class="arithmatex">\(D(x)\)</span> 不再输出概率，而是标量值。</p>
<ul>
<li><span class="arithmatex">\(D(x)\)</span> 是 1-Lipschitz 连续函数，即满足 <span class="arithmatex">\(|D(x_1) - D(x_2)| \leq \|x_1 - x_2\|\)</span>。</li>
</ul>
<p><strong>2.2 Wasserstein 距离定义</strong> Wasserstein 距离（<span class="arithmatex">\(W_1\)</span> 距离）定义为：$
 W_1(p_{\text{data}}, p_\theta) = \inf_{\gamma \in \Pi(p_{\text{data}}, p_\theta)} \mathbb{E}<em>{(x, y) \sim \gamma} [|x - y|]
$
其中 <span class="arithmatex">\(\Pi(p_{\text{data}}, p_\theta)\)</span> 是所有使边缘分布为 <span class="arithmatex">\(p_{\text{data}}\)</span> 和 <span class="arithmatex">\(p_\theta\)</span> 的联合分布。
根据 Kantorovich-Rubinstein 对偶性，Wasserstein 距离可以重写为：
$
 W_1(p</em>{\text{data}}, p_\theta) = \sup_{|D|<em>L \leq 1} \mathbb{E}</em>{x \sim p_{\text{data}}} [D(x)] - \mathbb{E}<em>{x \sim p</em>\theta} [D(x)]
$</p>
<p><strong>3. 总结</strong>
- <strong>Vanilla GAN：</strong>  判别器 <span class="arithmatex">\(D\)</span> 输出的是概率，优化目标是最小化生成分布和真实分布的 Jensen-Shannon 散度（JSD）。</p>
<ul>
<li>
<p><strong>WGAN：</strong>  判别器 <span class="arithmatex">\(D\)</span> 输出的是标量值，优化目标是最小化生成分布和真实分布的 Wasserstein 距离（<span class="arithmatex">\(W_1\)</span>）。</p>
</li>
<li>
<p><strong>两者的本质：</strong>  都在通过不同的分布差异度量指标优化生成分布 <span class="arithmatex">\(p_\theta(x)\)</span> 逼近真实数据分布 <span class="arithmatex">\(p_{\text{data}}(x)\)</span>。</p>
</li>
</ul>
<p><strong>4. 备注</strong></p>
<p>在wgan 中为什么出现了Lipschitz 条件。这是因为 Kantorovich-Rubinstein 对偶性要求目标函数 <span class="arithmatex">\(𝑓(𝑥)\)</span> 是 1-Lipschitz 函数。如果没有这个条件，Wasserstein 距离无法通过对偶形式计算。
在 WGAN 中，判别器 D(x) 实际上是 f(x) 的实现，因此需要满足 Lipschitz 连续性，保证优化目标与 Wasserstein 距离的数学定义一致。</p>
<hr />
<p>从另外一个角度说明：判别器<span class="arithmatex">\(D\)</span>的作用可能不一致，但是D的loss 都表示了两个分布之间的距离，分别是JSD 散度和Wasserstein 距离。优化<span class="arithmatex">\(D\)</span> 的作用及时让这个Loss 尽量准确模拟出两个分布之间的距离。如果把这个loss <span class="arithmatex">\(L_\theta(x,y)\)</span> 作为一个函数看待,它在训练过程中学习的就是两个分布之间的距离的近似。</p>
<p>当然在上面的分析中，我们是知道了GAN的实现，然后证明了它的作用。</p>
<p>理论上我们衡量两个分布之间的距离有不同的选择，那在"GAN"的设计中，我们就可以根据不同的距离选择可以让我们去设计不同的D和D的loss。</p>
<p>那么假设我们想要用KL 散度去衡量两个分布之间的距离，那是不是可以设计出相应的loss。
答案是肯定的，</p>
<p>我们可以推导出对特定散度的优化近似于
<span class="arithmatex">\(\min _\theta \max _\omega F(\theta, \omega)=\mathbb{E}_{x \sim P}\left[T_\omega(x)\right]-\mathbb{E}_{x \sim Q_\theta}\left[f^*\left(T_\omega(x)\right)\right]\)</span>.</p>
<p>从而KL散度对应的loss 则为
<span class="arithmatex">\(<span class="arithmatex">\(E_{x\sim P}\left[\log D(x)\right]-E_{x\sim Q_\theta}\left[\log D(x)\right]\)</span>\)</span></p>
<p>参考这个论文 https://arxiv.org/pdf/1606.00709 了解更多散度对应的loss</p>
<h3 id="_3">广泛含义上的分布之间的衡量设计</h3>
<table>
<thead>
<tr>
<th>距离衡量方法</th>
<th>GAN 类型</th>
<th>优势</th>
<th>劣势</th>
<th>论文链接</th>
</tr>
</thead>
<tbody>
<tr>
<td>Jensen-Shannon 散度</td>
<td>Vanilla GAN</td>
<td>理论基础清晰，目标明确</td>
<td>梯度消失，模式崩溃</td>
<td><a href="https://arxiv.org/abs/1406.2661">Generative Adversarial Nets</a></td>
</tr>
<tr>
<td>Wasserstein 距离</td>
<td>WGAN</td>
<td>更稳定的训练过程，有意义的梯度</td>
<td>计算代价高，需强制 Lipschitz 条件</td>
<td><a href="https://arxiv.org/abs/1701.07875">Wasserstein GAN</a></td>
</tr>
<tr>
<td>f-散度</td>
<td>f-GAN</td>
<td>灵活的散度选择，适应不同任务需求</td>
<td>需选择合适的 f-散度</td>
<td><a href="https://arxiv.org/abs/1606.00709">f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization</a></td>
</tr>
<tr>
<td>MMD（最大均值差异）</td>
<td>MMD-GAN</td>
<td>核函数灵活，高维数据表现优越</td>
<td>核函数选择影响性能</td>
<td><a href="https://arxiv.org/abs/1705.08584">MMD GAN: Towards Deeper Understanding of Moment Matching Network</a></td>
</tr>
<tr>
<td>Sliced Wasserstein 距离</td>
<td>Sliced-WGAN</td>
<td>改善高维数据的训练稳定性</td>
<td>需要选择适当的投影方向</td>
<td><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Deshpande_Max-Sliced_Wasserstein_Distance_and_Its_Use_for_GANs_CVPR_2019_paper.pdf">Max-Sliced Wasserstein Distance and Its Use for GANs</a></td>
</tr>
<tr>
<td>Sobolev 距离</td>
<td>Sobolev GAN</td>
<td>放宽 Lipschitz 条件，提高训练灵活性</td>
<td>理论复杂性增加</td>
<td><a href="https://arxiv.org/abs/2012.03420">Towards Generalized Implementation of Wasserstein Distance in GANs</a></td>
</tr>
</tbody>
</table>
<hr />
<h2 id="3-vae">定理3: VAE 是对最大似然的下界的优化，近似最大似然估计</h2>
<p>证明</p>
<ol>
<li>连续分布的最大似然估计目标</li>
</ol>
<p>对于观测数据的概率分布 <span class="arithmatex">\(p_{\text{data}}(x)\)</span>，最大似然估计的目标是最大化数据分布下模型 <span class="arithmatex">\(p_\theta(x)\)</span> 的对数似然：$
 \int_{x \sim p_{\text{data}}} p_{\text{data}}(x) \log p_\theta(x) \, dx
$</p>
<p>这里我们需要找到一种办法去表达或者近似 <span class="arithmatex">\(p_\theta(x)\)</span>。 这是关键的一部分。 对于隐变量生成模型而言，会有一个<span class="arithmatex">\(z\)</span> 和<span class="arithmatex">\(X\)</span>的对应关系，</p>
<p>我们可以写成 <span class="arithmatex">\(p_\theta(x) = \int p_\theta(x, z) \, dz\)</span> 包含对隐变量 <span class="arithmatex">\(z\)</span> 的积分。再对<span class="arithmatex">\(z\)</span> 做一些假设，可能就会简化求解的过程。</p>
<ol>
<li>重写边缘似然</li>
</ol>
<p>对于单个数据点 <span class="arithmatex">\(x\)</span>，观测数据的边缘对数似然可以写为：$
 \log p_\theta(x) = \log \int p_\theta(x, z) \, dz
$
利用联合分布的分解 <span class="arithmatex">\(p_\theta(x, z) = p_\theta(x \mid z) p(z)\)</span>，我们有：$
 \log p_\theta(x) = \log \int p_\theta(x \mid z) p(z) \, dz
$
直接优化这个目标通常很困难，因为积分 <span class="arithmatex">\(\int p_\theta(x \mid z)p(z) dz\)</span> 对于高维 <span class="arithmatex">\(z\)</span> 不可解析。</p>
<ol>
<li>引入变分分布 <span class="arithmatex">\(q_\phi(z \mid x)\)</span>为了解决积分不可解析的问题</li>
</ol>
<p>引入一个近似后验分布 <span class="arithmatex">\(q_\phi(z \mid x)\)</span>，用于近似真实后验 <span class="arithmatex">\(p_\theta(z \mid x)\)</span>。我们可以通过以下分解重新表示 <span class="arithmatex">\(\log p_\theta(x)\)</span>：$
 \log p_\theta(x) = \mathbb{E}<em>{q</em>\phi(z \mid x)} \left[ \log \frac{p_\theta(x, z)}{q_\phi(z \mid x)} \right] + \mathrm{KL}(q_\phi(z \mid x) | p_\theta(z \mid x))
$</p>
<p>其中：</p>
<ul>
<li>
<p>第一项是变分下界（Evidence Lower Bound, ELBO），我们可以优化它来间接优化 <span class="arithmatex">\(\log p_\theta(x)\)</span>。</p>
</li>
<li>
<p>第二项是 KL 散度，表示近似后验 <span class="arithmatex">\(q_\phi(z \mid x)\)</span> 与真实后验 <span class="arithmatex">\(p_\theta(z \mid x)\)</span> 的差距。</p>
</li>
</ul>
<p>由于 KL 散度总是非负：<span class="arithmatex">\(\mathrm{KL}(q_\phi(z \mid x) \| p_\theta(z \mid x)) \geq 0\)</span>，所以：<span class="arithmatex">\(\log p_\theta(x) \geq \mathcal{L}(\theta, \phi; x)\)</span>, 其中<span class="arithmatex">\(L\)</span> 表示变分下界,也就是 ELBO，为上式的第一项。</p>
<ol>
<li>. 变分下界（ELBO）的分解</li>
</ol>
<p>变分下界的具体形式为：
$
 \mathcal{L}(\theta, \phi; x) = \mathbb{E}<em>{q</em>\phi(z \mid x)} \left[ \log p_\theta(x, z) - \log q_\phi(z \mid x) \right]
$
进一步分解联合概率 <span class="arithmatex">\(p_\theta(x, z) = p_\theta(x \mid z)p(z)\)</span>，得到：$
 \mathcal{L}(\theta, \phi; x) = \mathbb{E}<em>{q</em>\phi(z \mid x)} \left[ \log p_\theta(x \mid z) \right] - \mathrm{KL}(q_\phi(z \mid x) | p(z))
$</p>
<ul>
<li>
<p><strong>第一项</strong>  <span class="arithmatex">\(\mathbb{E}_{q_\phi(z \mid x)} [\log p_\theta(x \mid z)]\)</span>：重构误差（Reconstruction Error），鼓励生成器能够生成接近真实数据 <span class="arithmatex">\(x\)</span> 的分布。</p>
</li>
<li>
<p><strong>第二项</strong>  <span class="arithmatex">\(\mathrm{KL}(q_\phi(z \mid x) \| p(z))\)</span>：正则化项，约束 <span class="arithmatex">\(q_\phi(z \mid x)\)</span> 的分布接近先验 <span class="arithmatex">\(p(z)\)</span>。</p>
</li>
<li>
<p>最终形式（VAE 损失函数）
为了优化上述目标，我们需要进行采样 <span class="arithmatex">\(z \sim q_\phi(z \mid x)\)</span>。为了解决采样过程中不可微的问题，使用 <strong>重参数化技巧（Reparameterization Trick）</strong> ：将 <span class="arithmatex">\(q_\phi(z \mid x)\)</span> 定义为高斯分布 <span class="arithmatex">\(\mathcal{N}(\mu_\phi(x), \sigma_\phi(x)^2)\)</span>，通过如下方式采样：$
 z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
$</p>
</li>
</ul>
<p>最终，VAE 的损失函数可以表示为：</p>
<div class="arithmatex">\[
 \mathcal{L}(\theta, \phi; x) = \mathbb{E}_{z \sim q_\phi(z \mid x)} [\log p_\theta(x \mid z)] - \mathrm{KL}(q_\phi(z \mid x) \| p(z))
\]</div>
<p>这对应于：</p>
<ol>
<li>
<p><strong>重构误差</strong> ：通过生成分布 <span class="arithmatex">\(p_\theta(x \mid z)\)</span> 学习如何生成数据。</p>
</li>
<li>
<p><strong>KL 散度正则化</strong> ：约束潜变量分布。</p>
</li>
</ol>
<p>优化该目标，即实现从最大似然估计到 VAE 的转化。</p>
<hr />
<p>根据VAE 的目标，可以看出，再给定 <span class="arithmatex">\(x\)</span> 的情况下，第一项最大化意味着 <span class="arithmatex">\(p_\theta(x \mid z)=1\)</span>, 也就是再给定<span class="arithmatex">\(X\)</span>的条件下，生成<span class="arithmatex">\(X\)</span> 的概率要等于1，也就是完全重建。</p>
<p>那第二项则代表着需要让<span class="arithmatex">\(q_\theta(z|x)\)</span> 和<span class="arithmatex">\(p(z)\)</span> 的分布近似于一样，也就是说，让两个分布越来越接近。这一项也没有办法计算. 因此在VAE的实现里，假设<span class="arithmatex">\(q_\theta(z|x)\)</span>也是一个高斯分布，编码器输出的是均值和方差，这样就可以计算KL散度了。但是如果模型输出的结果为均值和方差，而不是Z，那就需要根据均值和方差生成<span class="arithmatex">\(z\)</span>了,这也是为什么VAE的实现里有一个采样的过程。</p>
<p>实际的VAE过程
<img alt="alt text" src="../../../images/image-19.png" /></p>
<p>从这里我们可以看到，对于VAE而言，第一项的计算是等价于重建loss 的，关键是第二项需要怎么设计。不同的设计可能就以为着不同的方法。</p>
<p>这里我们列举了一系列对第二项计算的改进方法</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
<th>优点</th>
<th>缺点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>标准高斯 VAE</td>
<td>假设 <span class="arithmatex">\(q_\phi(z \mid x)\)</span>为高斯分布，编码器输出均值和方差。</td>
<td>简单高效，可解析计算 KL 散度。</td>
<td>表达能力有限，无法处理复杂分布。</td>
<td>常规任务，数据分布简单。</td>
</tr>
<tr>
<td>高斯混合 VAE</td>
<td>假设 <span class="arithmatex">\(q_\phi(z \mid x)\)</span> 是混合高斯分布（多个高斯成分）。</td>
<td>能够捕捉多模态分布，更适合复杂数据分布。</td>
<td>增加计算复杂度，需要估计更多参数。</td>
<td>多模态数据建模，例如聚类或分类任务。</td>
</tr>
<tr>
<td>正态化流 VAE</td>
<td>用一系列可逆变换构造 <span class="arithmatex">\(q_\phi(z \mid x)\)</span>，提高后验分布的灵活性。</td>
<td>后验分布更灵活，适合复杂分布建模。</td>
<td>增加计算复杂度，需要设计合理的流模型。</td>
<td>高维复杂分布的表示和建模。</td>
</tr>
<tr>
<td>离散 VAE</td>
<td>假设潜变量是离散变量，用 Gumbel-Softmax 技巧实现可微优化。</td>
<td>适合离散潜变量空间（如分类或文本生成）。</td>
<td>无法捕捉连续分布的细节，对温度参数<span class="arithmatex">\(\tau\)</span>敏感。</td>
<td>离散数据生成或分类任务。</td>
</tr>
<tr>
<td>能量模型 VAE</td>
<td>用能量函数定义 <span class="arithmatex">\(q_\phi(z \mid x)\)</span>,如<span class="arithmatex">\(q_\phi(z \mid x) \propto \exp(-E_\phi(x, z))\)</span>，并通过 MCMC 采样。</td>
<td>非参数化，能够灵活建模复杂分布。</td>
<td>采样效率较低，计算成本较高。</td>
<td>高度复杂或未知分布建模。</td>
</tr>
<tr>
<td>对抗式 VAE (AAE)</td>
<td>用 GAN 的对抗学习替代 KL 散度正则化，判别器用于匹配 <span class="arithmatex">\(q_\phi(z)\)</span> 和 <span class="arithmatex">\(p(z)\)</span>。</td>
<td>不需要显式计算 KL 散度，适合复杂分布。</td>
<td>对抗训练可能不稳定，需要精心调试。</td>
<td>复杂数据分布生成任务。</td>
</tr>
<tr>
<td>β-VAE</td>
<td>在 VAE 损失中增加 KL 散度的权重 <span class="arithmatex">\(1\beta &gt; 1\)</span>，强调潜变量的压缩性和解耦性。</td>
<td>提高潜变量的表示质量，更适合表征学习任务。</td>
<td>可能导致重构性能下降，平衡重构和正则化较难。</td>
<td>表征学习，特征解耦（如生成 disentangled 表示）。</td>
</tr>
<tr>
<td>层次化 VAE</td>
<td>引入多个层次的潜变量，例如 $z_1 \sim q_\phi(z_1 \mid x), z_2 \sim q_\phi(z_2 \mid z_1) $，捕捉分布的层次特性。</td>
<td>能够更好地表示复杂数据分布的层次关系。</td>
<td>增加模型复杂度，训练更困难。</td>
<td>高维复杂数据，例如图像或自然语言处理任务。</td>
</tr>
</tbody>
</table>
<h2 id="normalize-flow-nf">normalize flow （NF）</h2>
<p>如果映射是可逆的，那么我们就可以直接计算出生成样本的密度函数(分布), 这个时候直接优化最大似然就行了。
<img alt="alt text" src="../../../images/image-20.png" /></p>
<p>归一化流（Normalizing Flow）是一种概率密度估计方法，其核心思想是通过一系列可逆的变换，将一个复杂分布映射到一个简单分布（通常是标准正态分布），这些变换是由具有可微参数的函数定义的，因此可以通过最大似然估计对模型进行训练。以下是从最大似然的角度详细解释归一化流的原理：</p>
<h3 id="_4">最大似然估计</h3>
<ol>
<li><strong>目标：通过最大似然估计复杂分布的概率密度</strong> 给定一个数据集 <span class="arithmatex">\(\{x_1, x_2, \dots, x_N\}\)</span>，我们希望拟合一个概率分布 <span class="arithmatex">\(p_X(x)\)</span> 来描述数据的生成过程。通过最大似然估计（MLE)),目标是最大化模型分布对数据的对数似然：<span class="arithmatex">\(\mathcal{L} = \sum_{i=1}^N \log p_X(x_i)\)</span>.</li>
</ol>
<p>由于直接建模 <span class="arithmatex">\(p_X(x)\)</span> 可能非常复杂，归一化流通过变换将复杂分布 <span class="arithmatex">\(p_X(x)\)</span> 映射到一个简单的分布（如正态分布）。</p>
<ol>
<li><strong>可逆变换与变化公式</strong> 归一化流假设数据 <span class="arithmatex">\(x\)</span> 可以通过一系列可逆变换 <span class="arithmatex">\(f\)</span> 从一个简单的分布 <span class="arithmatex">\(p_Z(z)\)</span> 中生成：$
 x = f(z), \quad z = f^{-1}(x),
$
其中 <span class="arithmatex">\(z\)</span> 是潜在空间中的表示，其概率密度为 <span class="arithmatex">\(p_Z(z)\)</span>。根据概率变化公式，<span class="arithmatex">\(x\)</span> 的概率密度可以通过变换的雅可比行列式计算为：$
 p_X(x) = p_Z(z) \left| \det \frac{\partial f^{-1}}{\partial x} \right|,
$</li>
</ol>
<p>或者等价地：
$
 \log p_X(x) = \log p_Z(z) - \log \left| \det \frac{\partial f(x)}{\partial x} \right|.
$
这里 <span class="arithmatex">\(\det \frac{\partial f(x)}{\partial x}\)</span> 是变换 <span class="arithmatex">\(f\)</span> 的雅可比矩阵的行列式。</p>
<hr />
<ol>
<li><strong>最大似然估计</strong> 为了最大化对数似然 <span class="arithmatex">\(\mathcal{L}\)</span>，需要计算每个数据点 <span class="arithmatex">\(x\)</span> 的对数密度：</li>
<li>
<p><strong>计算潜在变量 <span class="arithmatex">\(z\)</span>：</strong>
利用 <span class="arithmatex">\(z = f^{-1}(x)\)</span>，将输入数据 <span class="arithmatex">\(x\)</span> 映射到潜在空间。</p>
</li>
<li>
<p><strong>计算简单分布 <span class="arithmatex">\(p_Z(z)\)</span>：</strong>
简单分布通常选择为标准正态分布 <span class="arithmatex">\(\mathcal{N}(0, I)\)</span>，因此 <span class="arithmatex">\(\log p_Z(z)\)</span> 可以直接通过 <span class="arithmatex">\(z\)</span> 的值计算。</p>
</li>
<li>
<p><strong>计算雅可比行列式：</strong>
变换 <span class="arithmatex">\(f(x)\)</span> 必须设计成易于计算其雅可比行列式 <span class="arithmatex">\(\det \frac{\partial f(x)}{\partial x}\)</span>。</p>
</li>
</ol>
<p>最终，通过优化参数，最大化以下对数似然：
$
 \log p_X(x) = \log p_Z(f^{-1}(x)) - \log \left| \det \frac{\partial f(x)}{\partial x} \right|.
$</p>
<hr />
<ol>
<li><strong>设计归一化流的变换</strong> 为了有效训练归一化流，变换 <span class="arithmatex">\(f\)</span> 通常需要满足以下要求：</li>
<li>
<p><strong>可逆性</strong> ：确保 <span class="arithmatex">\(f\)</span> 和 <span class="arithmatex">\(f^{-1}\)</span> 易于计算。</p>
</li>
<li>
<p><strong>雅可比行列式高效计算</strong> ：使得 <span class="arithmatex">\(\det \frac{\partial f(x)}{\partial x}\)</span> 的计算成本较低。</p>
</li>
</ol>
<p>常用的变换包括：</p>
<ol>
<li>
<p><strong>Affine Coupling Layer</strong> ：只对部分变量进行变换，简化雅可比行列式的计算。</p>
</li>
<li>
<p><strong>Spline Flows</strong> ：基于分段函数的流，能够捕获更多复杂性。</p>
</li>
<li>
<p><strong>RealNVP</strong>  和 <strong>Glow</strong> ：利用特定的结构设计高效的变换。</p>
</li>
</ol>
<h3 id="loss">Loss 的公式直观拆解</h3>
<p>在归一化流中，对数似然可以写为：
$$
 \log p_X(x) = \log p_Z(f^{-1}(x)) - \log \left| \det \frac{\partial f(x)}{\partial x} \right|.
$$
训练的目标是<strong>最大化这个对数似然</strong> ，即最小化负对数似然（Negative Log-Likelihood, NLL）：</p>
<div class="arithmatex">\[
 \text{Loss} = -\mathbb{E}_{x \sim p_X} \left[ \log p_Z(f^{-1}(x)) - \log \left| \det \frac{\partial f(x)}{\partial x} \right| \right].
\]</div>
<p>从直观角度，Loss 的两个部分可以理解为：</p>
<ol>
<li><strong>潜在空间的负对数密度</strong> （<span class="arithmatex">\(-\log p_Z(f^{-1}(x))\)</span>）：</li>
<li>
<p>这个项表示数据点 <span class="arithmatex">\(x\)</span> 映射到潜在空间的点 <span class="arithmatex">\(z = f^{-1}(x)\)</span> 在简单分布 <span class="arithmatex">\(p_Z(z)\)</span> 上的概率密度。</p>
</li>
<li>
<p>直观上，越高的密度表示 <span class="arithmatex">\(x\)</span> 被模型解释得越好，Loss 越小。</p>
</li>
<li>
<p>目标是让数据点 <span class="arithmatex">\(z\)</span> 更接近简单分布的高密度区域（如标准正态分布的中心）。</p>
</li>
<li>
<p><strong>变换复杂性的代价</strong> （<span class="arithmatex">\(-\log \left| \det \frac{\partial f(x)}{\partial x} \right|\)</span>）：</p>
</li>
<li>
<p>这个项量化了变换 <span class="arithmatex">\(f\)</span> 的复杂性，特别是变换如何拉伸或压缩空间。</p>
</li>
<li>
<p>直观上，如果变换需要对数据进行大范围的扭曲或拉伸来匹配数据分布，雅可比行列式会较大，导致这个项的值增加，从而损失增大。</p>
</li>
<li>
<p>目标是让变换 <span class="arithmatex">\(f\)</span> 尽量简单，同时能有效匹配数据分布。</p>
</li>
</ol>
<hr />
<h3 id="_5">用日常比喻直观理解</h3>
<p>可以将 Loss 的两个部分类比为：</p>
<ol>
<li><strong>适配数据的过程</strong> ：</li>
<li>
<p>想象你有一块布（简单分布），需要将它拉伸和折叠（变换）以完全覆盖一个复杂的地形（数据分布）。</p>
</li>
<li>
<p>布的每个部分越接近目标地形的实际形状（即概率密度高的区域），就说明你越贴合目标，第一项的 Loss 越小。</p>
</li>
<li>
<p><strong>拉伸布的复杂性</strong> ：</p>
</li>
<li>如果需要对布进行非常复杂的变形，布会变得更紧或更松（对应雅可比行列式的变化），这会增加第二项的 Loss。</li>
</ol>
<p>最终，Loss 就是这两者的加权总成本：既希望布能很好地覆盖地形，又希望变形的过程不要过于复杂。</p>
<p>实际实现的时候，normalize flow 可以设计为可逆映射的复合。</p>
<h2 id="refenrence">Refenrence:</h2>
<ul>
<li><a href="https://chatgpt.com/c/6788e69d-7f1c-8001-a4e2-f6c8abda90db">normalize flow</a></li>
<li><a href="https://www.cnblogs.com/Meloniala/p/18285101">Diffusion 综述阅读笔记</a></li>
</ul>


</section>
</div> <!-- end of search-noresults -->
<div class="search-results">
<div class="has-results">

<h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
<ul class="search-results-list"></ul>

</div> <!-- end of has-results -->
<div class="no-results">

<h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>

</div> <!-- end of no-results -->
</div> <!-- end of search-results -->
</div> <!-- end of book-search-results -->

</div> <!-- end of page-inner -->
</div> <!-- end of page-wrapper -->

</div> <!-- end of body-inner -->

</div> <!-- end of book-body -->
<script src="../../../js/main.js"></script>
<script src="../../../javascripts/mathjax.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="../../../search/main.js"></script>
<script src="../../../js/gitbook.min.js"></script>
<script src="../../../js/theme.min.js"></script>
</body>
</html>