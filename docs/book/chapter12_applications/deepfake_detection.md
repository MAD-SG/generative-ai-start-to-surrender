# Deepfake Detection
## Review
### An Analysis of Recent Advances in Deepfake Image Detection in an Evolving Threat Landscape

- author: 2024
- url: <https://arxiv.org/pdf/2404.16212>
- Sifat Muhammad Abdullah

- fully synthetic images created using generative models are considered
- partial synthetic images are not considered in this paper

#### Introduction

1. Current SOTA methods:
    - ture statistics [13]
    - finding imperfections in the frequency spectrum [12]
    - local patches [17]

2. Threat
    - Emergence of lightweight methods that allow users to customize large generative models
        Previously, generate images from few of generative models by GAN or diffusion
        Today, more than 3000 variants of generative models in platform like Huggingface, Civitai
    - Use foundation model fine-tuning to fool deepfake detection models
3. Contirbution
    - critical analysis on the trianing and evaluation methods in current SOTA
    - performance on user-customized generative models
    - create adversarial samples with foundation model without adversary noise

#### Generative models

1. Stable Difussion
2. StyleClip

#### Defense Models

1. UnivCLIP (2023): first one that use foundation model to detect deepfake
2. DE-FAKE: augment the imageâ€™s embedding along with an embedding of the text prompt. The intuition is that real images usually have more information than their respective captions, whereas fake images generated from prompts only show content that is specific to that prompt. Achieve **90.9%** on DALLÂ·E 2 images
3. DCT: frequency domain provides discriminatory features for deepfake detection. Logscaled version of the DCT features. DCT achieves 97.7% and 73% accuracy on images generated by GAN and Diffusion model,
4. Patch-Forensics: searching for artifacts in local patches of the image provides more generalizable patterns for detection
5. GramNet: texture statistics of fake images (e.g., face content) are significantly different from real images.
6. Resynthesis: generating testing images based on different auxiliary tasks (super-resolution, denoising and colorization)
7. CNN-F: CNN-based generators leave detectable fingerprints. Highlights that the detector needs to be trained only on images from a single CNN-based generator to generalize across different fake sources
8. MesoNet: Originally designed to detect deepfake videos. microscopic (ä¸­è§‚å±‚é¢) features of real images are more diverse than those of fake images compared with macroscopic or microscopic features. Inception module has better performance than usual convolutional modules.

| Defense           | SD Precision | SD Recall | SD F1  | StyleCLIP Precision | StyleCLIP Recall | StyleCLIP F1 |
|------------------|--------------|-----------|--------|----------------------|------------------|--------------|
| UnivCLIP         | 90.20        | 93.90     | 92.01  | 93.79                | 92.20            | 92.99        |
| DE-FAKE          | 93.82        | 94.20     | 94.01  | 74.41                | 78.80            | 76.54        |
| DCT              | 100          | 88.80     | 94.07  | 100                  | 99.60            | 99.80        |
| Patch-Forensics  | -            | -         | -      | 91.76                | 91.30            | 91.53        |
| Gram-Net         | 99.99        | 99.10     | 99.55  | 99.99                | 99.60            | 99.80        |
| Resynthesis      | 85.39        | 86.50     | 85.94  | 98.80                | 98.70            | 98.75        |
| CNN-F            | 99.41        | 83.80     | 90.94  | 99.90                | 97.10            | 98.48        |
| MesoNet          | 99.99        | 98.00     | 98.98  | 96.70                | 99.50            | 98.08        |

#### current limitations

1. lack control of the content and quality of training data: real and fake images should be consistent in content and quality
2. Lack adversary attack
3. Prior work only focused only on limited content types, e.g., faces, animals, bedrooms, and buildings

#### findings
FM: full fintune model
user-cutomized models: use full fituning or the Lora finetuning to train new models

1. All models have performance degradation on user-cutomized models
2. Soly relay on foundation model's feature is not enough to have generalization on deepfake detection
3. Frequency domain show the best generalization performance
4. CNN-based model has the worst generalization performance
5. Content-agnostic features can help boost generalization performance for deepfake detection.
6. Combining domain-specific features (i.e., features known to identify imperfections in fake images) with features from a foundation model
    improves generalization. Cimbine the DCT features with the foundation model feature achieves the best performance
7. adversary attack: attacker has a realistic photo, and manipulate the photo by the text prompt like 'a smilling face'.
    - How to do adversary attack:
        1. First, train three surrogate deepfake classifiers using dataset with fake images from current generator and real images from public datasets
        2. Second, adversary train the generator again the surroage deepfake classifiers for each deceptive image which can be detected by current deepfake detection model (the testing model, not the surrogate model) with looss:
            - classification loss
            - perception loss from vgg pretrained network in imagenet
        For each image, it takes 39 seconds to generate an adversarial image using dgx A100

    ![alt text](../../images/image-151.png)
    For each surrogate model, the adversary attack will leads to the performance drop ($\Delta R$)
8. Defense based on the frequency features is the weakest against adversary attack
9. Defense using foundation model is the strongest against adversary attack
10. Defense with stronger foundation model is more robust against adversary attack
11. Adversary training can improve the robustness of adversary attack

    ![alt text](../../images/image-152.png)

## Deepfake Image Detection
### FSBI
![alt text](../../images/image-153.png)

- year: 2024 Jun
- author: Ahmed Abul Hasnanaath
- instituiton: Information and Computer Science Department, King Fahd University of Petroleum and Minerals
- code: at <https://github.com/gufranSabri/FSBI>
- main contribution
    1. data synthetic methods: SBI methods generate more various fake images for training
    2. use discrete wavelet transform to extract features from images

#### SBI (self blended image)

Give an iamge, it first do augmentations including

- RGB and HSV jitters
- contrast and brightness jitters
- downsampling
- translation
For single image, it do two different random augmentations and produces $I_t$ and $I_s$.

Secondly, it detect the landmark on $I_s$ using some landmark detection model and produce the face mask with the convex hull of the detected landmarks. After applying the gaussian and dilation on the face mask, it will produce $M$.
Lastly, it combine the $I_s$ and $I_t$ with the mask according to the formula

$$I_{SBI} = I_s \cdot M + I_t \cdot (1 - M)$$


Here $M$ is the gray tensor in range [0, 1]. The edge of the mask could be smoothed by gaussian and dilation to produce continuous self blending image.

#### Frequency Feature Generator
It first split the image into R,G,B channel seperately
Secondly, it calculate the wavelet coefficients of each channel
Last, it resize the coefficients to the same size and stack them together

### DE-FAKE

- zeyang sha
- salesforce research
- 2023 Jan
- main contribution
    1. proposed three questions
        1. whether the fake image be distinguished
        2. can we attribute the fake images to their sources (which model generated the image)
        3. what kind of prompt is more likely to generate authentic images

#### Detection method
Train a binary classifier on the images generated by one text2image model and then evaludate on the images generated by other models. Two types of detector

1. Image only detector: accepts only image as input
2. Hybrid detector: accepts both image and its prompt as input

##### Image only detector

1. dataset
    - random sample 20k images from MSCOCO, treat as real images
    - use stable diffusion model to generate another 20k images, treat as fake images with the prompt
2. network
    - resnet18
    - binary output

##### Hybrid detector

1. dataset
    Same construction as image only detector
2. network
    - pretrained clip model to extract both image and text embedding and concat them together, followed by 2-layer MLP
3. captions
    - Use the captions of the image if it is provided by the dataset
    - Otherwise, use BLIP model to generate captions for queried images

Questions:

- Cause image only detector use resnet18, why not use more powerful model? or use same model as hybrid detector to compare their performance fairly

Results
![alt text](../../images/image-154.png)
Hybird detector can obtain much better performance than image only detector
#### Fake Image Attribute (Generator source identification)
The goal is to predict which generator is used to generate the image
To train such model, we first generate the iamges from different text2image models (3 models, SD, LD, and GLIDE)
Network construction is similar to the previous section but now their are four classes

- 0: real
- 1: SD
- 2: LD
- 3: GLIDE

Results

|                                   | MSCOCO | Flickr30k |
|-----------------------------------|--------|-----------|
| Image-Only                        | 0.864  | 0.863     |
| Hybrid (natural prompts)          | 0.936  | 0.933     |
| Hybrid (generated prompts)        | 0.903  | 0.892     |

#### Prompt analysis
Analysis which kind of prompt would has more likely to pass the deepfake detection
##### Semantic Analysis

- **Topic-based Grouping**:
  - Used MSCOCO's 80 predefined topics to group prompts
  - Measured proportion of fake images misclassified as real by topic
  - Results: "skis," "snowboard," and animal-related topics produced most realistic fakes

- **Embedding Cluster Analysis**:
  - Used BERT-based sentence transformer for prompt embeddings
  - Applied DBSCAN clustering for semantic grouping
  - Finding: "person"-related prompts generated most realistic fakes
  - "Skis" and "snowboard" topics effective because they often included people

- **Typical Prompts Analysis**:
  - Extracted top 5 most realistic and most fake-looking prompts
  - Finding: Detailed object descriptions (4/5 top realistic prompts) outperformed environment descriptions (4/5 top fake-looking prompts)

##### Structural Analysis

- **Prompt Length**:
  - Tested 5,000 random MSCOCO prompts with Stable Diffusion
  - Finding: 25-75 word prompts generated most realistic images
  - Very short or long prompts performed worse

- **Noun Proportion**:
  - Used NLTK to analyze noun ratio in prompts
  - Finding: No significant correlation between noun proportion and image realism

### Mastering Deepfake Detection: A Cutting-edge Approach to Distinguish GAN and Diffusion-model Images
![alt text](../../images/image-143.png)

- <https://dl.acm.org/doi/pdf/10.1145/3652027>
- University of Catania
- LUCA GUARNERA
- 2024

![alt text](../../images/image-142.png)

- AttGAN: <https://github.com/LynnHo/AttGAN-Tensorflow>
- CycleGAN: <https://github.com/junyanz/pytorch-CycleGANand-pix2pix>
- GDWCT: <https://github.com/WonwoongCho/GDWCT>
- IMLE: <https://github.com/zth667/Diverse-ImageSynthesis-from-Semantic-Layout>
- ProGAN: <https://github.com/tkarras/progressive_growing_of_gans>
- StarGAN: <https://github.com/wkentaro/StarGAN>
- StarGAN-v2: <https://github.com/clovaai/stargan-v2>
- StyleGAN: <https://github.com/NVlabs/stylegan>
- StyleGAN2: <https://github.com/NVlabs/stylegan2>
- DALL-E 2: <https://github.com/lucidrains/DALLE2pytorch>
- GLIDE: <https://github.com/openai/glide-text2im>
- Latent Diffusion: <https://github.com/CompVis/latent-diffusion>
- Stable Diffusion: <https://github.com/CompVis/stable-diffusion>
![alt text](../../images/image-145.png)
![alt text](../../images/image-144.png)
- in the spetral space, â€œrealâ€ class has the  isotropic behavior

### Leveraging Frequency Analysis for Deep Fake Image Recognition
![alt text](../../images/image-146.png)

- <https://proceedings.mlr.press/v119/frank20a/frank20a.pdf>>
- Joel Frank
- Horst GÂ¨ortz Institute for ITSecurity, Bochum, Germany
- 2020

- Upsampling will leads to spectral differences
    ![alt text](../../images/image-147.png)
- both upsampling and downsampling operations have recently been linked to compromising shift invariance in neural networks, i.e., they cause classifier predictions to vary dramatically due to a simple one-pixel shift in the input image (Azulay & Weiss, 2018). Recently, Zhang (2019)

## Partial Deepfake Image Detection
### SIDA: Social Media Image Deepfake Detection, Localization and Explanation with Large Multimodal Model

- Zhenglin Huang1
- <https://arxiv.org/pdf/2412.04292>
- University of Liverpool, UK

#### contribution

1. Social media Image Detection data Set (SID-Set)
![alt text](../../images/image-148.png)
2. SIDA
![alt text](../../images/image-149.png)

#### Training Objectives Summary

The SIDA model training involves three main loss components:

1. **Detection loss** (\(\mathcal{L}_{det}\)) â€“ using CrossEntropy loss for detecting elements.
2. **Segmentation mask loss** (\(\mathcal{L}_{mask}\)) â€“ a weighted combination of Binary Cross Entropy (BCE) loss and DICE loss:

   $$
   \mathcal{L}_{mask} = \lambda_{bce} \mathcal{L}_{BCE}(\hat{M}, M) + \lambda_{dice} \mathcal{L}_{DICE}(\hat{M}, M)
   $$


3. **Text generation loss** (\(\mathcal{L}_{txt}\)) â€“ used in the fine-tuning phase with ground truth descriptions from 3,000 images:

   $$
   \mathcal{L}_{txt} = \mathcal{L}_{CE}(\hat{y}_{des}, y_{des})
   $$


The overall training loss during initial training:

$$
\mathcal{L} = \lambda_{det} \mathcal{L}_{det} + \lambda_{mask} \mathcal{L}_{mask}
$$


In the fine-tuning stage, the full loss function includes text generation:

$$
\mathcal{L}_{total} = \lambda_{det} \mathcal{L}_{det} + \lambda_{mask} \mathcal{L}_{mask} + \lambda_{txt} \mathcal{L}_{txt}
$$


Here, \(\lambda_{det}\), \(\lambda_{mask}\), and \(\lambda_{txt}\) are weighting factors used to balance the contributions of each loss term.

#### Results
![alt text](../../images/image-150.png)

### Detect Any Deepfakes: Segment Anything Meets Face Forgery Detection and Localization
![alt text](../../images/image-156.png)

- institution: xiamen university
- Yingxin Lai
- 2023 June
- proposed DADF (Detect Any Deepfakes)
- code: <https://github.com/laiyingxin2/DADF>

#### contribution
- finetune on segment anything for face forgery detection and localization
- reconstruction guided attention module is proposed
    1. Add gaussian noise to the original images
    2. Use differences of features from original images and the noised image to hiligh the differences as the attention.
        If it is high, means the part different from the forgery
    3. In this way, suppose the final feature only care about the real feature
    4. use L1 norm to calculate the difference between features from real images and the gaussian images (simulation of forgey)
    ![alt text](../../images/image-157.png)
    Results
    ![alt text](../../images/image-158.png)
#### Question
How much does the performance of the model improve after finetuning compared with train from scratch?

### Weakly-supervised deepfake localization in diffusion-generated images
- url: [text](https://openaccess.thecvf.com/content/WACV2024/papers/Tantaru_Weakly-Supervised_Deepfake_Localization_in_Diffusion-Generated_Images_WACV_2024_paper.pdf)
- code: [text](https://github.com/bit-ml/dolos)
- 2024
- institution: Bitdefender
- author: Dragos-Constaintin Tantaru
- contribution

ä»¥ä¸‹æ˜¯è¿™æ®µæ–‡å­—çš„ç»“æ„åŒ–æ€»ç»“ï¼š


| é¡¹ç›®                         | å†…å®¹                                                                                     |
|----------------------------|------------------------------------------------------------------------------------------|
| **ç ”ç©¶èƒŒæ™¯**                 | Denoising Diffusion Modelsï¼ˆæ‰©æ•£æ¨¡å‹ï¼‰ç”Ÿæˆèƒ½åŠ›å¼ºï¼Œå¼•å‘å›¾åƒçœŸå®æ€§æ‹…å¿§ã€‚ç°æœ‰æ£€æµ‹æ–¹æ³•å¤šåŸºäºGANï¼Œä»…è¾“å‡ºâ€œçœŸ/å‡â€æ ‡ç­¾ã€‚ |
| **æ ¸å¿ƒé—®é¢˜**                 | ç°æœ‰æ£€æµ‹æ¨¡å‹ç¼ºä¹å¯¹**ä¼ªé€ åŒºåŸŸçš„å®šä½èƒ½åŠ›**ï¼Œä»…åšäºŒåˆ†ç±»ï¼Œä¿¡æ¯ä¸è¶³ã€‚                                     |
| **ç ”ç©¶ç›®æ ‡**                 | å°†ä»»åŠ¡è½¬åŒ–ä¸º**å¼±ç›‘ç£å®šä½é—®é¢˜**ï¼Œè¾“å‡ºå›¾åƒä¸­è¢«ç¯¡æ”¹çš„åŒºåŸŸå›¾ï¼ˆlocalization mapï¼‰ã€‚                       |
| **æ–¹æ³•åˆ†ç±»**                 | ä¸‰ç±»æ–¹æ³•ï¼šåŸºäºè§£é‡Šï¼ˆexplanationsï¼‰ã€å±€éƒ¨å¾—åˆ†ï¼ˆlocal scoresï¼‰ã€æ³¨æ„åŠ›æœºåˆ¶ï¼ˆattentionï¼‰ã€‚            |
| **ç»Ÿä¸€æ¯”è¾ƒæ–¹æ³•**             | æ‰€æœ‰æ–¹æ³•ä½¿ç”¨ç›¸åŒçš„**Xceptionç½‘ç»œ**æ¶æ„ï¼Œç¡®ä¿å…¬å¹³å¯¹æ¯”ã€‚                                          |
| **å®éªŒè®¾è®¡**                 | æ„å»ºå¤šä¸ªæ§åˆ¶å˜é‡çš„æ•°æ®é›†ï¼Œ**å•ä¸€æ”¹å˜ç”Ÿæˆå™¨ã€ç›‘ç£æ–¹å¼æˆ–æ•°æ®æº**ï¼Œä»¥åˆ†æå…¶å¯¹æ€§èƒ½çš„å½±å“ã€‚               |
| **ä¸»è¦å‘ç°**                 | å±€éƒ¨å¾—åˆ†æ³•æ€§èƒ½æœ€ä½³ï¼Œå¯¹å¼±ç›‘ç£æ•æ„Ÿæ€§ä½ï¼Œä½†å¯¹ç”Ÿæˆå™¨/æ•°æ®é›†å˜åŒ–æ›´æ•æ„Ÿã€‚                                |
| **ç ”ç©¶è´¡çŒ®**                 | æä¾›å¯¹å¼±ç›‘ç£ä¼ªé€ åŒºåŸŸæ£€æµ‹è®¾è®¡ç©ºé—´çš„ç³»ç»Ÿåˆ†æï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„å¯è¡Œæ€§ã€‚                                 |

Different Sturcture of the detection and localization of deepfakes
![alt text](../../images/image-159.png)


#### ğŸ“Š å››ç§ä¼ªé€ åŒºåŸŸå®šä½æ–¹æ³•æ¯”è¾ƒè¡¨

| æ–¹æ³•ç±»å‹         | æ–¹æ³•å        | ä¸»ä½“ç»“æ„       | åŸç†ä¸æœºåˆ¶                                                                 | ç›‘ç£æ–¹å¼              | æœ¬æ–‡æ”¹è¿› / è¯´æ˜                                               |
|------------------|----------------|----------------|------------------------------------------------------------------------------|------------------------|----------------------------------------------------------------|
| ğŸ§  è§£é‡Šé©±åŠ¨ç±»     | **GradCAM**    | Xception (block 11) | ç”¨ GradCAM åœ¨å€’æ•°ç¬¬äºŒå±‚æ¿€æ´»å›¾ä¸Šç”Ÿæˆçƒ­å›¾ï¼ˆå®šä½è§£é‡Šï¼‰                              | å¼±ç›‘ç£ / å…¨ç›‘ç£å‡å¯      | æ·»åŠ å…¨å·ç§¯å±‚æ”¯æŒå…¨ç›‘ç£è®­ç»ƒï¼Œå¯¹ GradCAM å®šé‡è¯„ä¼°æ€§èƒ½                         |
| ğŸ“¦ å±€éƒ¨è¯„åˆ†ç±»     | **Patches (Patchâ€“Forensics)** | Truncated Xception (block 2) | ä½¿ç”¨ `1Ã—1` å·ç§¯å°†ä¸­é—´ç‰¹å¾æ˜ å°„åˆ° patch-level åˆ†æ•°ï¼Œè®­ç»ƒæ—¶å¯¹ patch å•ç‹¬ç›‘ç£            | å¼±ç›‘ç£ / å…¨ç›‘ç£å‡å¯      | åŸæ–¹æ³•åªå±•ç¤ºå¯è§†åŒ–ï¼Œæœ¬ç ”ç©¶é‡åŒ–å…¶å®šä½æ•ˆæœï¼›å…¨ç›‘ç£æ—¶ä½¿ç”¨ mask æ›¿ä»£ image label |
| ğŸ” æ³¨æ„åŠ›æœºåˆ¶ç±»   | **Attention**  | Xception + Learned Mask | ç½‘ç»œå­¦ä¹  attention maskï¼ŒåŒæ—¶ç›‘ç£æ•´ä½“å›¾åƒçœŸå‡å’Œ mask çš„æœ€å¤§å€¼ä¸æ ‡ç­¾ä¸€è‡´æ€§              | å¼±ç›‘ç£ / å…¨ç›‘ç£å‡å¯      | å°† L1 æŸå¤±æ”¹ä¸º binary cross-entropyï¼Œè°ƒå‚ä¼˜åŒ– Î»ï¼Œæå‡ç¨³å®šæ€§ä¸æ€§èƒ½               |
| ğŸ¯ å…¨ç›‘ç£ä¸Šé™å¯¹æ¯” | **å…¨ç›‘ç£ç‰ˆæœ¬** | åŒä¸Šä¸‰ç§æ–¹æ³•æ”¹å†™ç‰ˆ | æ˜ç¡®æä¾›ä¼ªé€ åŒºåŸŸ groundtruth maskï¼Œä½œä¸º upper bound åŸºçº¿                              | å…¨ç›‘ç£                 | æ¯ç§æ–¹æ³•æŒ‰éœ€ä¿®æ”¹æ”¯æŒ mask ç›‘ç£è®­ç»ƒ                                  |


#### ğŸ“¦ æ•°æ®é›†æ„å»ºæ¦‚è§ˆ
![alt text](../../images/image-160.png)
| æ•°æ®ç±»å‹ | æ•°æ®æ¥æº | æ“ä½œæ–¹æ³• | æ•°æ®é‡ | ç”¨é€”è¯´æ˜ |
|----------|----------|----------|--------|-----------|
| âœ… **çœŸå®å›¾åƒ** | CelebA-HQã€FFHQ | åŸå§‹äººè„¸å›¾åƒ | å„ 9k train / 900 val | ä½œä¸ºçœŸå®æ ·æœ¬ç”¨äºè®­ç»ƒä¸æµ‹è¯• |
| âœ… **å…¨å›¾ä¼ªé€ ** | Diffusion (P2 æ¨¡å‹) | ä½¿ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆå®Œæ•´å›¾åƒ | å„ 9k train / 1k val | ç”¨äºå¼±ç›‘ç£è®­ç»ƒï¼ˆSetup Aï¼‰ |
| âœ… **å±€éƒ¨ä¼ªé€ ** | CelebA-HQã€FFHQ | ä½¿ç”¨ä¸åŒæ–¹æ³•è¿›è¡Œé¢éƒ¨éƒ¨ä½ä¿®è¡¥ï¼ˆçœ¼ç›ã€å˜´ã€é¼»ç­‰ï¼‰ | å¤šè¾¾ 30k train / 8.5k test | ç”¨äºå¼±/å¼ºç›‘ç£è®­ç»ƒä¸è¯„ä¼°ï¼ˆSetup B/Cï¼‰ |

---

##### ğŸ§ª å±€éƒ¨ä¼ªé€ æ–¹æ³•ç»†èŠ‚

| æ–¹æ³• | åŸç† | æ¨¡å‹ | å¤‡æ³¨ |
|------|------|------|------|
| **Repaintâ€“P2** | åœ¨åƒç´ ç©ºé—´ç”¨æ‰©æ•£æ¨¡å‹æŒ‰ mask ä¿®è¡¥ | åŸºäº P2 æ¨¡å‹ | æ˜¯ä¸»æµ‹è¯•é›†ï¼Œæä¾› ground truth mask |
| **Repaintâ€“LDM** | åœ¨**latent ç©ºé—´**æ‰§è¡Œæ‰©æ•£ä¿®è¡¥ | åŸºäº LDM æ¨¡å‹ | æ½œåœ¨ç©ºé—´ä¿®å¤æ•ˆæœæ›´è‡ªç„¶ï¼Œæ›´éš¾å®šä½ |
| **LaMa** | Fourier å·ç§¯è¿›è¡ŒåŒºåŸŸä¿®è¡¥ | è‡ªåŠ¨ç¼–ç å™¨ç»“æ„ | ä¼ ç»Ÿå›¾åƒä¿®è¡¥æ–¹æ³•ï¼Œæ˜“äºè®­ç»ƒ |
| **Pluralistic** | æ¡ä»¶ VAE + å¯¹æŠ—è®­ç»ƒ | GAN ç»“æ„ | å¤šæ ·æ€§å¼ºçš„ä¿®è¡¥è¾“å‡ºï¼Œè§†è§‰çœŸå®æ„Ÿé«˜ |

---

##### ğŸ§° ä¿®è¡¥åŒºåŸŸæ¥æºä¸å¤„ç†

- **CelebA-HQ ä½¿ç”¨ CelebAMask-HQ æä¾›çš„äººè„¸éƒ¨ä½æ ‡ç­¾ï¼ˆå¦‚çœ¼ã€å˜´ã€é¼»ï¼‰**
- **FFHQ ä½¿ç”¨é¢„è®­ç»ƒ face segmentation æ¨¡å‹ç”Ÿæˆ mask**
- å°éƒ¨ä½åŒºåŸŸå¦‚çœ¼ç›ã€å˜´éƒ¨ä¼šéšæœºè†¨èƒ€ 1â€“15 åƒç´ ä»¥å¢åŠ å˜å¼‚æ€§

---

##### ğŸ§ª ä¸‰ç§è®­ç»ƒè®¾ç½®ï¼ˆç›‘ç£ç­‰çº§ï¼‰

| Setup | ç›‘ç£ç±»å‹ | æ•°æ®æ¥æº | ç‰¹ç‚¹ |
|-------|----------|----------|------|
| **A** | ä»…å›¾åƒçº§æ ‡ç­¾ï¼Œå…¨å›¾ä¼ªé€  | P2 æ¨¡å‹ç”Ÿæˆ | å‡å›¾å®Œå…¨ä¼ªé€  |
| **B** | ä»…å›¾åƒçº§æ ‡ç­¾ï¼Œå±€éƒ¨ä¼ªé€  | Repaintâ€“P2 | è¾“å…¥æ ‡ç­¾â€œå‡â€ï¼Œä½†éƒ¨åˆ†åŒºåŸŸä»ä¸ºçœŸ |
| **C** | å¼ºç›‘ç£ï¼Œæä¾› mask | Repaintâ€“P2 | mask ä½œä¸º ground truthï¼Œç”¨äºè¯„ä¼°ä¸Šé™ |

---

##### âœ… æ•°æ®æ„å»ºç›®æ ‡ä¸ä¼˜åŠ¿

1. **æ§åˆ¶å˜é‡è®¾è®¡**ï¼šé€šè¿‡æ”¹å˜ *generatorã€ç›‘ç£ç±»å‹ã€æ•°æ®æº* ä¸­å•ä¸€å› ç´ ï¼Œç³»ç»Ÿç ”ç©¶æ€§èƒ½å½±å“ã€‚
2. **å¤šç”Ÿæˆå™¨å¯¹æ¯”**ï¼šå…è®¸æµ‹è¯•æ¨¡å‹åœ¨è®­ç»ƒ-æµ‹è¯•åˆ†å¸ƒä¸ä¸€è‡´ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
3. **å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®**ï¼šå…±æ„å»ºå‡ºè¶…è¿‡ **12.5 ä¸‡å¼ å›¾åƒ**ï¼ŒåŒ…å«å¤šç§ç¯¡æ”¹ç±»å‹å’Œæ–¹æ³•ã€‚

##### ğŸ“ˆ å…³é”®å®éªŒç»“æœ
- Patches æ–¹æ³•è¡¨ç°æœ€ç¨³å®šï¼Œåœ¨æ‰€æœ‰ Setup ä¸‹éƒ½ä¼˜äº GradCAM å’Œ Attentionã€‚
- æ¨¡å‹åœ¨å¼±ç›‘ç£ä¸‹ä»å¯å­¦ä¹ åˆ°æœ‰æ•ˆçš„å±€éƒ¨ä¼ªé€ åŒºåŸŸï¼ˆå°¤å…¶æ˜¯ Patchesï¼‰ã€‚
- æ¨¡å‹å¯¹ ç”Ÿæˆå™¨å˜åŒ–æœ€æ•æ„Ÿï¼Œå¯¹ç›‘ç£ç±»å‹å˜åŒ–æ›´é²æ£’ã€‚
- å¯¹ LDM ç”Ÿæˆå›¾åƒçš„å®šä½æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå› å…¶æ½œç©ºé—´ä¿®è¡¥èƒ½â€œéšè—ç—•è¿¹â€ã€‚
- è·¨æ•°æ®é›†ï¼ˆCelebA â†” FFHQï¼‰æˆ–è·¨ç”Ÿæˆå™¨æ³›åŒ–èƒ½åŠ›æ˜¾è‘—ä¸‹é™ã€‚

Works that tackle localization rely on local noise fingerprint
patterns [21, 33, 40, 64], attention mechanisms [12, 13, 42]
or self-consistency checks [2, 27]. Very recent, concurrent
works proposed a forensic framework for general manipula-
tion localization [21] and a hierarchical fine-grained formu-
lation for image forgery detection [22].
### Multiâ€‘attentionâ€‘based approach for deepfake face and expression swap detection and localization
![alt text](../../images/image-161.png)

- paper: https://link.springer.com/content/pdf/10.1186/s13640-023-00614-z.pdf
- author: Saima Waseem
- institution: Universiti Teknologi Malaysia, Johor, Malaysia
- 2023
- code: <https://github.com/saimawaseem/Multi-Attention-Based-Approach-for-Deepfake-Face-and-Expression-Swap-Detection-and-Localization>
#### ğŸ§  **ä¸»è¦å†…å®¹æ¦‚è¿°**

è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§**åŸºäºå¤šé‡æ³¨æ„åŠ›æœºåˆ¶çš„å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶**ï¼Œç”¨äº**æ£€æµ‹å’Œå®šä½äººè„¸ç¯¡æ”¹çš„æ·±åº¦ä¼ªé€ ï¼ˆdeepfakeï¼‰å›¾åƒ**ï¼Œå°¤å…¶æ˜¯ä¸¤ç§å…¸å‹ä¼ªé€ å½¢å¼ï¼š

- **Face swapï¼ˆèº«ä»½æ›¿æ¢ï¼‰**
- **Expression swapï¼ˆè¡¨æƒ…æ›¿æ¢/é‡æ¼”ï¼‰**

è¯¥æ–¹æ³•èåˆäº†**ç©ºé—´åŸŸï¼ˆspatial domainï¼‰å’Œé¢‘ç‡åŸŸï¼ˆfrequency domainï¼‰ç‰¹å¾**ï¼Œé€šè¿‡å¼•å…¥**æ³¨æ„åŠ›æœºåˆ¶**ï¼Œæå‡ä¼ªé€ æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶èƒ½ç”Ÿæˆä¼ªé€ åŒºåŸŸçš„å®šä½å›¾ã€‚

---

#### ğŸš€ **æ ¸å¿ƒè´¡çŒ®**

1. **æå‡ºäº†å¤šé‡æ³¨æ„åŠ›æœºåˆ¶çš„U-Netç½‘ç»œç»“æ„**ï¼š
   - å¼•å…¥**spatial-channel Attention Blockï¼ˆscABï¼‰**ï¼Œåœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´åŠ¨æ€å¼ºè°ƒä¼ªé€ åŒºåŸŸçš„é‡è¦æ€§ã€‚
   - åœ¨è§£ç å™¨çš„æ¯ä¸ªè·³è·ƒè¿æ¥ä¸­é›†æˆäº†æ³¨æ„åŠ›æ¨¡å—ï¼Œæå‡å®šä½ç²¾åº¦ã€‚

2. **è”åˆä½¿ç”¨é¢‘åŸŸå’Œç©ºé—´åŸŸä¿¡æ¯**ï¼š
   - ä½¿ç”¨**FFTå˜æ¢æå–é¢‘ç‡ç‰¹å¾**ï¼Œä¸å±€éƒ¨åŒ–çš„ç©ºé—´ç‰¹å¾èåˆï¼Œæå‡æ£€æµ‹é²æ£’æ€§ã€‚
   - é‡‡ç”¨**Bilinear Pooling**æ–¹æ³•è¿›è¡Œç‰¹å¾èåˆï¼Œæé«˜è¡¨è¾¾èƒ½åŠ›ã€‚

3. **å¤šä»»åŠ¡å­¦ä¹ ç»“æ„ï¼šæ£€æµ‹ + å®šä½**ï¼š
   - åŒæ—¶ä¼˜åŒ–**åˆ†ç±»ä»»åŠ¡ï¼ˆæ˜¯å¦ä¼ªé€ ï¼‰**å’Œ**å®šä½ä»»åŠ¡ï¼ˆä¼ªé€ åŒºåŸŸï¼‰**ã€‚
   - ä½¿ç”¨**å›å½’æŸå¤±ï¼ˆL1/L2ï¼‰**è¿›è¡Œä¼ªé€ åŒºåŸŸå­¦ä¹ ï¼Œæ¯”ä¼ ç»Ÿåˆ†å‰²æŸå¤±ï¼ˆå¦‚Diceã€Focalï¼‰æ›´æœ‰æ•ˆã€‚

4. **å¼ºæ³›åŒ–èƒ½åŠ›**ï¼š
   - åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆFF++, DFDC-P, CelebDF, DFDï¼‰ä¸Šè¿›è¡Œ**è·¨æ•°æ®é›†å®éªŒ**ã€‚
   - è¡¨ç°ä¼˜äºæˆ–åª²ç¾ç°æœ‰SOTAæ¨¡å‹ï¼Œå°¤å…¶åœ¨è§†é¢‘å‹ç¼©ä¸¥é‡æ—¶ï¼Œä»ä¿æŒé«˜å‡†ç¡®ç‡ã€‚

5. **ä»£ç å¼€æºï¼Œä¾¿äºå¤ç°å’Œæ‹“å±•**ï¼š
   - GitHubåœ°å€ï¼šhttps://github.com/saimawaseem/Multi-Attention-Based-Approach-for-Deepfake-Face-and-Expression-Swap-Detection-and-Localization

---

#### ğŸ“Š å®éªŒäº®ç‚¹

- **å†…æ•°æ®é›†æµ‹è¯•**ï¼šåœ¨FF++æ•°æ®é›†ä¸Šï¼Œå‡†ç¡®ç‡å¯è¾¾ 99.97%ï¼ˆC23ï¼‰ï¼Œ96.47%ï¼ˆC40ï¼‰ã€‚
- **è·¨æ•°æ®é›†æµ‹è¯•**ï¼šåœ¨DFDC-Pä¸ŠAUCä¸º **79.10%**ï¼Œç›¸æ¯”MaDDæå‡è¶…è¿‡10%ã€‚
- **å®šä½æ€§èƒ½**ï¼šåœ¨FaceSwapæ•°æ®é›†ä¸ŠmIoUè¾¾ **70.04%**ï¼Œä¼˜äºå¤šä»»åŠ¡æ¨¡å‹å¦‚ADDå’ŒMulti-Taskã€‚

è¿™ç¯‡æ–‡ç« åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨äº†ä¸€äº›**æ ‡å‡†æ–¹æ³•**é…åˆ**ç‰¹åˆ«è®¾è®¡çš„æ•°æ®å¤„ç†å’Œå¢å¼ºç­–ç•¥**ï¼Œä»¥æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚ä»¥ä¸‹æ˜¯è¯¦ç»†æ€»ç»“ï¼š

---

#### ğŸ—ï¸ æ¨¡å‹è®­ç»ƒæµç¨‹

###### âœ… **1. è¾“å…¥æ•°æ®æ ¼å¼ä¸æ„é€ **
è®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨çš„æ˜¯ä¸€ä¸ª**å¤šè¾“å…¥ç»“æ„çš„æ•°æ®å…ƒç»„**ï¼š

```
H = (Ai, Bi, Di, yi)
```

å«ä¹‰å¦‚ä¸‹ï¼š

- `Ai`ï¼šRGBå›¾åƒï¼ˆ224Ã—224ï¼Œ3é€šé“ï¼‰ï¼ŒåŸå§‹/ä¼ªé€ äººè„¸å›¾åƒã€‚
- `Bi`ï¼šå‚è€ƒmaskï¼Œç”¨äºå®šä½ä¼ªé€ åŒºåŸŸï¼ŒåŒºåˆ†ï¼š
  - face swapï¼ˆæ•´å¼ è„¸ä¸ºä¼ªé€ åŒºåŸŸï¼‰
  - expression swapï¼ˆä»…é¢éƒ¨è¡¨æƒ…åŒºåŸŸä¸ºä¼ªé€ ï¼‰
- `Di`ï¼šå¯¹`Ai`è¿›è¡Œ**FFTé¢‘ç‡å˜æ¢åå¾—åˆ°çš„é¢‘åŸŸå›¾**ã€‚
- `yi`ï¼šæ ‡ç­¾ï¼ˆ0è¡¨ç¤ºçœŸå®ï¼Œ1è¡¨ç¤ºä¼ªé€ ï¼‰

---

###### ğŸ” 2. ç‰¹æ®Šçš„æ•°æ®é¢„å¤„ç†ä¸å¢å¼º

æ–‡ç« ç‰¹åˆ«å¼ºè°ƒäº†**é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆäººè„¸èº«ä»½ç‰¹å¾**çš„é—®é¢˜ï¼Œå› æ­¤ä½¿ç”¨ä»¥ä¸‹æ•°æ®å¢å¼ºæ‰‹æ®µï¼š

| æ•°æ®å¢å¼ºæ–¹æ³•          | ç›®çš„                                                         |
|-----------------------|--------------------------------------------------------------|
| æ°´å¹³ç¿»è½¬ (flip)       | å¢åŠ æ ·æœ¬å¤šæ ·æ€§                                               |
| æ—‹è½¬ (rotate)         | å¢åŠ è§†è§’å˜åŒ–ï¼Œæé«˜ç©ºé—´é²æ£’æ€§                                 |
| å¯¹æ¯”åº¦è°ƒæ•´ (contrast) | å¢å¼ºå¯¹ä¸åŒå›¾åƒè´¨é‡çš„é€‚åº”æ€§                                   |
| æ·»åŠ é«˜æ–¯å™ªå£°          | æ¨¡æ‹Ÿè§†é¢‘å‹ç¼©æˆ–å›¾åƒåŠ£åŒ–ï¼Œæé«˜å¯¹å‹ç¼©ä¼ªé€ çš„è¯†åˆ«èƒ½åŠ›            |
| å›¾åƒå‹ç¼©æ¨¡æ‹Ÿ (JPEG)   | æ¨¡æ‹ŸçœŸå®ç¤¾äº¤å¹³å°ä¸Šä¼ /å‹ç¼©åçš„ä¼ªé€ å†…å®¹ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›      |

---

##### âš™ï¸ 3. æ¨¡å‹æ¶æ„ä¸ä¼˜åŒ–ç»†èŠ‚

- **ä¸»å¹²ç½‘ç»œ**ï¼šResNet + Residual U-Net
- **æ£€æµ‹åˆ†æ”¯**ï¼šè¾“å…¥é¢‘åŸŸç‰¹å¾ï¼Œæå–æ·±åº¦ç‰¹å¾ååŠ å…¥æ³¨æ„åŠ›æ¨¡å—
- **å®šä½åˆ†æ”¯**ï¼šä½¿ç”¨ scABï¼ˆspatial-channel attention blockï¼‰æ¨¡å—æå–å®šä½ä¿¡æ¯
- **èåˆæ–¹å¼**ï¼šä½¿ç”¨ Bilinear Pooling å°†é¢‘ç‡ä¸ç©ºé—´ç‰¹å¾è¿›è¡Œèåˆ


| å‚æ•°               | å€¼                         |
|--------------------|----------------------------|
| Optimizer          | Adam                       |
| åˆå§‹å­¦ä¹ ç‡         | 1e-4                       |
| Î²1 / Î²2            | 0.9 / 0.999                |
| Batch size         | 16                         |
| è°ƒæ•´ç­–ç•¥           | 30ä¸ªepochä¸æå‡åˆ™LR Ã— 0.1 |
| è¾“å…¥å›¾åƒå°ºå¯¸       | 224 Ã— 224                  |

---

##### ğŸ“‰ 4. æŸå¤±å‡½æ•°è®¾è®¡

é‡‡ç”¨å¤šä»»åŠ¡æŸå¤±å‡½æ•°ç»„åˆï¼š

```math
L_comb = Ï_class * L_class + Ï_localize * L_localize
```

- `L_class`ï¼šäºŒåˆ†ç±»äº¤å‰ç†µï¼Œç”¨äºæ£€æµ‹çœŸå® vs ä¼ªé€ 
- `L_localize`ï¼šç”¨äºä¼ªé€ åŒºåŸŸå®šä½ï¼Œå°è¯•è¿‡ Dice/Focalï¼Œä½† **L1/L2ï¼ˆå›å½’æŸå¤±ï¼‰è¡¨ç°æœ€å¥½**

> æƒé‡å‚æ•° `Ï_class = Ï_localize = 1`ï¼Œè¯´æ˜ä¸¤ä¸ªä»»åŠ¡åŒç­‰é‡è¦ã€‚

##### âœ… å°ç»“ï¼šè®­ç»ƒå…³é”®ç‚¹

1. **ç©ºé—´ + é¢‘åŸŸè¾“å…¥**ï¼šæ˜¾è‘—æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å‹ç¼©ä¸¥é‡çš„è§†é¢‘ä¸­ã€‚
2. **ä¼ªé€ åŒºåŸŸmaskå¼•å¯¼çš„å®šä½è®­ç»ƒ**ï¼šæä¾›ç»†ç²’åº¦ç›‘ç£ã€‚
3. **å¤šç§å¢å¼ºæ‰‹æ®µ**ï¼šé˜²æ­¢æ¨¡å‹åªå­¦ä¼šèº«ä»½ç‰¹å¾ï¼Œæå‡å¯¹ä¸åŒæ•°æ®é›†çš„é€‚åº”èƒ½åŠ›ã€‚
4. **æ³¨æ„åŠ›æœºåˆ¶è¾…åŠ©å­¦ä¹ **ï¼šè®©æ¨¡å‹æ›´èšç„¦äºå…³é”®ä¼ªé€ åŒºåŸŸã€‚
![alt text](../../images/image-162.png)


## Q&A

1. Does stable/flux VAE decoder has its own features to distinguish from real images?




