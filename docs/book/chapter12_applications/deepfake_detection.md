# Deepfake Detection
## Review
### An Analysis of Recent Advances in Deepfake Image Detection in an Evolving Threat Landscape

- author: 2024
- url: <https://arxiv.org/pdf/2404.16212>
- Sifat Muhammad Abdullah

- fully synthetic images created using generative models are considered
- partial synthetic images are not considered in this paper

#### Introduction

1. Current SOTA methods:
    - ture statistics [13]
    - finding imperfections in the frequency spectrum [12]
    - local patches [17]

2. Threat
    - Emergence of lightweight methods that allow users to customize large generative models
        Previously, generate images from few of generative models by GAN or diffusion
        Today, more than 3000 variants of generative models in platform like Huggingface, Civitai
    - Use foundation model fine-tuning to fool deepfake detection models
3. Contirbution
    - critical analysis on the trianing and evaluation methods in current SOTA
    - performance on user-customized generative models
    - create adversarial samples with foundation model without adversary noise

#### Generative models

1. Stable Difussion
2. StyleClip

#### Defense Models

1. UnivCLIP (2023): first one that use foundation model to detect deepfake
2. DE-FAKE: augment the image’s embedding along with an embedding of the text prompt. The intuition is that real images usually have more information than their respective captions, whereas fake images generated from prompts only show content that is specific to that prompt. Achieve **90.9%** on DALL·E 2 images
3. DCT: frequency domain provides discriminatory features for deepfake detection. Logscaled version of the DCT features. DCT achieves 97.7% and 73% accuracy on images generated by GAN and Diffusion model,
4. Patch-Forensics: searching for artifacts in local patches of the image provides more generalizable patterns for detection
5. GramNet: texture statistics of fake images (e.g., face content) are significantly different from real images.
6. Resynthesis: generating testing images based on different auxiliary tasks (super-resolution, denoising and colorization)
7. CNN-F: CNN-based generators leave detectable fingerprints. Highlights that the detector needs to be trained only on images from a single CNN-based generator to generalize across different fake sources
8. MesoNet: Originally designed to detect deepfake videos. microscopic (中观层面) features of real images are more diverse than those of fake images compared with macroscopic or microscopic features. Inception module has better performance than usual convolutional modules.

| Defense           | SD Precision | SD Recall | SD F1  | StyleCLIP Precision | StyleCLIP Recall | StyleCLIP F1 |
|------------------|--------------|-----------|--------|----------------------|------------------|--------------|
| UnivCLIP         | 90.20        | 93.90     | 92.01  | 93.79                | 92.20            | 92.99        |
| DE-FAKE          | 93.82        | 94.20     | 94.01  | 74.41                | 78.80            | 76.54        |
| DCT              | 100          | 88.80     | 94.07  | 100                  | 99.60            | 99.80        |
| Patch-Forensics  | -            | -         | -      | 91.76                | 91.30            | 91.53        |
| Gram-Net         | 99.99        | 99.10     | 99.55  | 99.99                | 99.60            | 99.80        |
| Resynthesis      | 85.39        | 86.50     | 85.94  | 98.80                | 98.70            | 98.75        |
| CNN-F            | 99.41        | 83.80     | 90.94  | 99.90                | 97.10            | 98.48        |
| MesoNet          | 99.99        | 98.00     | 98.98  | 96.70                | 99.50            | 98.08        |

#### current limitations

1. lack control of the content and quality of training data: real and fake images should be consistent in content and quality
2. Lack adversary attack
3. Prior work only focused only on limited content types, e.g., faces, animals, bedrooms, and buildings

#### findings
FM: full fintune model
user-cutomized models: use full fituning or the Lora finetuning to train new models

1. All models have performance degradation on user-cutomized models
2. Soly relay on foundation model's feature is not enough to have generalization on deepfake detection
3. Frequency domain show the best generalization performance
4. CNN-based model has the worst generalization performance
5. Content-agnostic features can help boost generalization performance for deepfake detection.
6. Combining domain-specific features (i.e., features known to identify imperfections in fake images) with features from a foundation model
    improves generalization. Cimbine the DCT features with the foundation model feature achieves the best performance
7. adversary attack: attacker has a realistic photo, and manipulate the photo by the text prompt like 'a smilling face'.
    - How to do adversary attack:
        1. First, train three surrogate deepfake classifiers using dataset with fake images from current generator and real images from public datasets
        2. Second, adversary train the generator again the surroage deepfake classifiers for each deceptive image which can be detected by current deepfake detection model (the testing model, not the surrogate model) with looss:
            - classification loss
            - perception loss from vgg pretrained network in imagenet
        For each image, it takes 39 seconds to generate an adversarial image using dgx A100

    ![alt text](../../images/image-151.png)
    For each surrogate model, the adversary attack will leads to the performance drop ($\Delta R$)
8. Defense based on the frequency features is the weakest against adversary attack
9. Defense using foundation model is the strongest against adversary attack
10. Defense with stronger foundation model is more robust against adversary attack
11. Adversary training can improve the robustness of adversary attack

    ![alt text](../../images/image-152.png)

## Deepfake Image Detection
### FSBI
![alt text](../../images/image-153.png)

- year: 2024 Jun
- author: Ahmed Abul Hasnanaath
- instituiton: Information and Computer Science Department, King Fahd University of Petroleum and Minerals
- code: at <https://github.com/gufranSabri/FSBI>
- main contribution
    1. data synthetic methods: SBI methods generate more various fake images for training
    2. use discrete wavelet transform to extract features from images

#### SBI (self blended image)

Give an iamge, it first do augmentations including

- RGB and HSV jitters
- contrast and brightness jitters
- downsampling
- translation
For single image, it do two different random augmentations and produces $I_t$ and $I_s$.

Secondly, it detect the landmark on $I_s$ using some landmark detection model and produce the face mask with the convex hull of the detected landmarks. After applying the gaussian and dilation on the face mask, it will produce $M$.
Lastly, it combine the $I_s$ and $I_t$ with the mask according to the formula

$$I_{SBI} = I_s \cdot M + I_t \cdot (1 - M)$$


Here $M$ is the gray tensor in range [0, 1]. The edge of the mask could be smoothed by gaussian and dilation to produce continuous self blending image.

#### Frequency Feature Generator
It first split the image into R,G,B channel seperately
Secondly, it calculate the wavelet coefficients of each channel
Last, it resize the coefficients to the same size and stack them together

### DE-FAKE

- zeyang sha
- salesforce research
- 2023 Jan
- main contribution
    1. proposed three questions
        1. whether the fake image be distinguished
        2. can we attribute the fake images to their sources (which model generated the image)
        3. what kind of prompt is more likely to generate authentic images

#### Detection method
Train a binary classifier on the images generated by one text2image model and then evaludate on the images generated by other models. Two types of detector

1. Image only detector: accepts only image as input
2. Hybrid detector: accepts both image and its prompt as input

##### Image only detector

1. dataset
    - random sample 20k images from MSCOCO, treat as real images
    - use stable diffusion model to generate another 20k images, treat as fake images with the prompt
2. network
    - resnet18
    - binary output

##### Hybrid detector

1. dataset
    Same construction as image only detector
2. network
    - pretrained clip model to extract both image and text embedding and concat them together, followed by 2-layer MLP
3. captions
    - Use the captions of the image if it is provided by the dataset
    - Otherwise, use BLIP model to generate captions for queried images

Questions:

- Cause image only detector use resnet18, why not use more powerful model? or use same model as hybrid detector to compare their performance fairly

Results
![alt text](../../images/image-154.png)
Hybird detector can obtain much better performance than image only detector
#### Fake Image Attribute (Generator source identification)
The goal is to predict which generator is used to generate the image
To train such model, we first generate the iamges from different text2image models (3 models, SD, LD, and GLIDE)
Network construction is similar to the previous section but now their are four classes

- 0: real
- 1: SD
- 2: LD
- 3: GLIDE

Results

|                                   | MSCOCO | Flickr30k |
|-----------------------------------|--------|-----------|
| Image-Only                        | 0.864  | 0.863     |
| Hybrid (natural prompts)          | 0.936  | 0.933     |
| Hybrid (generated prompts)        | 0.903  | 0.892     |

#### Prompt analysis
Analysis which kind of prompt would has more likely to pass the deepfake detection
##### Semantic Analysis

- **Topic-based Grouping**:
  - Used MSCOCO's 80 predefined topics to group prompts
  - Measured proportion of fake images misclassified as real by topic
  - Results: "skis," "snowboard," and animal-related topics produced most realistic fakes

- **Embedding Cluster Analysis**:
  - Used BERT-based sentence transformer for prompt embeddings
  - Applied DBSCAN clustering for semantic grouping
  - Finding: "person"-related prompts generated most realistic fakes
  - "Skis" and "snowboard" topics effective because they often included people

- **Typical Prompts Analysis**:
  - Extracted top 5 most realistic and most fake-looking prompts
  - Finding: Detailed object descriptions (4/5 top realistic prompts) outperformed environment descriptions (4/5 top fake-looking prompts)

##### Structural Analysis

- **Prompt Length**:
  - Tested 5,000 random MSCOCO prompts with Stable Diffusion
  - Finding: 25-75 word prompts generated most realistic images
  - Very short or long prompts performed worse

- **Noun Proportion**:
  - Used NLTK to analyze noun ratio in prompts
  - Finding: No significant correlation between noun proportion and image realism

### Mastering Deepfake Detection: A Cutting-edge Approach to Distinguish GAN and Diffusion-model Images
![alt text](../../images/image-143.png)

- <https://dl.acm.org/doi/pdf/10.1145/3652027>
- University of Catania
- LUCA GUARNERA
- 2024

![alt text](../../images/image-142.png)

- AttGAN: <https://github.com/LynnHo/AttGAN-Tensorflow>
- CycleGAN: <https://github.com/junyanz/pytorch-CycleGANand-pix2pix>
- GDWCT: <https://github.com/WonwoongCho/GDWCT>
- IMLE: <https://github.com/zth667/Diverse-ImageSynthesis-from-Semantic-Layout>
- ProGAN: <https://github.com/tkarras/progressive_growing_of_gans>
- StarGAN: <https://github.com/wkentaro/StarGAN>
- StarGAN-v2: <https://github.com/clovaai/stargan-v2>
- StyleGAN: <https://github.com/NVlabs/stylegan>
- StyleGAN2: <https://github.com/NVlabs/stylegan2>
- DALL-E 2: <https://github.com/lucidrains/DALLE2pytorch>
- GLIDE: <https://github.com/openai/glide-text2im>
- Latent Diffusion: <https://github.com/CompVis/latent-diffusion>
- Stable Diffusion: <https://github.com/CompVis/stable-diffusion>
![alt text](../../images/image-145.png)
![alt text](../../images/image-144.png)
- in the spetral space, “real” class has the  isotropic behavior

### Leveraging Frequency Analysis for Deep Fake Image Recognition
![alt text](../../images/image-146.png)

- <https://proceedings.mlr.press/v119/frank20a/frank20a.pdf>>
- Joel Frank
- Horst G¨ortz Institute for ITSecurity, Bochum, Germany
- 2020

- Upsampling will leads to spectral differences
    ![alt text](../../images/image-147.png)
- both upsampling and downsampling operations have recently been linked to compromising shift invariance in neural networks, i.e., they cause classifier predictions to vary dramatically due to a simple one-pixel shift in the input image (Azulay & Weiss, 2018). Recently, Zhang (2019)

## Partial Deepfake Image Detection
### SIDA: Social Media Image Deepfake Detection, Localization and Explanation with Large Multimodal Model

- Zhenglin Huang1
- <https://arxiv.org/pdf/2412.04292>
- University of Liverpool, UK

#### contribution

1. Social media Image Detection data Set (SID-Set)
![alt text](../../images/image-148.png)
2. SIDA
![alt text](../../images/image-149.png)

#### Training Objectives Summary

The SIDA model training involves three main loss components:

1. **Detection loss** (\(\mathcal{L}_{det}\)) – using CrossEntropy loss for detecting elements.
2. **Segmentation mask loss** (\(\mathcal{L}_{mask}\)) – a weighted combination of Binary Cross Entropy (BCE) loss and DICE loss:

   $$
   \mathcal{L}_{mask} = \lambda_{bce} \mathcal{L}_{BCE}(\hat{M}, M) + \lambda_{dice} \mathcal{L}_{DICE}(\hat{M}, M)
   $$


3. **Text generation loss** (\(\mathcal{L}_{txt}\)) – used in the fine-tuning phase with ground truth descriptions from 3,000 images:

   $$
   \mathcal{L}_{txt} = \mathcal{L}_{CE}(\hat{y}_{des}, y_{des})
   $$


The overall training loss during initial training:

$$
\mathcal{L} = \lambda_{det} \mathcal{L}_{det} + \lambda_{mask} \mathcal{L}_{mask}
$$


In the fine-tuning stage, the full loss function includes text generation:

$$
\mathcal{L}_{total} = \lambda_{det} \mathcal{L}_{det} + \lambda_{mask} \mathcal{L}_{mask} + \lambda_{txt} \mathcal{L}_{txt}
$$


Here, \(\lambda_{det}\), \(\lambda_{mask}\), and \(\lambda_{txt}\) are weighting factors used to balance the contributions of each loss term.

#### Results
![alt text](../../images/image-150.png)

### Detect Any Deepfakes: Segment Anything Meets Face Forgery Detection and Localization
![alt text](../../images/image-156.png)

- institution: xiamen university
- Yingxin Lai
- 2023 June
- proposed DADF (Detect Any Deepfakes)
- code: <https://github.com/laiyingxin2/DADF>

#### contribution
- finetune on segment anything for face forgery detection and localization
- reconstruction guided attention module is proposed
    1. Add gaussian noise to the original images
    2. Use differences of features from original images and the noised image to hiligh the differences as the attention.
        If it is high, means the part different from the forgery
    3. In this way, suppose the final feature only care about the real feature
    4. use L1 norm to calculate the difference between features from real images and the gaussian images (simulation of forgey)
    ![alt text](../../images/image-157.png)
    Results
    ![alt text](../../images/image-158.png)

### diffusion inpainting localization
- url: [text](https://openaccess.thecvf.com/content/WACV2024/papers/Tantaru_Weakly-Supervised_Deepfake_Localization_in_Diffusion-Generated_Images_WACV_2024_paper.pdf)
- code: [text](https://github.com/bit-ml/dolos)
- 2024
- institution: Bitdefender
- author: Dragos-Constaintin Tantaru
- contribution

以下是这段文字的结构化总结：


| 项目                         | 内容                                                                                     |
|----------------------------|------------------------------------------------------------------------------------------|
| **研究背景**                 | Denoising Diffusion Models（扩散模型）生成能力强，引发图像真实性担忧。现有检测方法多基于GAN，仅输出“真/假”标签。 |
| **核心问题**                 | 现有检测模型缺乏对**伪造区域的定位能力**，仅做二分类，信息不足。                                     |
| **研究目标**                 | 将任务转化为**弱监督定位问题**，输出图像中被篡改的区域图（localization map）。                       |
| **方法分类**                 | 三类方法：基于解释（explanations）、局部得分（local scores）、注意力机制（attention）。            |
| **统一比较方法**             | 所有方法使用相同的**Xception网络**架构，确保公平对比。                                          |
| **实验设计**                 | 构建多个控制变量的数据集，**单一改变生成器、监督方式或数据源**，以分析其对性能的影响。               |
| **主要发现**                 | 局部得分法性能最佳，对弱监督敏感性低，但对生成器/数据集变化更敏感。                                |
| **研究贡献**                 | 提供对弱监督伪造区域检测设计空间的系统分析，验证了该方法的可行性。                                 |

Different Sturcture of the detection and localization of deepfakes
![alt text](../../images/image-159.png)

#### Question
How much does the performance of the model improve after finetuning compared with train from scratch?


Works that tackle localization rely on local noise fingerprint
patterns [21, 33, 40, 64], attention mechanisms [12, 13, 42]
or self-consistency checks [2, 27]. Very recent, concurrent
works proposed a forensic framework for general manipula-
tion localization [21] and a hierarchical fine-grained formu-
lation for image forgery detection [22].

## Q&A

1. Does stable/flux VAE decoder has its own features to distinguish from real images?




