{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Generative AI Documentation","text":"<p>Welcome to the documentation for \"Generative AI: From Start to Surrender\". This project aims to provide a comprehensive guide on generative AI technologies, including VAE, GANs, and more.</p>"},{"location":"#project-scope-globe_with_meridians","title":"Project Scope :globe_with_meridians:","text":"<p>Our project covers a wide range of topics in generative AI, from foundational concepts to advanced architectures like StyleGAN and VAE. We aim to make this resource accessible to both beginners and experts in the field.</p>"},{"location":"#objectives-dart","title":"Objectives :dart:","text":"<ul> <li>To educate and inform about the latest advancements in generative AI.</li> <li>To provide practical examples and code snippets for better understanding.</li> <li>To foster a community of AI enthusiasts and professionals who can contribute and expand the knowledge base.</li> </ul>"},{"location":"#how-to-contribute-handshake","title":"How to Contribute :handshake:","text":"<p>We welcome contributions from everyone. Whether it's fixing a typo, adding new content, or improving existing sections, your help is invaluable. Please visit our GitHub repository to get started:</p> <p></p> <p>Feel free to explore the chapters and sections to learn more about each topic and join us in advancing the field of generative AI.</p>"},{"location":"book/chapter1_Introduction/1.1terminology/","title":"\u672f\u8bed\u8868\uff08Terminology\uff09","text":"<p>\u6240\u6709\u672f\u8bed\u4f1a\u88ab\u6839\u636e\u5df2\u6709\u5185\u5bb9\u81ea\u52a8\u751f\u6210 </p>"},{"location":"book/chapter1_Introduction/1.1terminology/#_1","title":"\u56fe\u5f62\u5b66\u672f\u8bed","text":""},{"location":"book/chapter1_Introduction/1.1terminology/#artifacts","title":"Artifacts","text":"<p>Artifacts\u662f\u56fe\u5f62\u5b66\u4e0a\u7684\u4e00\u4e2a\u6982\u5ff5\uff0c\u6216\u8005\u8bf4\u662f\u4e00\u4e2a\u6897\uff0c\u8868\u793a\u4e86\u4e00\u5207\u5728\u56fe\u5f62\u5b66\u4e0a\u7684\u9519\u8bef\uff0c\u5f02\u5e38\uff0c\u4e0d\u5e0c\u671b\u770b\u5230\u7684\u7ed3\u679c\uff0c\u770b\u4e0a\u53bb\u4e0d\u5bf9\u7684\u6548\u679c\u4ee5\u53ca\u5404\u79cd\u7455\u75b5\u3002</p>"},{"location":"book/chapter1_Introduction/1.1terminology/#aliasing-artifacts","title":"Aliasing Artifacts","text":"<p>\u91c7\u6837\u800c\u5bfc\u81f4\u7684\u8d70\u6837\uff08Aliasing Artifacts\uff09\u4f1a\u5bfc\u81f4\u5f88\u591a\u73b0\u8c61\uff0c\u6bd4\u5982\uff1a</p> <ul> <li>\u952f\u9f7f\uff08Jaggies\uff0c\u5bf9\u7a7a\u95f4\u8fdb\u884c\u91c7\u6837\uff09</li> <li>\u6469\u5c14\u7eb9\uff08Moire\uff0c\u5bf9\u56fe\u50cf\u8fdb\u884c\u91c7\u6837\uff09</li> <li>\u8f6c\u8f6e\u6548\u5e94\uff08\u4e00\u5757\u8f6c\u8f6e\u4e0a\u4ea7\u751f\u89c6\u89c9\u4e0a\u7684\u4e0d\u540c\u8f6c\u901f\uff0c\u5bf9\u65f6\u95f4\u8fdb\u884c\u91c7\u6837\uff09</li> </ul>"},{"location":"book/chapter1_Introduction/1.1terminology/#_2","title":"\u4fe1\u53f7\u5904\u7406\u672f\u8bed","text":""},{"location":"book/chapter1_Introduction/1.1terminology/#_3","title":"\u5e26\u9650\u4fe1\u53f7","text":"<p>\u9891\u8c31\uff08\u5085\u91cc\u53f6\u53d8\u6362\uff09\u5728\u67d0\u4e2a\u9891\u7387\u8303\u56f4\u5185\u6709\u9650\u7684\u4fe1\u53f7\uff0c\u5373\u5b58\u5728\u6700\u9ad8\u9891\u7387B\uff0c\u4f7f\u5f97\u6240\u6709\u9ad8\u4e8eB\u7684\u9891\u7387\u5206\u91cf\u4e3a\u96f6\u3002</p>"},{"location":"book/chapter1_Introduction/1.1terminology/#_4","title":"\u5948\u594e\u65af\u7279\u9891\u7387","text":"<p>\u91c7\u6837\u9891\u7387\u5fc5\u987b\u5927\u4e8e\u4fe1\u53f7\u6700\u9ad8\u9891\u7387\u7684\u4e24\u500d\uff0c\u8fd9\u4e2a\u6700\u5c0f\u91c7\u6837\u9891\u7387\u79f0\u4e3a\u5948\u594e\u65af\u7279\u9891\u7387\u3002</p>"},{"location":"book/chapter1_Introduction/1.1terminology/#_5","title":"\u7406\u60f3\u6ee4\u6ce2\u5668","text":"<p>\u9891\u7387\u54cd\u5e94\u5728\u901a\u5e26\u548c\u963b\u5e26\u4e4b\u95f4\u6709\u7a81\u53d8\uff0c\u65e0\u8fc7\u6e21\u5e26\u7684\u6ee4\u6ce2\u5668\u3002\u4e0e\u4e4b\u76f8\u5bf9\u7684\u662f\u5b9e\u9645\u6ee4\u6ce2\u5668\uff0c\u5b83\u5b58\u5728\u8fc7\u6e21\u5e26\u548c\u901a\u5e26\u6ce2\u7eb9\u3002</p>"},{"location":"book/chapter1_Introduction/1.1terminology/#_6","title":"\u793a\u4f8b\u56fe\u7247","text":""},{"location":"book/chapter1_Introduction/1.1terminology/#_7","title":"\u51e0\u4f55\u8d70\u6837","text":""},{"location":"book/chapter1_Introduction/1.1terminology/#_8","title":"\u6e32\u67d3\u8d70\u6837","text":""},{"location":"book/chapter1_Introduction/1.1terminology/#_9","title":"\u7f29\u5199","text":""},{"location":"book/chapter1_Introduction/1.1terminology/#ebm","title":"EBM","text":"<p>Energey Based Model, \u80fd\u91cf\u6a21\u578b</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/","title":"1. \u5085\u91cc\u53f6\u53d8\u5316","text":""},{"location":"book/chapter1_Introduction/1.2fourier_transform/#11","title":"1.1 \u5085\u91cc\u53f6\u53d8\u6362\u548c\u9006\u53d8\u6362\u516c\u5f0f","text":""},{"location":"book/chapter1_Introduction/1.2fourier_transform/#111-continuous-fourier-transform","title":"1.1.1 \u8fde\u7eed\u5085\u91cc\u53f6\u53d8\u6362 (Continuous Fourier Transform)","text":"<ul> <li>\u516c\u5f0f\uff1a   \\(X(f) = \\int_{-\\infty}^{\\infty} x(t) e^{-j 2\\pi f t} \\, dt\\)</li> </ul> <p>\u5176\u4e2d\uff1a   - \\(x(t)\\)\uff1a\u65f6\u57df\u4fe1\u53f7\u3002   - \\(X(f)\\)\uff1a\u9891\u57df\u4fe1\u53f7\uff08\u9891\u8c31\uff09\u3002   - \\(f\\)\uff1a\u9891\u7387\uff0c\u5355\u4f4d\u4e3a\u8d6b\u5179 (Hz)\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#112-inverse-continuous-fourier-transform","title":"1.1.2 \u8fde\u7eed\u5085\u91cc\u53f6\u9006\u53d8\u6362 (Inverse Continuous Fourier Transform)","text":"<ul> <li>\u516c\u5f0f\uff1a   \\(x(t) = \\int_{-\\infty}^{\\infty} X(f) e^{j 2\\pi f t} \\, df\\)</li> </ul> <p>\u5176\u4e2d\uff1a   - \\(X(f)\\)\uff1a\u9891\u57df\u4fe1\u53f7\u3002   - \\(x(t)\\)\uff1a\u91cd\u5efa\u56de\u6765\u7684\u65f6\u57df\u4fe1\u53f7\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#113-discrete-fourier-transform-dft","title":"1.1.3 \u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362 (Discrete Fourier Transform, DFT)","text":"<p>\u5bf9\u4e8e\u79bb\u6563\u4fe1\u53f7 \\(x[n]\\)\uff1a - \u516c\u5f0f\uff1a   \\(X[k] = \\sum_{n=0}^{N-1} x[n] e^{-j \\frac{2\\pi}{N} kn}, \\quad k = 0, 1, 2, \\dots, N-1\\)</p> <p>\u5176\u4e2d\uff1a   - \\(x[n]\\)\uff1a\u65f6\u57df\u79bb\u6563\u4fe1\u53f7\u3002   - \\(X[k]\\)\uff1a\u9891\u57df\u79bb\u6563\u4fe1\u53f7\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#114-inverse-discrete-fourier-transform-idft","title":"1.1.4 \u79bb\u6563\u5085\u91cc\u53f6\u9006\u53d8\u6362 (Inverse Discrete Fourier Transform, IDFT)","text":"<ul> <li>\u516c\u5f0f\uff1a   \\(x[n] = \\frac{1}{N} \\sum_{k=0}^{N-1} X[k] e^{j \\frac{2\\pi}{N} kn}, \\quad n = 0, 1, 2, \\dots, N-1\\)</li> </ul> <p>\u5176\u4e2d\uff1a   - \\(X[k]\\)\uff1a\u9891\u57df\u79bb\u6563\u4fe1\u53f7\u3002   - \\(x[n]\\)\uff1a\u91cd\u5efa\u56de\u6765\u7684\u65f6\u57df\u4fe1\u53f7\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#12","title":"1.2 \u516c\u5f0f\u4e2d\u7684\u5173\u952e\u70b9","text":""},{"location":"book/chapter1_Introduction/1.2fourier_transform/#121","title":"1.2.1 \u6b63\u8d1f\u53f7\u533a\u522b\uff1a","text":"<ul> <li>\u5085\u91cc\u53f6\u53d8\u6362\u4e2d\uff0c\u6307\u6570\u90e8\u5206\u4e3a \\(e^{-j 2\\pi f t}\\)\uff08\u6b63\u5f26\u548c\u4f59\u5f26\u5206\u91cf\uff09\u3002</li> <li>\u9006\u53d8\u6362\u4e2d\uff0c\u6307\u6570\u90e8\u5206\u4e3a \\(e^{j 2\\pi f t}\\)\uff08\u76f8\u4f4d\u76f8\u53cd\uff09\u3002</li> </ul>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#122-vs","title":"1.2.2 \u8fde\u7eed vs \u79bb\u6563\uff1a","text":"<ul> <li>\u8fde\u7eed\u53d8\u6362\u6d89\u53ca\u79ef\u5206\uff08\u65f6\u57df\u5230\u9891\u57df\uff0c\u6216\u9891\u57df\u5230\u65f6\u57df\uff09\u3002</li> <li>\u79bb\u6563\u53d8\u6362\u6d89\u53ca\u6c42\u548c\uff08\u7528\u4e8e\u79bb\u6563\u4fe1\u53f7\u5904\u7406\uff09\u3002</li> </ul>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#123","title":"1.2.3 \u5f52\u4e00\u5316\u7cfb\u6570\uff1a","text":"<ul> <li>\u5bf9\u4e8e\u8fde\u7eed\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u4e0d\u9700\u8981\u663e\u5f0f\u5f52\u4e00\u5316\u7cfb\u6570\u3002</li> <li>\u5bf9\u4e8e\u79bb\u6563\u5085\u91cc\u53f6\u9006\u53d8\u6362\uff0c\u9700\u8981\u9664\u4ee5\u4fe1\u53f7\u957f\u5ea6 \\(N\\)\u3002</li> </ul>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#13","title":"1.3 \u793a\u4f8b","text":""},{"location":"book/chapter1_Introduction/1.2fourier_transform/#131","title":"1.3.1 \u8fde\u7eed\u4fe1\u53f7\u793a\u4f8b","text":"<p>\u65f6\u57df\u4fe1\u53f7\uff1a\\(x(t) = \\cos(2\\pi f_0 t)\\)</p> <ul> <li> <p>\u5085\u91cc\u53f6\u53d8\u6362\uff1a   \\(X(f) = \\frac{1}{2}[\\delta(f - f_0) + \\delta(f + f_0)]\\)</p> </li> <li> <p>\u9006\u53d8\u6362\uff1a   \\(x(t) = \\int_{-\\infty}^\\infty \\left[\\frac{1}{2}\\delta(f - f_0) + \\frac{1}{2}\\delta(f + f_0)\\right] e^{j 2\\pi f t} \\, df\\)   \u5f97\u5230 \\(x(t) = \\cos(2\\pi f_0 t)\\)\u3002</p> </li> </ul>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#132","title":"1.3.2 \u79bb\u6563\u4fe1\u53f7\u793a\u4f8b","text":"<p>\u65f6\u57df\u4fe1\u53f7\uff1a\\(x[n] = [1, 2, 3, 4]\\)</p> <ul> <li> <p>\u5085\u91cc\u53f6\u53d8\u6362\uff1a   \u4f7f\u7528\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\u516c\u5f0f\u8ba1\u7b97 \\(X[k]\\)\u3002</p> </li> <li> <p>\u9006\u53d8\u6362\uff1a   \u4f7f\u7528\u9006\u53d8\u6362\u516c\u5f0f\u8ba1\u7b97 \\(x[n]\\)\uff0c\u91cd\u5efa\u4fe1\u53f7\u3002</p> </li> </ul>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#14","title":"1.4 \u79bb\u6563\u4fe1\u53f7\u505a\u5085\u91cc\u53f6\u53d8\u6362\u7684\u4f8b\u5b50","text":""},{"location":"book/chapter1_Introduction/1.2fourier_transform/#141","title":"1.4.1 \u79bb\u6563\u4fe1\u53f7","text":"<p>\u5047\u8bbe\u4e00\u4e2a\u957f\u5ea6\u4e3a \\(N=4\\) \u7684\u79bb\u6563\u4fe1\u53f7\uff1a</p> \\[ x[n] = [1, 2, 3, 4], \\quad n = 0, 1, 2, 3 \\] <p>\u6211\u4eec\u7528\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362 (DFT) \u7684\u516c\u5f0f\u8ba1\u7b97\u5176\u9891\u57df\u4fe1\u53f7 \\(X[k]\\)\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#142-dft","title":"1.4.2 DFT \u516c\u5f0f","text":"\\[ X[k] = \\sum_{n=0}^{N-1} x[n] e^{-j \\frac{2\\pi}{N} kn}, \\quad k = 0, 1, \\dots, N-1 \\]"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#143","title":"1.4.3 \u8ba1\u7b97\u6b65\u9aa4","text":"<ol> <li> <p>\u4fe1\u53f7\u957f\u5ea6 \\(N=4\\)\uff0c\u9891\u7387\u7d22\u5f15 \\(k = 0, 1, 2, 3\\)\u3002</p> </li> <li> <p>\u6307\u6570\u9879\uff1a\\(e^{-j \\frac{2\\pi}{N} kn} = e^{-j \\frac{\\pi}{2} kn}\\)\u3002</p> </li> <li> <p>\u5206\u522b\u8ba1\u7b97 \\(k = 0, 1, 2, 3\\) \u65f6\u7684 \\(X[k]\\)\uff1a</p> </li> </ol>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#144-k-0","title":"1.4.4 k = 0","text":"\\[ X[0] = \\sum_{n=0}^{3} x[n] e^{-j \\frac{2\\pi}{4} \\cdot 0 \\cdot n} = x[0] + x[1] + x[2] + x[3] = 1 + 2 + 3 + 4 = 10 \\]"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#145-k-1","title":"1.4.5 k = 1","text":"\\[ X[1] = \\sum_{n=0}^{3} x[n] e^{-j \\frac{\\pi}{2} n} = x[0] + x[1] e^{-j \\frac{\\pi}{2}} + x[2] e^{-j \\pi} + x[3] e^{-j \\frac{3\\pi}{2}} \\] <p>\u8ba1\u7b97\u6bcf\u4e00\u9879:</p> <ul> <li>\\(x[0] = 1\\)</li> <li>\\(x[1] e^{-j \\frac{\\pi}{2}} = 2 \\cdot (-j) = -2j\\)</li> <li>\\(x[2] e^{-j \\pi} = 3 \\cdot (-1) = -3\\)</li> <li>\\(x[3] e^{-j \\frac{3\\pi}{2}} = 4 \\cdot j = 4j\\)</li> </ul> <p>\u76f8\u52a0\uff1a</p> \\[ X[1] = 1 - 2\\cdot j - 3 + 4\\cdot j = -2 + 2\\cdot j \\]"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#146-k-2","title":"1.4.6 k = 2","text":"\\[ X[2] = \\sum_{n=0}^{3} x[n] e^{-j \\pi n} = x[0] + x[1] e^{-j \\pi} + x[2] e^{-j 2\\pi} + x[3] e^{-j 3\\pi} \\] <p>\u8ba1\u7b97\u6bcf\u4e00\u9879\uff1a</p> <ul> <li>\\(x[0] = 1\\)</li> <li>\\(x[1] e^{-j \\pi} = 2 \\cdot (-1) = -2\\)</li> <li>\\(x[2] e^{-j 2\\pi} = 3 \\cdot 1 = 3\\)</li> <li>\\(x[3] e^{-j 3\\pi} = 4 \\cdot (-1) = -4\\)</li> </ul> <p>\u76f8\u52a0\uff1a</p> \\[ X[2] = 1 - 2 + 3 - 4 = -2 \\]"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#147-k-3","title":"1.4.7 k = 3","text":"\\[ X[3] = \\sum_{n=0}^{3} x[n] e^{-j \\frac{3\\pi}{2} n} = x[0] + x[1] e^{-j \\frac{3\\pi}{2}} + x[2] e^{-j 3\\pi} + x[3] e^{-j \\frac{9\\pi}{2}} \\] <p>\u8ba1\u7b97\u6bcf\u4e00\u9879\uff1a</p> <ul> <li>\\(x[0] = 1\\)</li> <li>\\(x[1] e^{-j \\frac{3\\pi}{2}} = 2 \\cdot j = 2j\\)</li> <li>\\(x[2] e^{-j 3\\pi} = 3 \\cdot (-1) = -3\\)</li> <li>\\(x[3] e^{-j \\frac{9\\pi}{2}} = 4 \\cdot (-j) = -4j\\)</li> </ul> <p>\u76f8\u52a0:</p> \\[ X[3] = 1 + 2j - 3 - 4j = -2 - 2j \\]"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#15","title":"1.5 \u7ed3\u679c\u6c47\u603b","text":"<p>\u9891\u57df\u4fe1\u53f7 \\(X[k]\\) \u4e3a\uff1a</p> \\[ X[k] = [10, -2 + 2j, -2, -2 - 2j] \\]"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#16","title":"1.6 \u89e3\u91ca","text":"<ol> <li>\\(X[0] = 10\\)\uff1a\u76f4\u6d41\u5206\u91cf\uff0c\u8868\u793a\u4fe1\u53f7\u7684\u5e73\u5747\u503c\u3002</li> <li>\u5176\u4f59 \\(X[k]\\)\uff1a\u8868\u793a\u4fe1\u53f7\u7684\u5176\u4ed6\u9891\u7387\u5206\u91cf\uff0c\u5305\u62ec\u632f\u5e45\u548c\u76f8\u4f4d\u3002</li> </ol>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#17","title":"1.7 \u53ef\u89c6\u5316\u9891\u8c31","text":"<ol> <li>\u5e45\u503c \\(|X[k]|\\)\uff1a</li> </ol> <p>$$    |X[0]| = 10, \\quad |X[1]| = \\sqrt{(-2)^2 + 2^2} = \\sqrt{8}, \\quad |X[2]| = 2, \\quad |X[3]| = \\sqrt{8}    $$</p> <ol> <li>\u76f8\u4f4d \\(\\text{arg}(X[k])\\)\uff1a</li> </ol> <p>$$    \\text{arg}(X[1]) = \\arctan\\left(\\frac{2}{-2}\\right) = \\frac{3\\pi}{4}, \\quad \\text{arg}(X[3]) = \\arctan\\left(\\frac{-2}{-2}\\right) = -\\frac{3\\pi}{4}    $$</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#18","title":"1.8 \u5085\u91cc\u53f6\u53d8\u6362\u540e\u7684\u7cfb\u6570\u4ee3\u8868\u7684\u6027\u8d28","text":""},{"location":"book/chapter1_Introduction/1.2fourier_transform/#181-magnitude","title":"1.8.1 \u5e45\u503c\uff08Magnitude\uff09","text":"<p>\u5e45\u503c \\(|X(f)|\\) \u6216 \\(|X[k]|\\) \u8868\u793a\u4fe1\u53f7\u5728\u67d0\u4e2a\u9891\u7387\u4e0a\u7684\u5f3a\u5ea6\u6216\u80fd\u91cf\u5206\u5e03\u3002</p> <ul> <li>\u8f83\u5927\u7684\u5e45\u503c\u610f\u5473\u7740\u4fe1\u53f7\u5728\u8be5\u9891\u7387\u4e0a\u5177\u6709\u8f83\u5f3a\u7684\u6210\u5206\u3002</li> <li>\u8f83\u5c0f\u7684\u5e45\u503c\u610f\u5473\u7740\u8be5\u9891\u7387\u4e0a\u7684\u6210\u5206\u8f83\u5f31\uff0c\u751a\u81f3\u53ef\u4ee5\u5ffd\u7565\u3002</li> </ul> <p>\u793a\u4f8b\uff1a - \u5982\u679c\u4fe1\u53f7\u4e3b\u8981\u662f\u4e00\u4e2a\u9891\u7387\u4e3a \\(f_0\\) \u7684\u6b63\u5f26\u6ce2\uff0c\u5176\u9891\u8c31\u5728 \\(f = f_0\\) \u548c \\(f = -f_0\\) \u5904\u6709\u5cf0\u503c\u3002 - \u591a\u4e2a\u9891\u7387\u6210\u5206\u7684\u4fe1\u53f7\u5728\u5bf9\u5e94\u9891\u7387\u4f4d\u7f6e\u4f1a\u51fa\u73b0\u591a\u4e2a\u5cf0\u503c\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#182-phase","title":"1.8.2 \u76f8\u4f4d\uff08Phase\uff09","text":"<p>\u76f8\u4f4d \\(\\text{arg}(X(f))\\) \u6216 \\(\\text{arg}(X[k])\\) \u8868\u793a\u4fe1\u53f7\u5728\u5bf9\u5e94\u9891\u7387\u4e0a\u7684\u76f8\u4f4d\u504f\u79fb\u3002</p> <ul> <li>\u76f8\u4f4d\u51b3\u5b9a\u4e86\u8be5\u9891\u7387\u6210\u5206\u7684\u6b63\u5f26\u6ce2\uff08\u6216\u4f59\u5f26\u6ce2\uff09\u5728\u65f6\u95f4\u4e0a\u7684\u504f\u79fb\u3002</li> <li>\u6539\u53d8\u76f8\u4f4d\u4f1a\u6539\u53d8\u4fe1\u53f7\u7684\u65f6\u57df\u8868\u73b0\uff0c\u4f46\u4e0d\u4f1a\u5f71\u54cd\u5176\u80fd\u91cf\u3002</li> </ul> <p>\u793a\u4f8b\uff1a - \u5982\u679c\u4fe1\u53f7\u7684\u76f8\u4f4d\u4e3a\u96f6\uff0c\u9891\u7387\u6210\u5206\u4e0e\u65f6\u95f4\u8f74\u5bf9\u9f50\u3002 - \u975e\u96f6\u76f8\u4f4d\u8868\u793a\u8be5\u9891\u7387\u6210\u5206\u7684\u65f6\u95f4\u504f\u79fb\u91cf\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#183-dc-component","title":"1.8.3 \u76f4\u6d41\u5206\u91cf\uff08DC Component\uff09","text":"<p>\u5f53\u9891\u7387 \\(f = 0\\) \u6216 \\(k = 0\\) \u65f6\u7684\u7cfb\u6570 \\(X(0)\\) \u6216 \\(X[0]\\) \u8868\u793a\u4fe1\u53f7\u7684\u5e73\u5747\u503c\uff0c\u79f0\u4e3a\u76f4\u6d41\u5206\u91cf\u3002</p> <ul> <li>\u5982\u679c\u4fe1\u53f7\u7684\u5e73\u5747\u503c\u4e3a\u96f6\uff08\u4f8b\u5982\u7eaf\u4ea4\u6d41\u4fe1\u53f7\uff09\uff0c\u76f4\u6d41\u5206\u91cf\u4e3a\u96f6\u3002</li> <li>\u5982\u679c\u4fe1\u53f7\u6709\u975e\u96f6\u5e73\u5747\u503c\uff0c\u5219 \\(X[0]\\) \u5bf9\u5e94\u8be5\u503c\u3002</li> </ul>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#184","title":"1.8.4 \u9891\u7387\u5206\u91cf","text":"<p>\u6bcf\u4e2a\u5085\u91cc\u53f6\u7cfb\u6570 \\(X(f)\\) \u6216 \\(X[k]\\) \u5bf9\u5e94\u4e8e\u4fe1\u53f7\u5728\u67d0\u4e2a\u7279\u5b9a\u9891\u7387\u4e0a\u7684\u8d21\u732e\u3002</p> <ul> <li>\u8fde\u7eed\u5085\u91cc\u53f6\u53d8\u6362\u4e2d\u7684 \\(X(f)\\)\uff1a\u5bf9\u5e94\u8fde\u7eed\u9891\u7387 \\(f\\)\u3002</li> <li>\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\u4e2d\u7684 \\(X[k]\\)\uff1a\u5bf9\u5e94\u79bb\u6563\u9891\u7387 \\(k \\cdot f_s / N\\)\uff0c\u5176\u4e2d \\(f_s\\) \u662f\u91c7\u6837\u9891\u7387\uff0c\\(N\\) \u662f\u4fe1\u53f7\u957f\u5ea6\u3002</li> </ul>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#185","title":"1.8.5 \u80fd\u91cf\u548c\u529f\u7387","text":"<p>\u901a\u8fc7\u5e15\u585e\u74e6\u5c14\u5b9a\u7406\uff0c\u5085\u91cc\u53f6\u53d8\u6362\u7684\u7cfb\u6570\u53ef\u4ee5\u7528\u6765\u8ba1\u7b97\u4fe1\u53f7\u7684\u603b\u80fd\u91cf\u6216\u529f\u7387\uff1a</p> <ul> <li>\u8fde\u7eed\u4fe1\u53f7\u7684\u80fd\u91cf\uff1a</li> </ul> <p>$$   E = \\int_{-\\infty}^\\infty |x(t)|^2 dt = \\int_{-\\infty}^\\infty |X(f)|^2 df   $$</p> <ul> <li>\u79bb\u6563\u4fe1\u53f7\u7684\u80fd\u91cf\uff1a</li> </ul> <p>$$   E = \\sum_{n=0}^{N-1} |x[n]|^2 = \\frac{1}{N} \\sum_{k=0}^{N-1} |X[k]|^2   $$</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#186","title":"1.8.6 \u5bf9\u79f0\u6027","text":"<p>\u5982\u679c\u4fe1\u53f7\u662f\u5b9e\u503c\u4fe1\u53f7\uff08\u4f8b\u5982\u97f3\u9891\u4fe1\u53f7\uff09\uff1a - \u9891\u8c31\u5173\u4e8e\u96f6\u9891\u5bf9\u79f0\uff1a</p> <p>$$   X(-f) = \\overline{X(f)} \\quad \\text{\uff08\u5171\u8f6d\u5bf9\u79f0\uff09}   $$</p> <p>\u5bf9\u4e8e\u79bb\u6563\u4fe1\u53f7\uff1a</p> <p>$$   X[N-k] = \\overline{X[k]}   $$</p> <p>\u5982\u679c\u4fe1\u53f7\u662f\u5076\u51fd\u6570\u6216\u5947\u51fd\u6570\uff0c\u9891\u8c31\u4e5f\u4f1a\u8868\u73b0\u51fa\u5bf9\u79f0\u6027\u6216\u53cd\u5bf9\u79f0\u6027\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#187","title":"1.8.7 \u9891\u57df\u5206\u8fa8\u7387","text":"<p>\u5085\u91cc\u53f6\u7cfb\u6570\u7684\u6570\u91cf\u548c\u95f4\u9694\u53d6\u51b3\u4e8e\u4fe1\u53f7\u7684\u91c7\u6837\u7387\u548c\u957f\u5ea6\uff1a - \u4fe1\u53f7\u957f\u5ea6\u8d8a\u957f\uff0c\u9891\u57df\u5206\u8fa8\u7387\u8d8a\u9ad8\uff08\u9891\u7387\u95f4\u9694\u8d8a\u5c0f\uff09\u3002 - \u5982\u679c\u4fe1\u53f7\u662f\u5468\u671f\u4fe1\u53f7\uff0c\u5176\u9891\u8c31\u4f1a\u8868\u73b0\u4e3a\u79bb\u6563\u7684\u5cf0\u503c\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#19","title":"1.9 \u603b\u7ed3","text":"<p>\u5085\u91cc\u53f6\u53d8\u6362\u540e\u7684\u7cfb\u6570\u4e3b\u8981\u63cf\u8ff0\u4fe1\u53f7\u7684\u4ee5\u4e0b\u6027\u8d28\uff1a 1. \u9891\u7387\u6210\u5206\u7684\u5f3a\u5ea6\uff08\u5e45\u503c\uff09\u3002 2. \u9891\u7387\u6210\u5206\u7684\u65f6\u95f4\u504f\u79fb\uff08\u76f8\u4f4d\uff09\u3002 3. \u4fe1\u53f7\u7684\u76f4\u6d41\u5206\u91cf\uff08\u96f6\u9891\u5206\u91cf\uff09\u3002 4. \u4fe1\u53f7\u7684\u9891\u7387\u5206\u5e03\u548c\u80fd\u91cf\u5206\u5e03\u3002</p> <p>\u8fd9\u4e9b\u6027\u8d28\u5171\u540c\u5b9a\u4e49\u4e86\u4fe1\u53f7\u5728\u9891\u57df\u4e2d\u7684\u8868\u73b0\uff0c\u662f\u4fe1\u53f7\u5206\u6790\u548c\u5904\u7406\u7684\u57fa\u7840\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#110","title":"1.10 \u89d2\u9891\u7387\u4e0e\u9891\u7387","text":""},{"location":"book/chapter1_Introduction/1.2fourier_transform/#1101-frequency","title":"1.10.1 \u9891\u7387\uff08Frequency\uff09","text":"<ul> <li>\u5b9a\u4e49\uff1a\u9891\u7387\u8868\u793a\u4fe1\u53f7\u6bcf\u79d2\u5b8c\u6210\u632f\u8361\u7684\u6b21\u6570\uff0c\u901a\u5e38\u7528\u7b26\u53f7 \\(f\\) \u8868\u793a\u3002</li> <li>\u5355\u4f4d\uff1a\u8d6b\u5179 (Hz)\uff0c\u5373\u6bcf\u79d2\u632f\u8361\u7684\u5468\u671f\u6570\u3002</li> <li>\u516c\u5f0f\uff1a</li> </ul> <p>$$   f = \\frac{1}{T}   $$</p> <p>\u5176\u4e2d \\(T\\) \u662f\u4fe1\u53f7\u7684\u5468\u671f\uff0c\u8868\u793a\u5b8c\u6210\u4e00\u6b21\u5b8c\u6574\u632f\u8361\u6240\u9700\u7684\u65f6\u95f4\u3002</p> <p>\u793a\u4f8b\uff1a - \u4e00\u4e2a\u4fe1\u53f7\u6bcf\u79d2\u632f\u8361 50 \u6b21\uff0c\u5176\u9891\u7387\u4e3a \\(f = 50 \\, \\text{Hz}\\)\u3002 - \u7535\u7f51\u4ea4\u6d41\u7535\u7684\u9891\u7387\u901a\u5e38\u4e3a \\(50 \\, \\text{Hz}\\) \u6216 \\(60 \\, \\text{Hz}\\)\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#1102-angular-frequency","title":"1.10.2 \u89d2\u9891\u7387\uff08Angular Frequency\uff09","text":"<ul> <li>\u5b9a\u4e49\uff1a\u89d2\u9891\u7387\u8868\u793a\u4fe1\u53f7\u6bcf\u79d2\u8f6c\u8fc7\u7684\u5f27\u5ea6\u6570\uff0c\u901a\u5e38\u7528\u7b26\u53f7 \\(\\omega\\) \u8868\u793a\u3002</li> <li>\u5355\u4f4d\uff1a\u5f27\u5ea6\u6bcf\u79d2 (rad/s)\u3002</li> <li>\u516c\u5f0f\uff1a</li> </ul> <p>$$   \\omega = 2\\pi f   $$</p> <p>\u5176\u4e2d \\(2\\pi\\) \u8868\u793a\u4e00\u4e2a\u5b8c\u6574\u7684\u5706\uff08\u5bf9\u5e94\u4fe1\u53f7\u5b8c\u6210\u4e00\u4e2a\u5468\u671f\uff09\u3002</p> <p>\u89d2\u9891\u7387\u7684\u7269\u7406\u610f\u4e49\uff1a - \u4e00\u4e2a\u5468\u671f\u5185\uff0c\u4fe1\u53f7\u7684\u76f8\u4f4d\u53d8\u5316\u4e3a \\(2\\pi\\) \u5f27\u5ea6\uff08\u5bf9\u5e94\u5b8c\u6574\u7684\u4e00\u4e2a\u6ce2\u5f62\uff09\u3002 - \u89d2\u9891\u7387\u662f\u9891\u7387\u7684\u53e6\u4e00\u79cd\u8868\u793a\u65b9\u5f0f\uff0c\u66f4\u9002\u5408\u63cf\u8ff0\u6b63\u5f26\u6ce2\u4e2d\u7684\u632f\u8361\u7279\u6027\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#1103","title":"1.10.3 \u9891\u7387\u4e0e\u89d2\u9891\u7387\u7684\u533a\u522b\u4e0e\u8054\u7cfb","text":"<ul> <li>\u9891\u7387 \\(f\\) \u63cf\u8ff0\u7684\u662f\u4fe1\u53f7\u5728\u5355\u4f4d\u65f6\u95f4\u5185\u7684\u5468\u671f\u6570\uff08\u6b21/\u79d2\uff09\u3002</li> <li>\u89d2\u9891\u7387 \\(\\omega\\) \u63cf\u8ff0\u7684\u662f\u4fe1\u53f7\u5728\u5355\u4f4d\u65f6\u95f4\u5185\u7684\u76f8\u4f4d\u53d8\u5316\u91cf\uff08\u5f27\u5ea6/\u79d2\uff09\u3002</li> <li>\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u4e3a\uff1a</li> </ul> <p>$$   \\omega = 2\\pi f \\quad \\text{\u6216} \\quad f = \\frac{\\omega}{2\\pi}   $$</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#1104","title":"1.10.4 \u6b63\u5f26\u6ce2\u4e2d\u7684\u4f7f\u7528","text":"<p>\u5728\u6b63\u5f26\u6ce2\u4e2d\uff1a - \u4f7f\u7528\u89d2\u9891\u7387\u7684\u8868\u8fbe\uff1a</p> <p>$$   x(t) = A \\cos(\\omega t + \\phi)   $$</p> <p>\u5176\u4e2d\uff1a   - \\(\\omega\\) \u662f\u89d2\u9891\u7387\uff08\u5f27\u5ea6/\u79d2\uff09\u3002   - \\(A\\) \u662f\u632f\u5e45\u3002   - \\(\\phi\\) \u662f\u521d\u59cb\u76f8\u4f4d\u3002</p> <ul> <li>\u4f7f\u7528\u9891\u7387\u7684\u8868\u8fbe\uff1a</li> </ul> <p>$$   x(t) = A \\cos(2\\pi f t + \\phi)   $$</p> <p>\u6ce8\u610f\uff1a - \u89d2\u9891\u7387\u5728\u6b63\u5f26\u4fe1\u53f7\u7684\u6570\u5b66\u5206\u6790\u4e2d\u66f4\u4e3a\u5e38\u7528\u3002 - \u9891\u7387\u7528\u4e8e\u63cf\u8ff0\u4fe1\u53f7\u7684\u5468\u671f\u6027\u548c\u8ba1\u6570\u7279\u6027\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#1105","title":"1.10.5 \u793a\u4f8b\u5bf9\u6bd4","text":"<p>\u5047\u8bbe\u4e00\u4e2a\u4fe1\u53f7\u7684\u9891\u7387\u4e3a \\(f = 10 \\, \\text{Hz}\\)\uff1a - \u5468\u671f\uff1a</p> <p>$$   T = \\frac{1}{f} = \\frac{1}{10} = 0.1 \\, \\text{\u79d2}   $$</p> <ul> <li>\u89d2\u9891\u7387\uff1a</li> </ul> <p>$$   \\omega = 2\\pi f = 2\\pi \\cdot 10 \\approx 62.8 \\, \\text{rad/s}   $$</p> <p>\u8fd9\u610f\u5473\u7740\u6bcf\u79d2\u5b8c\u6210 10 \u4e2a\u5468\u671f\uff0c\u540c\u65f6\u6bcf\u79d2\u76f8\u4f4d\u53d8\u5316\u7ea6\u4e3a \\(62.8 \\, \\text{\u5f27\u5ea6}\\)\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#1106","title":"1.10.6 \u4fe1\u53f7\u9891\u8c31\u56fe","text":"<p>\u70b9\u51fb\u94fe\u63a5\u67e5\u770b\u4fe1\u53f7\u9891\u8c31\u4ecb\u7ecd  \u7eb5\u8f74\u8868\u793a\u5e45\u5ea6\uff0c\u6a2a\u8f74\u8868\u793a\u89d2\u8bc4\u7387\uff0c\u5ffd\u7565\u4e86\u76f8\u4f4d\u4fe1\u606f\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#1107","title":"1.10.7 \u603b\u7ed3","text":"\u7279\u6027 \u9891\u7387 \\(f\\) \u89d2\u9891\u7387 \\(\\omega\\) \u5355\u4f4d \u8d6b\u5179 (Hz) \u5f27\u5ea6\u6bcf\u79d2 (rad/s) \u63cf\u8ff0 \u6bcf\u79d2\u7684\u5468\u671f\u6570 \u6bcf\u79d2\u7684\u76f8\u4f4d\u53d8\u5316\u91cf \u5173\u7cfb \\(f = \\frac{\\omega}{2\\pi}\\) \\(\\omega = 2\\pi f\\) \u5e94\u7528\u573a\u666f \u5468\u671f\u6027\u4fe1\u53f7\u7684\u63cf\u8ff0 \u6b63\u5f26\u6ce2\u548c\u52a8\u6001\u7cfb\u7edf\u7684\u6570\u5b66\u8868\u8fbe"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#2","title":"2. \u5468\u671f\u4fe1\u53f7\u548c\u975e\u5468\u671f\u4fe1\u53f7\u7684\u9891\u8c31","text":""},{"location":"book/chapter1_Introduction/1.2fourier_transform/#21","title":"2.1 \u5468\u671f\u4fe1\u53f7\u4e0e\u975e\u5468\u671f\u4fe1\u53f7\u7684\u9891\u8c31","text":""},{"location":"book/chapter1_Introduction/1.2fourier_transform/#211","title":"2.1.1 \u57fa\u672c\u6982\u5ff5","text":"<ul> <li>\u5468\u671f\u4fe1\u53f7\u7684\u9891\u8c31\u662f\u79bb\u6563\u7684\uff0c\u7531\u57fa\u9891\u53ca\u5176\u6574\u6570\u500d\uff08\u8c10\u6ce2\uff09\u7ec4\u6210\u3002   \u5468\u671f\u4fe1\u53f7\u53ef\u4ee5\u7528\u5085\u91cc\u53f6\u7ea7\u6570\u8868\u793a\u4e3a\uff1a</li> </ul> <p>$$   x(t) = \\sum_{n=-\\infty}^\\infty c_n e^{j 2\\pi n f_0 t}   $$</p> <p>\u5176\u4e2d \\(f_0 = 1/T\\) \u662f\u57fa\u9891\uff0c\\(c_n\\) \u662f\u5085\u91cc\u53f6\u7cfb\u6570\u3002</p> <ul> <li>\u975e\u5468\u671f\u4fe1\u53f7\u7684\u9891\u8c31\u662f\u8fde\u7eed\u7684\uff0c\u4e0d\u80fd\u4ec5\u7528\u57fa\u9891\u53ca\u5176\u6574\u6570\u500d\u7684\u9891\u7387\u6765\u8868\u793a\u3002   \u975e\u5468\u671f\u4fe1\u53f7\u901a\u5e38\u901a\u8fc7\u5085\u91cc\u53f6\u53d8\u6362\u5f97\u5230\u9891\u8c31\uff1a</li> </ul> <p>$$   X(f) = \\int_{-\\infty}^\\infty x(t) e^{-j 2\\pi f t} dt   $$</p> <p>\u5b83\u5728\u6574\u4e2a\u9891\u7387\u8f74\u4e0a\u662f\u8fde\u7eed\u5206\u5e03\u7684\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#22","title":"2.2 \u4fe1\u53f7\u91cd\u5efa\u5206\u6790","text":""},{"location":"book/chapter1_Introduction/1.2fourier_transform/#221","title":"2.2.1 \u5b8c\u5168\u91cd\u5efa","text":"<ul> <li>\u5468\u671f\u4fe1\u53f7\u53ef\u4ee5\u5b8c\u5168\u7528\u57fa\u9891\u53ca\u5176\u6574\u6570\u500d\u7684\u9891\u7387\u5206\u91cf\u6765\u8868\u793a\u3002</li> <li>\u975e\u5468\u671f\u4fe1\u53f7\u5219\u4e0d\u80fd\u5b8c\u5168\u91cd\u5efa\uff0c\u56e0\u4e3a\u5b83\u7684\u9891\u8c31\u662f\u8fde\u7eed\u7684\uff0c\u9891\u7387\u6210\u5206\u4e0d\u5c40\u9650\u4e8e\u79bb\u6563\u70b9\u3002</li> </ul>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#222","title":"2.2.2 \u8fd1\u4f3c\u91cd\u5efa","text":"<p>\u5c3d\u7ba1\u975e\u5468\u671f\u4fe1\u53f7\u65e0\u6cd5\u5b8c\u5168\u7528\u79bb\u6563\u9891\u7387\u6210\u5206\u6765\u91cd\u5efa\uff0c\u4f46\u53ef\u4ee5\u7528\u6709\u9650\u4e2a\u79bb\u6563\u9891\u7387\u6210\u5206\u6765\u8fd1\u4f3c\u91cd\u5efa\uff0c\u5373\uff1a</p> \\[ x(t) \\approx \\sum_{n=-N}^N c_n e^{j 2\\pi n f_0 t} \\] <p>\u5f53\u9009\u53d6\u7684\u9891\u7387\u70b9\uff08\\(N\\) \u8f83\u5927\uff09\u8db3\u591f\u591a\u65f6\uff0c\u53ef\u4ee5\u8fd1\u4f3c\u6a21\u62df\u4fe1\u53f7\u7684\u4e3b\u8981\u7279\u6027\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u6709\u4ee5\u4e0b\u9650\u5236\uff1a - \u6a21\u62df\u7684\u7ed3\u679c\u662f\u6709\u9650\u7cbe\u5ea6\u7684\u3002 - \u5bf9\u9ad8\u9891\u5206\u91cf\u8f83\u4e30\u5bcc\u7684\u4fe1\u53f7\uff0c\u8fd1\u4f3c\u9700\u8981\u66f4\u591a\u7684\u9891\u7387\u70b9\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#23","title":"2.3 \u793a\u4f8b\u5206\u6790","text":""},{"location":"book/chapter1_Introduction/1.2fourier_transform/#231","title":"2.3.1 \u5468\u671f\u4fe1\u53f7\u793a\u4f8b","text":"<p>\u8bbe \\(x(t) = \\cos(2\\pi f_0 t)\\) \u662f\u4e00\u4e2a\u5468\u671f\u4fe1\u53f7\uff0c\u5176\u9891\u8c31\u4e3a\uff1a</p> \\[ X(f) = \\frac{1}{2}[\\delta(f - f_0) + \\delta(f + f_0)] \\] <p>\u663e\u7136\u53ef\u4ee5\u7528\u57fa\u9891 \\(f_0\\) \u7684\u79bb\u6563\u9891\u7387\u5206\u91cf\u8868\u793a\uff0c\u65e0\u9700\u989d\u5916\u9891\u7387\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#232","title":"2.3.2 \u975e\u5468\u671f\u4fe1\u53f7\u793a\u4f8b\uff08\u77e9\u5f62\u8109\u51b2\uff09","text":"<p>\u8bbe \\(x(t)\\) \u662f\u4e00\u4e2a\u5bbd\u5ea6\u4e3a \\(T\\) \u7684\u77e9\u5f62\u8109\u51b2\uff1a</p> \\[ x(t) = \\begin{cases} 1, &amp; -T/2 \\leq t \\leq T/2 \\\\ 0, &amp; \\text{\u5176\u4ed6} \\end{cases} \\] <p>\u5176\u9891\u8c31\u4e3a\uff1a</p> \\[ X(f) = T \\cdot \\text{sinc}(fT) \\] <p>\\(\\text{sinc}(fT)\\) \u662f\u4e00\u4e2a\u8fde\u7eed\u51fd\u6570\uff0c\u8986\u76d6\u6240\u6709\u9891\u7387\uff0c\u4e0d\u80fd\u5b8c\u5168\u7528\u79bb\u6563\u9891\u7387\u8868\u793a\u3002\u4f46\u5982\u679c\u53ea\u53d6\u82e5\u5e72\u4e3b\u8981\u9891\u7387\u5206\u91cf\uff0c\u53ef\u4ee5\u8fd1\u4f3c\u91cd\u5efa\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#233","title":"2.3.3 \u5e26\u566a\u4fe1\u53f7\u793a\u4f8b","text":"<p>\u5bf9\u4e8e\u975e\u5468\u671f\u4e14\u5e26\u566a\u58f0\u7684\u4fe1\u53f7\uff0c\u5176\u9891\u8c31\u53ef\u80fd\u5206\u5e03\u5728\u4e00\u4e2a\u8f83\u5bbd\u7684\u9891\u7387\u8303\u56f4\u5185\uff0c\u7528\u79bb\u6563\u9891\u7387\u5206\u91cf\u53ea\u80fd\u90e8\u5206\u8fd1\u4f3c\u91cd\u5efa\uff0c\u4e14\u9ad8\u9891\u90e8\u5206\u7684\u8bef\u5dee\u53ef\u80fd\u8f83\u5927\u3002</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#24","title":"2.4 \u5e38\u89c1\u7684\u9891\u8c31\u903c\u8fd1\u65b9\u6cd5","text":"<p>\u5bf9\u4e8e\u975e\u5468\u671f\u4fe1\u53f7\uff0c\u5982\u679c\u5e0c\u671b\u7528\u79bb\u6563\u9891\u7387\u8868\u793a\uff0c\u53ef\u4ee5\u8003\u8651\u4ee5\u4e0b\u65b9\u6cd5\uff1a</p>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#241-dft","title":"2.4.1 \u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362 (DFT)","text":"<ul> <li>\u5c06\u8fde\u7eed\u4fe1\u53f7\u91c7\u6837\u4e3a\u79bb\u6563\u4fe1\u53f7\uff0c\u5e76\u7528\u6709\u9650\u9891\u7387\u70b9\u7684\u9891\u8c31\u8868\u793a\u3002</li> <li>\u7ed3\u679c\u662f\u4fe1\u53f7\u7684\u9891\u8c31\u5728\u6709\u9650\u9891\u7387\u70b9\u4e0a\u7684\u79bb\u6563\u8fd1\u4f3c\u3002</li> </ul>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#242","title":"2.4.2 \u7a97\u53e3\u5316\u5904\u7406","text":"<ul> <li>\u975e\u5468\u671f\u4fe1\u53f7\u53ef\u4ee5\u901a\u8fc7\u65f6\u95f4\u7a97\u53e3\u622a\u65ad\uff0c\u4f7f\u5176\u8fd1\u4f3c\u4e3a\u6709\u9650\u65f6\u957f\u7684\u5468\u671f\u4fe1\u53f7\u3002</li> <li>\u622a\u65ad\u540e\u7684\u4fe1\u53f7\u9891\u8c31\u4f1a\u63a5\u8fd1\u4e8e\u79bb\u6563\u9891\u7387\u5206\u91cf\uff0c\u4f46\u4f1a\u5f15\u5165\u9891\u8c31\u6cc4\u9732\u3002</li> </ul>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#243","title":"2.4.3 \u5e26\u5bbd\u9650\u5236","text":"<ul> <li>\u5bf9\u975e\u5468\u671f\u4fe1\u53f7\u65bd\u52a0\u4f4e\u901a\u6ee4\u6ce2\u5668\uff0c\u53bb\u6389\u9ad8\u9891\u5206\u91cf\u3002</li> <li>\u5e26\u5bbd\u9650\u5236\u540e\uff0c\u4fe1\u53f7\u9891\u8c31\u80fd\u66f4\u597d\u5730\u7528\u6709\u9650\u79bb\u6563\u9891\u7387\u6a21\u62df\u3002</li> </ul>"},{"location":"book/chapter1_Introduction/1.2fourier_transform/#25","title":"2.5 \u603b\u7ed3","text":"<ul> <li>\u5b8c\u5168\u91cd\u5efa\uff1a\u53ea\u6709\u5468\u671f\u4fe1\u53f7\u80fd\u5b8c\u5168\u7528\u57fa\u9891\u53ca\u5176\u6574\u6570\u500d\u9891\u7387\u8868\u793a\u3002</li> <li>\u8fd1\u4f3c\u91cd\u5efa\uff1a\u975e\u5468\u671f\u4fe1\u53f7\u53ef\u4ee5\u7528\u6709\u9650\u4e2a\u79bb\u6563\u9891\u7387\u5206\u91cf\u8fdb\u884c\u8fd1\u4f3c\uff0c\u4f46\u65e0\u6cd5\u5b8c\u5168\u91cd\u5efa\u3002</li> <li>\u5bf9\u4e8e\u975e\u5468\u671f\u4fe1\u53f7\uff0c\u7528\u79bb\u6563\u9891\u7387\u8868\u793a\u7684\u7cbe\u5ea6\u53d6\u51b3\u4e8e\uff1a</li> <li>\u9891\u7387\u5206\u91cf\u7684\u6570\u91cf\u3002</li> <li>\u4fe1\u53f7\u7684\u9891\u8c31\u5e26\u5bbd\u3002</li> </ul>"},{"location":"book/chapter1_Introduction/1.3signal_processing/","title":"signal processing","text":""},{"location":"book/chapter1_Introduction/1.3signal_processing/#_1","title":"\u91c7\u6837\u5b9a\u7406","text":"<pre><code>\u6838\u5fc3\uff1a\u91c7\u6837\u9891\u7387\u9700\u8981\u662f\u539f\u59cb\u4fe1\u53f7\u7684\u5e26\u5bbd\u7684\u4e24\u500d\n</code></pre>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#_2","title":"\u91c7\u6837\u5b9a\u7406\u7684\u6570\u5b66\u63cf\u8ff0","text":"<p>\u5047\u8bbe\u4e00\u4e2a\u8fde\u7eed\u65f6\u95f4\u4fe1\u53f7 \\(x(t)\\) \u662f\u5e26\u9650\u7684\uff0c\u5373\u5b83\u7684\u9891\u8c31\uff08\u5085\u91cc\u53f6\u53d8\u6362\uff09\u6ee1\u8db3\uff1a \\(X(f)=0,\\quad\\forall |f|&gt;B\\), \u5176\u4e2d \\(B\\) \u662f\u4fe1\u53f7\u7684\u6700\u9ad8\u9891\u7387\uff08\u5e26\u5bbd\uff09\uff0c\u5355\u4f4d\u662f\u8d6b\u5179\uff08Hz\uff09\u3002\u8fd9\u79cd\u4fe1\u53f7\u79f0\u4e3a\u5e26\u9650\u4fe1\u53f7\u3002 \u6839\u636e\u91c7\u6837\u5b9a\u7406\uff1a</p> <ol> <li>\u5982\u679c\u7528\u91c7\u6837\u95f4\u9694 \\(T_s = \\frac{1}{2B}\\) \u6216\u66f4\u5c0f\u7684\u65f6\u95f4\u95f4\u9694\u5bf9\u4fe1\u53f7 \\(x(t)\\) \u8fdb\u884c\u91c7\u6837\uff1a\\(x[n] = x(nT_s), \\quad n \\in \\mathbb{Z},\\)    \u5219\u53ef\u4ee5\u901a\u8fc7\u91c7\u6837\u503c\u5b8c\u5168\u91cd\u5efa\u539f\u4fe1\u53f7 \\(x(t)\\)\u3002</li> <li>\u91cd\u5efa\u516c\u5f0f\u662f\u901a\u8fc7\u5948\u594e\u65af\u7279\u91cd\u5efa\u516c\u5f0f\u7ed9\u51fa\u7684\uff1a</li> </ol> <p>$$    x(t) = \\sum_{n=-\\infty}^{\\infty} x[n] \\, \\text{sinc}\\left(\\frac{t - nT_s}{T_s}\\right),    $$</p> <p>\u5176\u4e2d    \\(\\text{sinc}(x) = \\frac{\\sin(\\pi x)}{\\pi x}\\)</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#_3","title":"\u5173\u952e\u6761\u4ef6\u4e0e\u89e3\u91ca","text":"<ul> <li>\u5e26\u9650\u6027\uff1a\u4fe1\u53f7 $ x(t) $ \u7684\u9891\u8c31\u4e2d\u6ca1\u6709\u8d85\u8fc7 $ B $ \u7684\u9891\u7387\u5206\u91cf\u3002</li> <li>\u91c7\u6837\u7387\u8981\u6c42\uff1a\u91c7\u6837\u9891\u7387\u5fc5\u987b\u6ee1\u8db3 $ f_s \\geq 2B $\uff0c\u5373\u91c7\u6837\u95f4\u9694 $ T_s \\leq \\frac{1}{2B} $\u3002\u8fd9\u91cc\u7684 $ f_s = \\frac{1}{T_s} $ \u662f\u91c7\u6837\u9891\u7387\uff0c\u79f0\u4e3a\u5948\u594e\u65af\u7279\u9891\u7387\u3002 \u53ea\u6709\u5728\u6ee1\u8db3\u8fd9\u4e9b\u6761\u4ef6\u65f6\uff0c\u91c7\u6837\u4fe1\u53f7 $ x[n] $ \u53ef\u4ee5\u65e0\u635f\u5730\u91cd\u5efa\u4e3a\u539f\u4fe1\u53f7 $ x(t) $.</li> </ul>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#_4","title":"\u91cd\u5efa\u4f8b\u5b50","text":""},{"location":"book/chapter1_Introduction/1.3signal_processing/#13","title":"1.3 \u91cd\u5efa\u4f8b\u5b50","text":"<p>\u8003\u8651\u4e00\u4e2a\u8fde\u7eed\u65f6\u95f4\u4fe1\u53f7\uff1a</p> \\[ x(t) = \\cos(10\\pi t) \\] <p>\u8fd9\u4e2a\u4fe1\u53f7\u7684\u9891\u7387\u4e3a \\(f = 5\\text{ Hz}\\)\uff08\u56e0\u4e3a \\(10\\pi = 2\\pi \\cdot 5\\)\uff09\u3002\u6839\u636e\u5948\u594e\u65af\u7279\u91c7\u6837\u5b9a\u7406\uff0c\u91c7\u6837\u9891\u7387\u5e94\u6ee1\u8db3\uff1a</p> \\[ f_s &gt; 2f = 10\\text{ Hz} \\] <p>\u8ba9\u6211\u4eec\u9009\u62e9\u91c7\u6837\u9891\u7387 \\(f_s = 12\\text{ Hz}\\)\uff0c\u5373\u91c7\u6837\u95f4\u9694 \\(T_s = \\frac{1}{12}\\text{ s}\\)\u3002</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#131","title":"1.3.1 \u91c7\u6837\u8fc7\u7a0b","text":"<p>\u91c7\u6837\u70b9\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a</p> \\[ x[n] = \\cos(10\\pi \\cdot \\frac{n}{12}), \\quad n = 0, \\pm1, \\pm2, \\dots \\]"},{"location":"book/chapter1_Introduction/1.3signal_processing/#132","title":"1.3.2 \u91cd\u5efa\u516c\u5f0f","text":"<p>\u6839\u636e\u5948\u594e\u65af\u7279\u91cd\u5efa\u516c\u5f0f\uff0c\u539f\u59cb\u4fe1\u53f7\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u516c\u5f0f\u91cd\u5efa\uff1a</p> \\[ x(t) = \\sum_{n=-\\infty}^{\\infty} x[n] \\cdot \\text{sinc}(\\frac{t-nT_s}{T_s}) \\] <p>\u4ee3\u5165\u6211\u4eec\u7684\u4f8b\u5b50\uff1a</p> \\[ x(t) = \\sum_{n=-\\infty}^{\\infty} \\cos(10\\pi \\cdot \\frac{n}{12}) \\cdot \\text{sinc}(12t-n) \\]"},{"location":"book/chapter1_Introduction/1.3signal_processing/#133","title":"1.3.3 \u91cd\u5efa\u539f\u7406\u89e3\u91ca","text":"<ol> <li>\u6bcf\u4e2a\u91c7\u6837\u70b9 \\(x[n]\\) \u90fd\u901a\u8fc7 sinc \u51fd\u6570\u8fdb\u884c\u63d2\u503c</li> <li>sinc \u51fd\u6570\u5177\u6709\u4ee5\u4e0b\u6027\u8d28\uff1a</li> <li>\u5728\u91c7\u6837\u70b9\u5904\uff0c\u5f53\u524d\u91c7\u6837\u70b9\u7684 sinc \u51fd\u6570\u503c\u4e3a 1\uff0c\u5176\u4ed6\u91c7\u6837\u70b9\u5904\u4e3a 0</li> <li>\u6240\u6709\u91c7\u6837\u70b9\u7684 sinc \u51fd\u6570\u4e4b\u548c\u5728\u4efb\u610f\u65f6\u523b t \u90fd\u80fd\u91cd\u5efa\u51fa\u539f\u59cb\u4fe1\u53f7\u503c</li> </ol>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#134","title":"1.3.4 \u9a8c\u8bc1","text":"<p>\u53ef\u4ee5\u8bc1\u660e\u8fd9\u4e2a\u91cd\u5efa\u516c\u5f0f\u786e\u5b9e\u80fd\u5f97\u5230\u539f\u59cb\u4fe1\u53f7 \\(\\cos(10\\pi t)\\)\uff1a</p> <ol> <li>\u91cd\u5efa\u4fe1\u53f7\u7684\u9891\u8c31\u88ab\u9650\u5236\u5728 \\([-6\\text{ Hz}, 6\\text{ Hz}]\\) \u8303\u56f4\u5185</li> <li>\u7531\u4e8e\u539f\u59cb\u4fe1\u53f7\u9891\u7387\u4e3a 5 Hz\uff0c\u4f4e\u4e8e\u5948\u594e\u65af\u7279\u9891\u7387 6 Hz</li> <li>\u56e0\u6b64\u91cd\u5efa\u4fe1\u53f7\u4e0e\u539f\u59cb\u4fe1\u53f7\u5b8c\u5168\u76f8\u540c</li> </ol>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#135","title":"1.3.5 \u63a8\u5bfc\u8fc7\u7a0b","text":"<p>\u8ba9\u6211\u4eec\u8bc1\u660e\u91cd\u5efa\u516c\u5f0f \\(\\sum_{n=-\\infty}^{\\infty} \\cos(10\\pi \\cdot \\frac{n}{12}) \\cdot \\text{sinc}(12t-n)\\) \u7b49\u4e8e \\(\\cos(10\\pi t)\\)\uff1a</p> <ol> <li>\u9891\u57df\u5206\u6790</li> <li> <p>\u539f\u59cb\u4fe1\u53f7 \\(\\cos(10\\pi t)\\) \u7684\u9891\u8c31\u5305\u542b\u4e24\u4e2a\u51b2\u6fc0\u51fd\u6570\uff1a</p> <p>$$  X(f) = \\frac{1}{2}[\\delta(f-5) + \\delta(f+5)]  $$</p> </li> <li> <p>\u91c7\u6837\u4fe1\u53f7\u9891\u8c31</p> </li> <li>\u91c7\u6837\u540e\u7684\u4fe1\u53f7\u9891\u8c31\u662f\u539f\u59cb\u9891\u8c31\u7684\u5468\u671f\u5ef6\u62d3\uff0c\u5468\u671f\u4e3a\u91c7\u6837\u9891\u7387 \\(f_s = 12\\text{ Hz}\\)\uff1a</li> </ol> <p>$$X_s(f) = \\frac{1}{12}\\sum_{k=-\\infty}^{\\infty} \\frac{1}{2}[\\delta(f-5-12k) + \\delta(f+5-12k)]    $$</p> <ol> <li>\u91cd\u5efa\u6ee4\u6ce2\u5668</li> <li>sinc \u51fd\u6570\u7684\u5085\u91cc\u53f6\u53d8\u6362\u662f\u77e9\u5f62\u7a97\uff1a</li> </ol> <p>$$    \\mathcal{F}{\\text{sinc}(12t)} = \\text{rect}(\\frac{f}{12})    $$</p> <ul> <li> <p>\u8fd9\u4e2a\u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\u5668\u5728 \\([-6\\text{ Hz}, 6\\text{ Hz}]\\) \u8303\u56f4\u5185\u4e3a 1\uff0c\u5176\u4ed6\u9891\u7387\u4e3a 0</p> </li> <li> <p>\u9891\u57df\u91cd\u5efa\u8fc7\u7a0b</p> </li> <li> <p>\u91cd\u5efa\u8fc7\u7a0b\u7b49\u4ef7\u4e8e\u91c7\u6837\u4fe1\u53f7\u4e0e\u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\u5668\u76f8\u4e58\uff1a</p> <p>$$  X_r(f) = X_s(f) \\cdot \\text{rect}(\\frac{f}{12})  $$</p> </li> <li> <p>\u7531\u4e8e\u539f\u59cb\u4fe1\u53f7\u9891\u7387\uff085 Hz\uff09\u5c0f\u4e8e\u5948\u594e\u65af\u7279\u9891\u7387\uff086 Hz\uff09\uff0c\u53ea\u6709 \\(k=0\\) \u7684\u9891\u8c31\u5206\u91cf\u4f1a\u88ab\u4fdd\u7559\uff1a</p> <p>$$  X_r(f) = \\frac{1}{2}[\\delta(f-5) + \\delta(f+5)]  $$</p> </li> <li> <p>\u65f6\u57df\u7ed3\u679c</p> </li> <li> <p>\u5bf9 \\(X_r(f)\\) \u8fdb\u884c\u9006\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u5f97\u5230\uff1a</p> <p>$$  x_r(t) = \\cos(10\\pi t)  $$</p> </li> </ul> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u91cd\u5efa\u516c\u5f0f\u786e\u5b9e\u80fd\u5b8c\u7f8e\u91cd\u5efa\u539f\u59cb\u4fe1\u53f7 \\(\\cos(10\\pi t)\\)\u3002\u8fd9\u4e2a\u63a8\u5bfc\u8fc7\u7a0b\u4e5f\u8bf4\u660e\u4e86\u4e3a\u4ec0\u4e48\u91c7\u6837\u9891\u7387\u5fc5\u987b\u5927\u4e8e\u4fe1\u53f7\u9891\u7387\u7684\u4e24\u500d\uff1a\u53ea\u6709\u8fd9\u6837\uff0c\u91c7\u6837\u540e\u7684\u9891\u8c31\u5468\u671f\u5ef6\u62d3\u624d\u4e0d\u4f1a\u53d1\u751f\u6df7\u53e0\u3002</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#2","title":"2. \u9891\u8c31\u6df7\u53e0","text":""},{"location":"book/chapter1_Introduction/1.3signal_processing/#_5","title":"\u72c4\u62c9\u514b\u51fd\u6570\u4e0e\u72c4\u62c9\u514b\u68b3\u72b6\u51fd\u6570\u7684\u7279\u6027","text":""},{"location":"book/chapter1_Introduction/1.3signal_processing/#1-dirac-delta-function","title":"1. \u72c4\u62c9\u514b\u51fd\u6570\uff08Dirac Delta Function\uff09","text":""},{"location":"book/chapter1_Introduction/1.3signal_processing/#_6","title":"\u5b9a\u4e49","text":"<ul> <li>\u72c4\u62c9\u514b\u51fd\u6570 \\(\\delta(t)\\) \u662f\u4e00\u4e2a\u7406\u60f3\u5316\u7684\u8109\u51b2\u51fd\u6570\uff0c\u7528\u4e8e\u63cf\u8ff0\u65f6\u95f4 \\(t=0\\) \u65f6\u7684\u65e0\u9650\u7a84\u548c\u65e0\u9650\u9ad8\u7684\u4fe1\u53f7\u3002 </li> <li>\u6ee1\u8db3\u4ee5\u4e0b\u6027\u8d28\uff1a</li> </ul> <p>$$   \\int_{-\\infty}^\\infty \\delta(t) \\, dt = 1   $$</p> <p>\u5bf9\u4efb\u610f\u51fd\u6570 \\(x(t)\\)\uff0c\u6709\uff1a</p> <p>$$   \\int_{-\\infty}^\\infty x(t) \\delta(t - t_0) \\, dt = x(t_0)   $$</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#_7","title":"\u7279\u6027","text":"<ol> <li>\u96c6\u4e2d\u6027\uff1a\\(\\delta(t) = 0\\) \u5f53 \\(t \\neq 0\\)\u3002</li> <li>\u5355\u4f4d\u5316\uff1a\\(\\int_{-\\infty}^\\infty \\delta(t) \\, dt = 1\\)\u3002</li> <li>\u5bf9\u79f0\u6027\uff1a\\(\\delta(-t) = \\delta(t)\\)\u3002</li> <li>\u7f29\u653e\u6027\uff1a\\(\\delta(at) = \\frac{1}{|a|} \\delta(t)\\)\u3002</li> <li>\u5377\u79ef\u7279\u6027\uff1a</li> <li>\u4e0e\u4efb\u610f\u51fd\u6570\u5377\u79ef\uff1a\\((x * \\delta)(t) = x(t)\\)\u3002</li> <li>\u4e0e\u4f4d\u79fb\u7684\u72c4\u62c9\u514b\u51fd\u6570\u5377\u79ef\uff1a\\((x * \\delta(t - t_0))(t) = x(t - t_0)\\)\u3002</li> </ol>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#2-dirac-comb-function","title":"2. \u72c4\u62c9\u514b\u68b3\u72b6\u51fd\u6570\uff08Dirac Comb Function\uff09","text":""},{"location":"book/chapter1_Introduction/1.3signal_processing/#_8","title":"\u72c4\u62c9\u514b\u68b3\u72b6\u51fd\u6570\u5b9a\u4e49","text":"<ul> <li>\u72c4\u62c9\u514b\u68b3\u72b6\u51fd\u6570 \\(\\mathrm{III}(t)\\) \u662f\u4e00\u5217\u7b49\u95f4\u9694\u7684\u72c4\u62c9\u514b\u51fd\u6570\u7ec4\u6210\u7684\u5468\u671f\u6027\u8109\u51b2\u5217\uff1a </li> </ul> <p>$$   \\mathrm{III}(t) = \\sum_{n=-\\infty}^\\infty \\delta(t - nT)   $$</p> <p>\u5176\u4e2d \\(T\\) \u662f\u8109\u51b2\u95f4\u9694\u3002</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#_9","title":"\u72c4\u62c9\u514b\u68b3\u72b6\u51fd\u6570\u7684\u7279\u6027","text":"<ol> <li>\u5468\u671f\u6027\uff1a\\(\\mathrm{III}(t)\\) \u662f\u5468\u671f\u4e3a \\(T\\) \u7684\u51fd\u6570\u3002</li> <li>\u9891\u57df\u5173\u7cfb\uff1a</li> <li> <p>\u5085\u91cc\u53f6\u53d8\u6362\u4ecd\u662f\u4e00\u4e2a\u72c4\u62c9\u514b\u68b3\u72b6\u51fd\u6570\uff0c\u9891\u7387\u95f4\u9694\u4e3a \\(\\frac{1}{T}\\)\uff1a</p> <p>$$  \\mathcal{F}{\\mathrm{III}(t)} = \\frac{1}{T} \\mathrm{III}\\left(\\frac{f}{T}\\right)  $$</p> </li> <li> <p>\u5377\u79ef\u7279\u6027\uff1a</p> </li> <li>\u4e0e\u4efb\u610f\u51fd\u6570\u5377\u79ef\uff1a\\((x * \\mathrm{III})(t) = \\sum_{n=-\\infty}^\\infty x(t - nT)\\)\u3002</li> <li>\u4e0e\u81ea\u8eab\u5377\u79ef\uff1a\\(\\mathrm{III}(t) * \\mathrm{III}(t) = \\mathrm{III}(t)\\)\u3002</li> </ol>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#3","title":"3. \u72c4\u62c9\u514b\u51fd\u6570\u4e0e\u4fe1\u53f7\u91c7\u6837","text":""},{"location":"book/chapter1_Introduction/1.3signal_processing/#_10","title":"\u91c7\u6837\u8fc7\u7a0b","text":"<ul> <li>\u8fde\u7eed\u4fe1\u53f7 \\(x(t)\\) \u7684\u91c7\u6837\u53ef\u8868\u793a\u4e3a\u4e0e\u72c4\u62c9\u514b\u68b3\u72b6\u51fd\u6570\u7684\u4e58\u79ef\uff1a</li> </ul> <p>$$   x_s(t) = x(t) \\cdot \\mathrm{III}(t) = \\sum_{n=-\\infty}^\\infty x(nT) \\delta(t - nT)   $$</p> <p>\u5176\u4e2d \\(T\\) \u662f\u91c7\u6837\u95f4\u9694\u3002 </p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#_11","title":"\u9891\u57df\u7279\u6027","text":"<ul> <li>\u91c7\u6837\u7684\u9891\u8c31\u662f\u539f\u4fe1\u53f7\u9891\u8c31\u7684\u5468\u671f\u6027\u5ef6\u62d3\uff1a</li> </ul> <p>$$   X_s(f) = \\frac{1}{T} \\sum_{k=-\\infty}^\\infty X(f - kf_s)   $$</p> <p>\u5176\u4e2d \\(f_s = \\frac{1}{T}\\) \u662f\u91c7\u6837\u9891\u7387\u3002    \u4fe1\u53f7\u9891\u8c31        \u91c7\u6837\u578b\u53f7\u9891\u8c31    </p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#4","title":"4. \u603b\u7ed3","text":"\u7279\u6027 \u72c4\u62c9\u514b\u51fd\u6570 \u72c4\u62c9\u514b\u68b3\u72b6\u51fd\u6570 \u5b9a\u4e49 \u7406\u60f3\u5316\u7684\u8109\u51b2\uff0c\u53ea\u6709 \\(t=0\\) \u65f6\u975e\u96f6 \u5468\u671f\u6027\u8109\u51b2\u5217\uff0c\u7531\u591a\u4e2a\u72c4\u62c9\u514b\u51fd\u6570\u7ec4\u6210 \u6570\u5b66\u8868\u793a \\(\\delta(t)\\) \\(\\mathrm{III}(t) = \\sum_{n=-\\infty}^\\infty \\delta(t - nT)\\) \u5377\u79ef\u7279\u6027 \u4fdd\u6301\u51fd\u6570\u539f\u6837\u6216\u5e73\u79fb \u751f\u6210\u4fe1\u53f7\u7684\u5468\u671f\u6027\u91cd\u590d \u9891\u57df\u5173\u7cfb \u4e0e\u4efb\u610f\u51fd\u6570\u5377\u79ef\u4fdd\u6301\u9891\u8c31\u4e0d\u53d8 \u9891\u8c31\u5ef6\u62d3\u6216\u5468\u671f\u6027\u91cd\u590d \u91c7\u6837\u4e2d\u7684\u4f5c\u7528 \u63d0\u53d6\u8fde\u7eed\u4fe1\u53f7\u7684\u79bb\u6563\u503c \u8868\u793a\u91c7\u6837\u8fc7\u7a0b\u53ca\u9891\u8c31\u5ef6\u62d3"},{"location":"book/chapter1_Introduction/1.3signal_processing/#3_1","title":"3. \u6ee4\u6ce2\u5668","text":"<p>\u6ee4\u6ce2\u5668\u662f\u4e00\u79cd\u5bf9\u4fe1\u53f7\u8fdb\u884c\u9891\u7387\u9009\u62e9\u6027\u5904\u7406\u7684\u7cfb\u7edf\uff0c\u53ef\u4ee5\u8ba9\u67d0\u4e9b\u9891\u7387\u6210\u5206\u901a\u8fc7\uff0c\u540c\u65f6\u6291\u5236\u6216\u963b\u6b62\u5176\u4ed6\u9891\u7387\u6210\u5206\u3002</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#31","title":"3.1 \u57fa\u672c\u6982\u5ff5","text":""},{"location":"book/chapter1_Introduction/1.3signal_processing/#311","title":"3.1.1 \u9891\u7387\u54cd\u5e94","text":"<p>\u6ee4\u6ce2\u5668\u7684\u9891\u7387\u54cd\u5e94 \\(H(f)\\) \u63cf\u8ff0\u4e86\u6ee4\u6ce2\u5668\u5bf9\u4e0d\u540c\u9891\u7387\u5206\u91cf\u7684\u5904\u7406\u65b9\u5f0f\uff1a</p> <ul> <li>\u5e45\u9891\u54cd\u5e94 \\(|H(f)|\\)\uff1a\u8868\u793a\u5bf9\u4e0d\u540c\u9891\u7387\u5206\u91cf\u7684\u589e\u76ca\u6216\u8870\u51cf</li> <li>\u76f8\u9891\u54cd\u5e94 \\(\\angle H(f)\\)\uff1a\u8868\u793a\u5bf9\u4e0d\u540c\u9891\u7387\u5206\u91cf\u7684\u76f8\u4f4d\u53d8\u5316</li> </ul>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#312","title":"3.1.2 \u91cd\u8981\u53c2\u6570","text":"<ul> <li>\u622a\u6b62\u9891\u7387\uff08Cutoff Frequency\uff09\uff1a\u6ee4\u6ce2\u5668\u7684\u5e45\u9891\u54cd\u5e94\u4e0b\u964d\u5230 -3dB \u5904\u7684\u9891\u7387</li> <li>\u901a\u5e26\uff08Passband\uff09\uff1a\u4fe1\u53f7\u51e0\u4e4e\u65e0\u8870\u51cf\u901a\u8fc7\u7684\u9891\u7387\u8303\u56f4</li> <li>\u963b\u5e26\uff08Stopband\uff09\uff1a\u4fe1\u53f7\u88ab\u663e\u8457\u8870\u51cf\u7684\u9891\u7387\u8303\u56f4</li> <li>\u8fc7\u6e21\u5e26\uff08Transition Band\uff09\uff1a\u901a\u5e26\u548c\u963b\u5e26\u4e4b\u95f4\u7684\u8fc7\u6e21\u533a\u57df</li> <li>\u963b\u5e26\u8870\u51cf\uff08Stopband Attenuation\uff09\uff1a\u5728\u963b\u5e26\u4e2d\u7684\u6700\u5c0f\u8870\u51cf\u91cf</li> </ul>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#32","title":"3.2 \u6ee4\u6ce2\u5668\u7c7b\u578b","text":""},{"location":"book/chapter1_Introduction/1.3signal_processing/#321-low-pass-filter","title":"3.2.1 \u4f4e\u901a\u6ee4\u6ce2\u5668\uff08Low-Pass Filter\uff09","text":"<ul> <li>\u529f\u80fd\uff1a\u5141\u8bb8\u4f4e\u9891\u4fe1\u53f7\u901a\u8fc7\uff0c\u8870\u51cf\u9ad8\u9891\u4fe1\u53f7</li> <li>\u5e94\u7528\uff1a\u53bb\u9664\u9ad8\u9891\u566a\u58f0\uff0c\u5e73\u6ed1\u4fe1\u53f7</li> <li>\u9891\u7387\u54cd\u5e94\uff1a</li> </ul> <p>$$   H(f) = \\begin{cases}   1, &amp; |f| \\leq f_c \\   0, &amp; |f| &gt; f_c   \\end{cases}   $$</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#322-high-pass-filter","title":"3.2.2 \u9ad8\u901a\u6ee4\u6ce2\u5668\uff08High-Pass Filter\uff09","text":"<ul> <li>\u529f\u80fd\uff1a\u5141\u8bb8\u9ad8\u9891\u4fe1\u53f7\u901a\u8fc7\uff0c\u8870\u51cf\u4f4e\u9891\u4fe1\u53f7</li> <li>\u5e94\u7528\uff1a\u53bb\u9664\u76f4\u6d41\u5206\u91cf\uff0c\u7a81\u51fa\u4fe1\u53f7\u7684\u5feb\u901f\u53d8\u5316</li> <li>\u9891\u7387\u54cd\u5e94\uff1a</li> </ul> <p>$$   H(f) = \\begin{cases}   0, &amp; |f| \\leq f_c \\   1, &amp; |f| &gt; f_c   \\end{cases}   $$</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#323-band-pass-filter","title":"3.2.3 \u5e26\u901a\u6ee4\u6ce2\u5668\uff08Band-Pass Filter\uff09","text":"<ul> <li>\u529f\u80fd\uff1a\u53ea\u5141\u8bb8\u7279\u5b9a\u9891\u7387\u8303\u56f4\u5185\u7684\u4fe1\u53f7\u901a\u8fc7</li> <li>\u5e94\u7528\uff1a\u63d0\u53d6\u7279\u5b9a\u9891\u6bb5\u7684\u4fe1\u53f7\uff0c\u5982\u97f3\u9891\u5904\u7406</li> <li>\u9891\u7387\u54cd\u5e94\uff1a</li> </ul> <p>$$   H(f) = \\begin{cases}   1, &amp; f_1 \\leq |f| \\leq f_2 \\   0, &amp; \\text{\u5176\u4ed6}   \\end{cases}   $$</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#324-band-stop-filter","title":"3.2.4 \u5e26\u963b\u6ee4\u6ce2\u5668\uff08Band-Stop Filter\uff09","text":"<ul> <li>\u529f\u80fd\uff1a\u963b\u6b62\u7279\u5b9a\u9891\u7387\u8303\u56f4\u5185\u7684\u4fe1\u53f7\u901a\u8fc7</li> <li>\u5e94\u7528\uff1a\u53bb\u9664\u7279\u5b9a\u9891\u7387\u7684\u5e72\u6270\uff0c\u5982\u5de5\u9891\u5e72\u6270</li> <li>\u9891\u7387\u54cd\u5e94\uff1a</li> </ul> <p>$$   H(f) = \\begin{cases}   0, &amp; f_1 \\leq |f| \\leq f_2 \\   1, &amp; \\text{\u5176\u4ed6}   \\end{cases}   $$</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#33","title":"3.3 \u6ee4\u6ce2\u5668\u7279\u6027","text":""},{"location":"book/chapter1_Introduction/1.3signal_processing/#331","title":"3.3.1 \u7406\u60f3\u6ee4\u6ce2\u5668\u4e0e\u5b9e\u9645\u6ee4\u6ce2\u5668","text":"<ul> <li>\u7406\u60f3\u6ee4\u6ce2\u5668\uff1a\u9891\u7387\u54cd\u5e94\u5728\u901a\u5e26\u548c\u963b\u5e26\u4e4b\u95f4\u6709\u7a81\u53d8\uff0c\u65e0\u8fc7\u6e21\u5e26</li> <li>\u5b9e\u9645\u6ee4\u6ce2\u5668\uff1a</li> <li>\u5b58\u5728\u8fc7\u6e21\u5e26</li> <li>\u901a\u5e26\u6709\u6ce2\u7eb9</li> <li>\u963b\u5e26\u8870\u51cf\u6709\u9650</li> <li>\u76f8\u4f4d\u54cd\u5e94\u975e\u7ebf\u6027</li> </ul>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#332","title":"3.3.2 \u5e38\u89c1\u6ee4\u6ce2\u5668\u7c7b\u578b","text":"<ol> <li>\u5df4\u7279\u6c83\u65af\u6ee4\u6ce2\u5668\uff1a</li> <li>\u7279\u70b9\uff1a\u901a\u5e26\u6700\u5e73\u5766\uff0c\u76f8\u4f4d\u54cd\u5e94\u8f83\u597d</li> <li> <p>\u7f3a\u70b9\uff1a\u8fc7\u6e21\u5e26\u8f83\u5bbd</p> </li> <li> <p>\u5207\u6bd4\u96ea\u592b\u6ee4\u6ce2\u5668\uff1a</p> </li> <li>\u7279\u70b9\uff1a\u8fc7\u6e21\u5e26\u8f83\u7a84</li> <li> <p>\u7f3a\u70b9\uff1a\u901a\u5e26\u6709\u6ce2\u7eb9</p> </li> <li> <p>\u692d\u5706\u6ee4\u6ce2\u5668\uff1a</p> </li> <li>\u7279\u70b9\uff1a\u6700\u9661\u5ced\u7684\u8fc7\u6e21\u5e26</li> <li>\u7f3a\u70b9\uff1a\u901a\u5e26\u548c\u963b\u5e26\u90fd\u6709\u6ce2\u7eb9</li> </ol>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#34","title":"3.4 \u6ee4\u6ce2\u5668\u7684\u5e94\u7528","text":""},{"location":"book/chapter1_Introduction/1.3signal_processing/#341","title":"3.4.1 \u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u5e94\u7528","text":"<ol> <li>\u566a\u58f0\u53bb\u9664\uff1a\u4f7f\u7528\u4f4e\u901a\u6ee4\u6ce2\u5668\u53bb\u9664\u9ad8\u9891\u566a\u58f0</li> <li>\u4fe1\u53f7\u5206\u79bb\uff1a\u4f7f\u7528\u5e26\u901a\u6ee4\u6ce2\u5668\u63d0\u53d6\u7279\u5b9a\u9891\u6bb5\u7684\u4fe1\u53f7</li> <li>\u5e72\u6270\u6d88\u9664\uff1a\u4f7f\u7528\u5e26\u963b\u6ee4\u6ce2\u5668\u53bb\u9664\u7279\u5b9a\u9891\u7387\u7684\u5e72\u6270</li> <li>\u4fe1\u53f7\u91cd\u5efa\uff1a\u4f7f\u7528\u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\u5668\u8fdb\u884c\u4fe1\u53f7\u91cd\u5efa</li> </ol>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#342","title":"3.4.2 \u5b9e\u9645\u5e94\u7528\u4e3e\u4f8b","text":"<ol> <li>\u97f3\u9891\u5904\u7406\uff1a\u5747\u8861\u5668\u4f7f\u7528\u591a\u4e2a\u5e26\u901a\u6ee4\u6ce2\u5668</li> <li>\u901a\u4fe1\u7cfb\u7edf\uff1a\u4fe1\u9053\u9009\u62e9\u4f7f\u7528\u5e26\u901a\u6ee4\u6ce2\u5668</li> <li>\u751f\u7269\u533b\u5b66\uff1a\u5fc3\u7535\u4fe1\u53f7\u5904\u7406\u4f7f\u7528\u5e26\u963b\u6ee4\u6ce2\u5668\u53bb\u9664\u5de5\u9891\u5e72\u6270</li> </ol>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#4-idea-upsampling","title":"4. idea upsampling","text":"<p>\u5728\u6570\u5b57\u4fe1\u53f7\u5904\u7406\uff08DSP\uff09\u4e2d\uff0c\u7406\u60f3\u5347\u91c7\u6837\u6307\u7684\u662f\u5c06\u4e00\u4e2a\u79bb\u6563\u4fe1\u53f7\u4ece\u539f\u91c7\u6837\u7387 \\(F_s\\) \u65e0\u5931\u771f\u5730\u53d8\u6362\u5230\u66f4\u9ad8\u91c7\u6837\u7387 \\(L \\times F_s\\) \u7684\u7406\u8bba\u8fc7\u7a0b\u3002\u5b83\u901a\u5e38\u5305\u542b\u4e24\u6b65\uff1a\u63d2\u96f6\uff08Zero-stuffing\uff09 \u548c \u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\uff08Ideal LPF\uff09\u3002\u4e0b\u9762\u4ece\u591a\u4e2a\u89d2\u5ea6\u8fdb\u884c\u9610\u8ff0\u3002</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#41","title":"4.1. \u57fa\u672c\u6982\u5ff5","text":"<ol> <li>\u63d2\u96f6\uff08Zero-stuffing\uff09    \u5728\u65f6\u57df\u4e0a\u5c06\u539f\u4fe1\u53f7 \\(x[n]\\) \u7684\u6bcf\u4e2a\u91c7\u6837\u4e4b\u95f4\u63d2\u5165 \\(L-1\\) \u4e2a\u96f6\uff0c\u5f97\u5230\u65b0\u5e8f\u5217 \\(x_{\\uparrow L}[n]\\)\u3002\u5f62\u5f0f\u4e0a\u53ef\u5199\u4e3a\uff1a</li> </ol> <p>$$    x_{\\uparrow L}[m] =    \\begin{cases}    x\\bigl(\\tfrac{m}{L}\\bigr), &amp; \\text{\u5f53 } m \\text{ \u4e3a } L \\text{ \u7684\u6574\u6570\u500d} \\    0, &amp; \\text{\u5426\u5219}    \\end{cases}    $$</p> <ol> <li>\u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\uff08Ideal LPF\uff09    \u5bf9\u63d2\u96f6\u540e\u7684\u4fe1\u53f7\u8fdb\u884c\u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\uff08\u5e26\u5bbd\u4e3a \\(\\pi/L\\)\uff09\uff0c\u5373\u4f7f\u7528\u7406\u60f3\u77e9\u5f62\u9891\u7387\u54cd\u5e94\u6ee4\u6ce2\u5668 \\(H_{\\text{ideal}}(e^{j\\omega})\\)\u3002\u5b83\u5728 \\(|\\omega|\\le \\pi/L\\) \u4e0a\u4e3a 1\uff0c\u5176\u4ed6\u533a\u95f4\u4e3a 0\u3002\u6b64\u8fc7\u7a0b\u53ef\u6ee4\u9664\u63d2\u96f6\u5bfc\u81f4\u7684\u201c\u955c\u50cf\u9891\u8c31\u201d\u6210\u5206\u3002</li> </ol>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#42","title":"4.2. \u9891\u57df\u5206\u6790","text":"<ol> <li> <p>\u539f\u4fe1\u53f7\u7684\u9891\u8c31    \u8bbe\u539f\u4fe1\u53f7 \\(x[n]\\) \u7684\u79bb\u6563\u65f6\u95f4\u5085\u91cc\u53f6\u53d8\u6362\u4e3a \\(X(e^{j\\omega})\\)\uff0c\u4e14\u5b83\u5e26\u9650\u4e8e\u67d0\u4e2a \\(\\omega_0 \\le \\pi\\)\u3002</p> </li> <li> <p>\u63d2\u96f6\u540e\u9891\u8c31\u7684\u590d\u5236    \u63d2\u96f6\u64cd\u4f5c\u4f1a\u5728\u9891\u57df\u4ea7\u751f\u5468\u671f\u6027\u590d\u5236\uff1a\u65b0\u4fe1\u53f7 \\(x_{\\uparrow L}[n]\\) \u7684\u9891\u8c31\u5728 \\([-\\pi,\\pi]\\) \u5185\u51fa\u73b0\u591a\u4e2a\u7b49\u8ddd\u201c\u955c\u50cf\uff08image\uff09\u201d\u526f\u672c\uff0c\u6bcf\u4e2a\u526f\u672c\u5bbd\u5ea6\u4e3a\u539f\u5e26\u5bbd\u7684 \\(1/L\\)\u3002</p> </li> </ol> <p>\u89e3\u91ca    \u5728\u5b9a\u4e49\u4e86 \u63d2\u96f6\u540e\u5e8f\u5217 \\(x_{\\uparrow L}[n]\\) \u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u5199\u51fa\u5b83\u7684\u79bb\u6563\u65f6\u95f4\u5085\u91cc\u53f6\u53d8\u6362 (DTFT) \u5982\u4e0b\uff1a</p> <p>$$    \\begin{aligned}    X_{\\uparrow L}(e^{j\\omega})    &amp;=\\; \\sum_{n=-\\infty}^{\\infty} x_{\\uparrow L}[n]\\;e^{-j\\,\\omega\\,n}    \\,=\\, \\sum_{n=-\\infty}^{\\infty}    \\Bigl[\\underbrace{x_{\\uparrow L}[n]}{\\text{\u4ec5\u5f53 }n=Lm\\text{\u65f6\u975e\u96f6}}\\Bigr] e^{-j\\,\\omega\\,n}.\\    &amp;=\\; \\sum{m=-\\infty}^{\\infty}    \\Bigl[x_{\\uparrow L}\\bigl(L\\,m\\bigr)\\Bigr]\\;    e^{-j\\,\\omega\\,(L\\,m)}.    \\end{aligned}    $$</p> <p>\u7531\u4e8e \\(x_{\\uparrow L}[\\,L\\,m\\,] = x[m]\\)\uff0c\u4e0a\u5f0f\u7ee7\u7eed\u5316\u7b80\u5f97\u5230\uff1a</p> <p>$$    X_{\\uparrow L}(e^{j\\omega})    =\\; \\sum_{m=-\\infty}^{\\infty} x[m]\\;e^{-j\\,(\\omega\\,L)\\,m}.    $$</p> <p>\u5982\u679c\u5c06 \\(\\omega\\,L\\) \u89c6\u4e3a\u65b0\u7684\u9891\u7387\u53d8\u91cf \\(\\Theta\\)\uff0c\u5219\u53ef\u8bc6\u522b\u4e3a\u539f\u4fe1\u53f7 \\(x[n]\\) \u7684 DTFT\uff1a</p> <p>$$    X_{\\uparrow L}(e^{j\\omega})    =\\; X\\bigl(e^{\\,j(\\omega\\,L)}\\bigr).    $$</p> <p>\u5728 \\(\\omega\\) \u7684\u4e00\u4e2a \\(2\\pi\\) \u5468\u671f\u8303\u56f4\u5185\uff0c\\(X_{\\uparrow L}(e^{j\\omega})\\) \u4f1a\u51fa\u73b0\u591a\u6b21\u91cd\u590d\uff08\u955c\u50cf\uff09\u5206\u91cf\u3002</p> <ol> <li>\u7406\u60f3\u6ee4\u6ce2\u5668\u4fdd\u7559\u4e3b\u74e3    \u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\u5668 \\(H_{\\text{ideal}}(e^{j\\omega})\\) \u5728 \\(|\\omega|\\le \\pi/L\\) \u5185\u4e3a 1\uff0c\u8d85\u51fa\u6b64\u8303\u56f4\u4e3a 0\uff0c\u4ec5\u4fdd\u7559\u6700\u4e2d\u5fc3\u7684\u90a3\u6bb5\u539f\u59cb\u9891\u8c31\u526f\u672c\uff0c\u5176\u4f59\u955c\u50cf\u5206\u91cf\u88ab\u6ee4\u9664\u3002</li> </ol>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#43","title":"4.3. \u65f6\u57df\u7ed3\u679c","text":"<p>\u6ee4\u6ce2\u540e\u7684\u8f93\u51fa\u4fe1\u53f7\u53ef\u5199\u4e3a</p> \\[ y[n] = \\bigl[x_{\\uparrow L} * h_{\\text{ideal}}\\bigr](n), \\] <p>\u5176\u4e2d \\(h_{\\text{ideal}}[n]\\) \u4e3a\u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\u5668\u7684\u51b2\u6fc0\u54cd\u5e94\uff08\u79bb\u6563\u7248 \\(\\mathrm{sinc}\\) \u51fd\u6570\uff09\u3002\u82e5\u6ee4\u6ce2\u5668\u53ef\u7406\u60f3\u5b9e\u73b0\uff0c\u5219\u8f93\u51fa \\(y[n]\\) \u4e0e\u539f\u4fe1\u53f7\u5728\u66f4\u9ad8\u91c7\u6837\u7387\u4e0b\u4e00\u4e00\u5bf9\u5e94\uff0c\u8fbe\u5230\u201c\u5b8c\u7f8e\u63d2\u503c\u201d\u7684\u6548\u679c\u3002</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#44","title":"4.4. \u5b9e\u9645\u5b9e\u73b0\u7684\u5dee\u5f02","text":"<ul> <li> <p>\u65e0\u9650\u957f\u6ee4\u6ce2\u5668\u7684\u96be\u9898   \u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\u5668\u5728\u65f6\u57df\u4e2d\u5bf9\u5e94\u65e0\u9650\u957f\u7684 \\(\\mathrm{sinc}\\)\uff0c\u73b0\u5b9e\u4e2d\u96be\u4ee5\u5b9e\u73b0\u3002</p> </li> <li> <p>FIR \u6ee4\u6ce2\u5668\u8fd1\u4f3c   \u5de5\u7a0b\u4e0a\u4f7f\u7528\u6709\u9650\u957f\u5ea6\u7684 FIR\uff08\u6216 IIR\uff09\u6ee4\u6ce2\u5668\u6765\u903c\u8fd1\u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\u5668\uff0c\u9700\u8981\u5728\u8fc7\u6e21\u5e26\u4e0e\u963b\u5e26\u8870\u51cf\u7b49\u65b9\u9762\u505a\u6743\u8861\u3002</p> </li> </ul>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#45","title":"4.5. \u7ed3\u8bba","text":"<ol> <li> <p>\u7406\u60f3\u5347\u91c7\u6837\u7684\u672c\u8d28    \u5148\u8fdb\u884c\u201c\u63d2\u96f6\u201d\uff0c\u518d\u7528\u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\u5668\u6291\u5236\u955c\u50cf\u9891\u8c31\uff0c\u4ec5\u4fdd\u7559\u539f\u59cb\u5e26\u9650\u4fe1\u606f\u3002</p> </li> <li> <p>\u610f\u4e49    \u5728\u66f4\u9ad8\u91c7\u6837\u7387\u4e0b\uff0c\u5b9e\u73b0\u7406\u8bba\u4e0a\u201c\u65e0\u5931\u771f\u201d\u7684\u4fe1\u53f7\u63d2\u503c\u4e0e\u91cd\u91c7\u6837\u3002</p> </li> <li> <p>\u5de5\u7a0b\u6298\u4e2d    \u7531\u4e8e\u771f\u6b63\u7684\u7406\u60f3\u6ee4\u6ce2\u5668\u65e0\u6cd5\u5b9e\u73b0\uff0c\u5b9e\u9645\u53ea\u80fd\u505a\u201c\u5c3d\u53ef\u80fd\u597d\u201d\u7684\u63d2\u503c\u6ee4\u6ce2\uff0c\u5f97\u5230\u8f83\u597d\u7684\u8fd1\u4f3c\u7ed3\u679c\u3002</p> <p>\u4e00\u53e5\u8bdd\u6982\u62ec\uff1a\u7406\u60f3\u5347\u91c7\u6837 = \u201c\u63d2\u96f6\u201d + \u201c\u7406\u60f3\u4f4e\u901a\u201d\uff0c\u4f7f\u5f97\u5728\u66f4\u9ad8\u91c7\u6837\u7387\u4e0b\u4fdd\u6301\u539f\u5e26\u9650\u4fe1\u53f7\u7684\u6240\u6709\u9891\u8c31\u4fe1\u606f\u800c\u4e0d\u5931\u771f\u3002</p> </li> <li> <p>\u201c\u4fe1\u53f7\u4e0d\u53d8\u201d\u4e0e\u201c\u9891\u7387\u538b\u7f29\u201d\u5e76\u4e0d\u77db\u76fe    \u7269\u7406\u4e0a/\u8fde\u7eed\u57df\u770b\uff0c\u201c\u540c\u4e00\u4e2a\u4fe1\u53f7\u201d\u5e76\u6ca1\u6709\u771f\u6b63\u88ab\u7be1\u6539\uff1b\u82e5\u771f\u7684\u589e\u52a0\u91c7\u6837\u7387\uff0c\u4fe1\u53f7\u672c\u8eab\u5728\u6ce2\u5f62\u4e0a\u4e5f\u662f\u5149\u6ed1\u5730\u63d2\u503c\u3002    \u79bb\u6563-\u6570\u5b66\u4e0a\u770b\uff0c\u201c\u4e0a\u91c7\u6837\u201d\u5c31\u610f\u5473\u7740\u63d2\u5165\u96f6(\u6216\u901a\u8fc7\u63d2\u503c\u6ee4\u6ce2\u5668\u8ba9\u65b0\u6837\u672c\u5e76\u975e\u5168\u662f\u96f6)\uff0c\u5bfc\u81f4\u201c\u5e8f\u5217\u7d22\u5f15\u201d\u52a0\u5bc6\uff1b\u7531\u6b64\u5728\u79bb\u6563\u9891\u7387\u5750\u6807(    \\(\\omega\\)\u4e0a\u4f1a\u770b\u5230\u9891\u8c31\u538b\u7f29\u548c\u955c\u50cf\u7684\u73b0\u8c61\u3002    \u5173\u952e\u662f\uff1a\u201c\u79bb\u6563\u89d2\u9891\u7387 \\(\\omega\\)\u4e0d\u662f\u76f4\u63a5\u7b49\u540c\u4e8e\u7269\u7406\u9891\u7387(Hz)\uff0c\u800c\u662f\u76f8\u5bf9\u2018\u6bcf\u4e2a\u91c7\u6837\u70b9\u2019\u7684\u5f52\u4e00\u5316\u9891\u7387\u3002</p> </li> </ol> <p>\u5f53\u4f60\u6539\u53d8\u201c\u6bcf\u4e2a\u91c7\u6837\u70b9\u95f4\u7684\u5b9e\u9645\u65f6\u95f4\u95f4\u9694\u201d\uff0c\u5c31\u4f1a\u5728 \\(\\omega\\)\u8f74\u4e0a\u770b\u5230\u5e26\u5bbd\u53d1\u751f\u53d8\u5316(\u53d8\u5f97\u66f4\u5c0f\u6216\u66f4\u5927)\u3002\u4f46\u662f\u201c\u7269\u7406\u4fe1\u53f7\u201d\u5e76\u672a\u5fc5\u4ea7\u751f\u5931\u771f\uff0c\u53ea\u662f\u5b83\u7684\u79bb\u6563\u7d22\u5f15\u523b\u5ea6\u53d1\u751f\u4e86\u4f38\u7f29\u3002    \u5728\u201c\u7406\u60f3\u4e0a\u91c7\u6837\u201d\u4e2d\uff0c\u4e3a\u4ec0\u4e48\u8981\u914d\u5408\u6ee4\u6ce2\u5668\uff1f    \u4ec5\u63d2\u96f6\u4e0d\u591f\uff1a\u4f1a\u5728\u9891\u57df\u4ea7\u751f\u955c\u50cf\u5206\u91cf\uff08image/alias\uff09\u3002    \u7406\u60f3\u4e0a\u91c7\u6837 = \u201c\u63d2\u96f6 + \u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2(\u5e26\u5bbd \\(\\pi/L\\)\u201d\uff0c\u4ece\u800c\u6ee4\u9664\u9664\u4e3b\u5e26\u5916\u7684\u90a3\u4e9b\u955c\u50cf\u9891\u8c31\u3002    \u8fd9\u6837\u5f97\u5230\u7684\u8f93\u51fa\u5728\u65b0\u7684\u91c7\u6837\u7387\u4e0b\u4ecd\u7136\u53ea\u4fdd\u7559\u539f\u5148\u7684\u5e26\u5bbd\u4fe1\u606f\uff0c\u4f46\u5bf9\u5e94\u4e8e\u79bb\u6563\u89d2\u9891\u7387\\(\\omega\\)\uff0c\u5b83\u5c31\u538b\u7f29\u5230\u4e86 \\(- \\pi/L, + \\pi/L\\) \u4ee5\u5185\u3002    \u6b64\u65f6\u4f60\u4f1a\u770b\u5230\uff0c\u9891\u8c31\u4e5f\u4e0d\u4f1a\u201c\u53d8\u5927\u201d\uff0c\u800c\u662f\u4fdd\u6301\u539f\u5e26\u5bbd\u201d \u2014\u2014 \u53ea\u662f\u91cf\u7eb2\u4ece\u201c\u9891\u7387/\u91c7\u6837\u70b9\u201d\u53d8\u6210\u65b0\u7684\u66f4\u5bc6\u96c6\u91c7\u6837\u4e0b\u7684\u201c\u9891\u7387/\u91c7\u6837\u70b9\u201d\u3002\u8fd9\u5c31\u662f\u201c\u5728\u79bb\u6563\u57df\u51fa\u73b0\u4e86\u9891\u7387\u538b\u7f29\u201d\u3002</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#5-downsampling","title":"5. \u4e0b\u91c7\u6837\uff08Downsampling\uff09","text":"<p>\u4e0b\u91c7\u6837\uff08\u4e5f\u79f0\u201c\u964d\u91c7\u6837\u201d\u6216\u201c\u62bd\u53d6\u201d\uff09\u6307\u5728\u6570\u5b57\u4fe1\u53f7\u5904\u7406\u4e2d\uff0c\u5c06\u79bb\u6563\u4fe1\u53f7\u7684\u91c7\u6837\u7387\u4ece\u539f\u5148\u7684 \\(F_s\\) \u964d\u4f4e\u5230 \\(\\tfrac{F_s}{M}\\)\uff08\\(M\\) \u4e3a\u964d\u91c7\u6837\u56e0\u5b50\uff09\uff0c\u4ece\u800c\u51cf\u5c11\u91c7\u6837\u70b9\u6570\u3002\u8bbe\u539f\u5e8f\u5217\u4e3a \\(x[n]\\)\uff0c\u5219\u4e0b\u91c7\u6837\u540e\u7684\u5e8f\u5217\u4e3a</p> \\[ y[m] \\;=\\; x[m\\,M]. \\]"},{"location":"book/chapter1_Introduction/1.3signal_processing/#51","title":"5.1. \u65f6\u57df\u63cf\u8ff0","text":"<ul> <li>\u64cd\u4f5c\uff1a\u6bcf\u95f4\u9694 \\(M\\) \u4e2a\u7d22\u5f15\u53d6\u4e00\u4e2a\u6837\u672c\uff0c\u5176\u4f59\u6837\u672c\u88ab\u820d\u5f03\u3002</li> <li>\u793a\u4f8b\uff1a\u5982\u679c \\(M = 2\\)\uff0c\u5219</li> </ul> <p>$$     y[0] = x[0], \\quad y[1] = x[2], \\quad y[2] = x[4], \\dots   $$</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#52-aliasing","title":"5.2. \u9891\u57df\u5f71\u54cd\uff1a\u6df7\u53e0(Aliasing)","text":"<p>\u5728\u79bb\u6563\u65f6\u95f4\u5085\u91cc\u53f6\u53d8\u6362(DTFT)\u89c6\u89d2\uff0c\u4e0b\u91c7\u6837\u4f1a\u4ea7\u751f\u6df7\u53e0(aliasing)\u3002 \u5e38\u89c1\u7ed3\u8bba\u662f\uff1a</p> \\[ Y(e^{j\\Omega}) \\;=\\; \\frac{1}{M}\\, \\sum_{k=0}^{M-1} X\\!\\Bigl(e^{\\,j\\,\\tfrac{\\Omega + 2\\pi k}{M}}\\Bigr), \\] <p>\u5176\u4e2d \\(X(e^{j\\omega})\\) \u548c \\(Y(e^{j\\Omega})\\) \u5206\u522b\u4e3a \\(x[n]\\) \u548c \\(y[m]\\) \u7684 DTFT\u3002\u8be5\u516c\u5f0f\u8868\u660e\uff0c\u65b0\u5e8f\u5217\u7684\u9891\u8c31\u662f\u539f\u5e8f\u5217\u9891\u8c31\u7684\u591a\u4e2a\u5e73\u79fb/\u6298\u53e0\u526f\u672c\u76f8\u52a0\u5728\u4e00\u8d77\uff0c\u8fd9\u5c31\u662f\u6df7\u53e0\u7684\u672c\u8d28\u3002</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#53-anti-aliasing-filter","title":"5.3. \u9700\u8981\u5148\u4f4e\u901a\u6ee4\u6ce2 (\u6297\u6df7\u53e0\u6ee4\u6ce2\uff0cAnti-Aliasing Filter)","text":"<ol> <li>\u76ee\u7684</li> <li>\u82e5\u4fe1\u53f7\u539f\u672c\u5305\u542b\u9ad8\u4e8e\u65b0\u7684\u5948\u594e\u65af\u7279\u9891\u7387 \\(\\tfrac{\\pi}{M}\\)\uff08\u76f8\u5bf9\u4e8e\u65e7\u91c7\u6837\u7387\u8ba1\uff09\u7684\u6210\u5206\uff0c\u5219\u5728\u4e0b\u91c7\u6837\u540e\u4f1a\u51fa\u73b0\u91cd\u53e0\u6df7\u53e0\u3002</li> <li>\u89e3\u51b3\u65b9\u6848</li> <li>\u5728\u4e0b\u91c7\u6837\u524d\u7528\u4f4e\u901a\u6ee4\u6ce2\u5668\u6ee4\u9664\u8d85\u51fa \\(\\tfrac{\\pi}{M}\\) \u7684\u9ad8\u9891\u5206\u91cf\uff0c\u907f\u514d\u8fd9\u4e9b\u9891\u7387\u5206\u91cf\u5728\u65b0\u91c7\u6837\u7387\u4e0b\u4ea7\u751f\u6df7\u53e0\u5931\u771f\u3002</li> <li>\u5b9e\u73b0</li> <li>\u5b9e\u9645\u4e2d\u5e38\u7528\u6709\u9650\u51b2\u6fc0\u54cd\u5e94(FIR)\u4f4e\u901a\u6ee4\u6ce2\u5668\u6216 IIR \u6ee4\u6ce2\u5668\u8fdb\u884c\u5904\u7406\uff0c\u7136\u540e\u518d\u6bcf\u9694 \\(M\\) \u70b9\u53d6\u6837\u3002</li> </ol>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#54","title":"5.4. \u5e94\u7528\u573a\u666f","text":"<ul> <li>\u591a\u901f\u7387\u6570\u5b57\u4fe1\u53f7\u5904\u7406\uff1a\u5728\u6ee4\u6ce2\u5668\u7ec4\u3001\u5b50\u5e26\u7f16\u7801\u7b49\u4e2d\uff0c\u901a\u8fc7\u964d\u91c7\u6837\u51cf\u5c11\u6570\u636e\u901f\u7387\uff0c\u964d\u4f4e\u8fd0\u7b97\u91cf\u3002</li> <li>\u97f3\u9891/\u56fe\u50cf\u8f6c\u7801\uff1a\u4ece\u66f4\u9ad8\u91c7\u6837\u7387\u8f6c\u5230\u66f4\u4f4e\u91c7\u6837\u7387\uff1b\u4f8b\u5982\u97f3\u9891\u4ece 48 kHz \u964d\u5230 16 kHz\uff0c\u5fc5\u987b\u5148\u6ee4\u9664 8 kHz \u4ee5\u4e0a\u7684\u9891\u7387\u6210\u5206\u3002</li> <li>\u5c0f\u6ce2\u53d8\u6362\uff1a\u5c06\u4fe1\u53f7\u5206\u89e3\u4e3a\u4e0d\u540c\u5206\u8fa8\u7387\u5b50\u5e26\u540e\uff0c\u5bf9\u6bcf\u4e2a\u5b50\u5e26\u505a\u62bd\u53d6\u4ee5\u51cf\u5c11\u5197\u4f59\u3002</li> </ul>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#55","title":"5.5 \u79bb\u6563\u8868\u8ff0\u548c\u8fde\u7eed\u8868\u8ff0\u7684\u5173\u7cfb","text":"<p>\u4e0b\u91c7\u6837\u9891\u8c31\u5206\u6790\uff1a\u79bb\u6563\u8868\u8ff0 vs. \u8fde\u7eed\u8868\u8ff0</p> <p>\u5728\u6570\u5b57\u4fe1\u53f7\u5904\u7406\u91cc\uff0c\u5e38\u89c1\u201c\u4e0b\u91c7\u6837(Downsampling)\u201d\u64cd\u4f5c\u662f\u5c06\u91c7\u6837\u7387\u964d\u4f4e\u5230\u539f\u6765\u7684 <code>1/M</code>\u3002\u4e0b\u91c7\u6837\u524d\u540e\uff0c\u4fe1\u53f7\u5728\u79bb\u6563\u57df\u4e0e\u8fde\u7eed\u57df\u4f1a\u6709\u4e0d\u540c\u7684\u9891\u8c31\u8868\u8ff0\uff0c\u4f46\u5b83\u4eec\u672c\u8d28\u4e0a\u8868\u8fbe\u7684\u662f\u540c\u4e00\u4e2a\u6df7\u53e0(aliasing)\u539f\u7406\u3002\u4ee5\u4e0b\u5206\u522b\u505a\u7b80\u8981\u8bf4\u660e\u3002</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#551-m","title":"5.5.1. \u79bb\u6563\u8868\u8ff0\uff1a\u5e26\u5bbd &lt; \u03c0/M","text":"<ul> <li>\u79bb\u6563\u65f6\u95f4\u4e2d\uff0c\u4e0b\u91c7\u6837\u56e0\u5b50\u4e3a <code>M</code> \u65f6\uff0c\u82e5\u8981\u907f\u514d\u6df7\u53e0\uff0c\u9700\u8981\u4fe1\u53f7\u5728 \u79bb\u6563\u89d2\u9891\u7387 <code>\u03c9</code> \u4e0a\u6ee1\u8db3</li> </ul> <p>$$   \\text{(\u5e26\u5bbd)} \\;&lt;\\; \\frac{\\pi}{M} \\quad (\\text{rad/sample}).   $$</p> <ul> <li>\u5728\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362(DTFT)\u4e2d\uff0c<code>\u03c9</code> \u53d6\u503c\u8303\u56f4\u901a\u5e38\u662f <code>[-\u03c0, \u03c0]</code>\uff0c\u5bf9\u5e94\u4e00\u4e2a\u5b8c\u6574\u7684 2\u03c0-\u5468\u671f\u3002\u82e5\u539f\u4fe1\u53f7\u5e26\u5bbd\u8d85\u8fc7 <code>\u03c0/M</code>\uff0c\u4e0b\u91c7\u6837\u540e\u4f1a\u51fa\u73b0\u591a\u4e2a\u5e73\u79fb\u526f\u672c\u4e92\u76f8\u91cd\u53e0\uff0c\u5f15\u53d1\u6df7\u53e0\u3002</li> </ul>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#552-f2m","title":"5.5.2. \u8fde\u7eed\u8868\u8ff0\uff1a\u5e26\u5bbd &lt; F/(2M)","text":"<ul> <li>\u8fde\u7eed\u65f6\u95f4\u4e2d\uff0c\u5982\u679c\u539f\u5148\u91c7\u6837\u7387\u662f <code>F</code> (Hz)\uff0c\u4e0b\u91c7\u6837\u56e0\u5b50 <code>M</code> \u4f1a\u4f7f \u65b0\u91c7\u6837\u7387 \u53d8\u4e3a <code>F' = F/M</code>\u3002</li> <li>\u6839\u636e\u91c7\u6837\u5b9a\u7406(Nyquist-Shannon)\uff0c\u8981\u907f\u514d\u6df7\u53e0\uff0c\u9700\u8981\u5e26\u5bbd <code>B</code> \u5c0f\u4e8e\u65b0\u5948\u594e\u65af\u7279\u9891\u7387 <code>(F/M)/2</code>\uff0c\u5373</li> </ul> <p>$$   B \\;&lt;\\; \\frac{F}{2\\,M} \\quad (\\text{Hz}).   $$</p> <ul> <li>\u5f53\u5e26\u5bbd\u8d85\u8fc7 <code>F/(2M)</code> \u65f6\uff0c\u5728\u65b0\u7684\u91c7\u6837\u7387\u4e0b\u5fc5\u7136\u4f1a\u4ea7\u751f aliasing\uff08\u522b\u540d\u5931\u771f\uff09\u3002</li> </ul>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#553","title":"5.5.3. \u4e8c\u8005\u4e00\u81f4\u6027","text":"<ul> <li>\u79bb\u6563\u57df\u91cc \u201c\u5e26\u5bbd &lt; \u03c0/M (rad/sample)\u201d</li> <li>\u8fde\u7eed\u57df\u91cc \u201c\u5e26\u5bbd &lt; F/(2M) (Hz)\u201d</li> </ul> <p>\u4e8c\u8005\u770b\u4f3c\u4e0d\u540c\uff0c\u4f46\u672c\u8d28\u662f\u540c\u4e00\u4e2a\u6761\u4ef6\uff0c\u56e0\u4e3a\u79bb\u6563\u89d2\u9891\u7387 <code>\u03c9 = \u03c0</code> \u76f8\u5f53\u4e8e\u7269\u7406\u9891\u7387 <code>F/2</code> (Nyquist \u9891\u7387)\u3002\u6362\u8a00\u4e4b\uff1a</p> \\[ \\omega = \\pi \\;\\;\\longleftrightarrow\\;\\; f = \\frac{F}{2}. \\] <p>\u56e0\u6b64\uff0c</p> \\[ \\omega &lt; \\frac{\\pi}{M} \\quad\\longleftrightarrow\\quad f &lt; \\frac{F}{2\\,M}. \\] <p>\u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48\u5728\u79bb\u6563\u516c\u5f0f\u548c\u8fde\u7eed\u516c\u5f0f\u4e2d\uff0c\u9608\u503c\u770b\u8d77\u6765\u4e0d\u540c\u4f46\u542b\u4e49\u76f8\u540c\u3002</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#554","title":"5.5.4. \u4e0b\u91c7\u6837\u5728\u8fde\u7eed\u57df\u7684\u9891\u8c31\u516c\u5f0f","text":"<p>\u5728\u8fde\u7eed\u57df\uff0c\u7528\u51b2\u6fc0\u5217(Dirac comb) \u8868\u793a\u91c7\u6837\uff0c\u5468\u671f\u4ece <code>T = 1/F</code> \u589e\u5927\u5230 <code>M\u00b7T</code> \u65f6\uff0c\u9891\u57df\u8868\u73b0\u4e3a \u539f\u4fe1\u53f7\u8c31\u7684\u591a\u4e2a\u5e73\u79fb\u5e76\u76f8\u52a0\uff1a</p> \\[ Y_{\\mathrm{sample}}(\\omega) \\;=\\; \\frac{1}{M\\,T} \\sum_{k=-\\infty}^{\\infty} X\\!\\Bigl(\\omega - 2\\pi\\,\\tfrac{k}{M\\,T}\\Bigr). \\] <ul> <li>\u8fd9\u91cc\uff0c<code>X(\u03c9)</code> \u662f <code>x(t)</code> \u7684\u5085\u91cc\u53f6\u53d8\u6362\uff0c<code>Y_sample(\u03c9)</code> \u5219\u662f\u4e0b\u91c7\u6837\u540e\u4fe1\u53f7\u7684\u9891\u8c31\u3002</li> <li>\u82e5\u5e26\u5bbd\u8d85\u8fc7 <code>1/(2M\u00b7T) = F/(2M)</code>\uff0c\u8fd9\u4e9b\u526f\u672c\u5728\u9891\u57df\u91cc\u53d1\u751f\u91cd\u53e0\uff0c\u5bfc\u81f4\u6df7\u53e0\u3002</li> </ul>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#5","title":"5. \u5c0f\u7ed3","text":"<ol> <li>\u79bb\u6563 vs. \u8fde\u7eed</li> <li>\u79bb\u6563\u65f6\u95f4\u5085\u91cc\u53f6\u53d8\u6362(DTFT)\u4ee5 <code>\u03c9</code> (rad/sample) \u4e3a\u8f74\uff0c\u5b8c\u6574\u5468\u671f\u662f <code>2\u03c0</code>\uff1b</li> <li> <p>\u8fde\u7eed\u5085\u91cc\u53f6\u53d8\u6362(CFT)\u4ee5 <code>\u03c9</code> (rad/s) \u6216 <code>f</code> (Hz) \u4e3a\u8f74\uff0c\u53ef\u80fd\u6709\u65e0\u7a77\u5bbd\u5ea6\uff0c\u4f46\u91c7\u6837\u540e\u51fa\u73b0\u9891\u8c31\u5468\u671f\u590d\u5236\u3002</p> </li> <li> <p>\u907f\u514d\u6df7\u53e0\u7684\u5e26\u5bbd\u9650\u5236</p> </li> <li>\u79bb\u6563\u57df\uff1a<code>\u03c9_band &lt; \u03c0/M</code>\uff1b</li> <li> <p>\u8fde\u7eed\u57df\uff1a<code>B &lt; F/(2M)</code>\u3002    \u8fd9\u4e8c\u8005\u76f8\u4e92\u5bf9\u5e94\uff1a\u201c<code>\u03c9 = \u03c0</code>\u201d \u2194 \u201c<code>f = F/2</code>\u201d\uff0c\u5b9e\u5219\u540c\u4e00\u4e2aNyquist\u9650\u5236\u3002</p> </li> <li> <p>\u4e0b\u91c7\u6837\u524d\u5e38\u7528\u4f4e\u901a\u6ee4\u6ce2</p> </li> <li>\u5b9e\u9645\u7cfb\u7edf\u4e2d\uff0c\u4e3a\u4e86\u6ee1\u8db3\u4e0a\u8ff0\u5e26\u5bbd\u9650\u5236\uff0c\u5f80\u5f80\u5728\u4e0b\u91c7\u6837\u4e4b\u524d\u5148\u505a\u4f4e\u901a\u6ee4\u6ce2 (\u6297\u6df7\u53e0\u6ee4\u6ce2\u5668)\uff0c\u5c06\u4fe1\u53f7\u5e26\u5bbd\u7ea6\u675f\u5230 <code>(F/2M)</code> \u4ee5\u5185\u3002</li> </ol> <p>\u4e00\u53e5\u8bdd\u603b\u7ed3\uff1a\u5728\u79bb\u6563\u8868\u8ff0\u4e0e\u8fde\u7eed\u8868\u8ff0\u4e0b\uff0c\u907f\u514d\u6df7\u53e0\u7684\u6761\u4ef6\u4e4b\u6240\u4ee5\u770b\u8d77\u6765\u6709\u4e24\u4e2a\u4e0d\u540c\u7684\u9608\u503c(\u201c&lt; \u03c0/M\u201d vs. \u201c&lt; F/(2M)\u201d)\uff0c\u53ea\u662f\u9891\u7387\u5355\u4f4d\u53d8\u5316\u7684\u7f18\u6545\uff0c\u672c\u8d28\u4e0a\u662f\u540c\u4e00\u4e2a\u9891\u7387\u5e26\u5bbd\u9650\u5236\u3002</p>"},{"location":"book/chapter1_Introduction/1.3signal_processing/#references","title":"references","text":"<ol> <li>\u9891\u8c31\u6df7\u53e0</li> <li>\u89c6\u9891\u6559\u7a0b\u91c7\u6837</li> <li>\u4e8c\u7ef4\u63d2\u503c\u5b9a\u7406</li> </ol>"},{"location":"book/chapter1_Introduction/1.4statistics/","title":"\u7edf\u8ba1\u5b66\u7b80\u4ecb","text":""},{"location":"book/chapter1_Introduction/1.4statistics/#1-sampling","title":"1.  \u62bd\u6837 sampling","text":""},{"location":"book/chapter1_Introduction/1.4statistics/#11-sampling-in-statistics","title":"1.1 \u62bd\u6837 (Sampling in Statistics)\u6570\u5b66\u548c\u7edf\u8ba1\u5b9a\u4e49","text":"<ul> <li>\u62bd\u6837\u662f\u4ece\u4e00\u4e2a\u603b\u4f53 (Population) \u4e2d\u9009\u53d6\u4e00\u4e2a\u5b50\u96c6 (Sample) \u7684\u8fc7\u7a0b\u3002</li> <li>\u5047\u8bbe\u603b\u4f53\u662f\u96c6\u5408 \\(\\mathcal{P}\\)\uff0c\u5176\u5927\u5c0f\u4e3a \\(N\\)\uff0c\u62bd\u6837\u9009\u53d6\u7684\u6837\u672c\u96c6\u5408\u8bb0\u4e3a \\(\\mathcal{S} \\subset \\mathcal{P}\\)\uff0c\u6837\u672c\u5927\u5c0f\u4e3a \\(n\\)\uff0c\u5176\u4e2d \\(n \\ll N\\)\u3002 \u6838\u5fc3\u76ee\u6807</li> <li> <p>\u7edf\u8ba1\u63a8\u65ad \uff1a\u901a\u8fc7\u7814\u7a76\u6837\u672c \\(\\mathcal{S}\\)\uff0c\u4f30\u8ba1\u603b\u4f53 \\(\\mathcal{P}\\) \u7684\u53c2\u6570\uff08\u5982\u5747\u503c\u3001\u65b9\u5dee\u3001\u5206\u5e03\u7b49\uff09\u3002</p> </li> <li> <p>\u6838\u5fc3\u95ee\u9898 \uff1a\u5982\u4f55\u4fdd\u8bc1\u6837\u672c\u5177\u6709\u4ee3\u8868\u6027\uff0c\u4ee5\u51cf\u5c11\u603b\u4f53\u4e0e\u6837\u672c\u4e4b\u95f4\u7684\u8bef\u5dee\u3002 \u6570\u5b66\u65b9\u6cd5</p> </li> <li>\u968f\u673a\u62bd\u6837\uff1a\u6837\u672c\u662f\u4ece\u603b\u4f53\u4e2d\u72ec\u7acb\u4e14\u7b49\u6982\u7387\u5730\u62bd\u53d6\u7684\uff0c\u6bcf\u4e2a\u5143\u7d20\u88ab\u62bd\u53d6\u7684\u6982\u7387\u4e3a \\(\\frac{1}{N}\\)\u3002</li> <li>\u5206\u5c42\u62bd\u6837\uff1a\u603b\u4f53\u5206\u4e3a\u82e5\u5e72\u5b50\u96c6\uff0c\u5206\u522b\u4ece\u6bcf\u4e2a\u5b50\u96c6\u4e2d\u6309\u6bd4\u4f8b\u62bd\u53d6\u6837\u672c\u3002</li> <li>\u6837\u672c\u5747\u503c\uff1a$ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$</li> </ul> <p>\u6837\u672c\u5747\u503c\u662f\u603b\u4f53\u5747\u503c\u7684\u65e0\u504f\u4f30\u8ba1\u3002</p>"},{"location":"book/chapter1_Introduction/1.4statistics/#12","title":"1.2 \u7edf\u8ba1\u4f9d\u636e","text":"<ul> <li> <p>\u5927\u6570\u5b9a\u5f8b \uff1a\u6837\u672c\u91cf \\(n\\) \u8d8a\u5927\uff0c\u6837\u672c\u5747\u503c \\(\\bar{x}\\) \u8d8b\u8fd1\u4e8e\u603b\u4f53\u5747\u503c \\(\\mu\\)\u3002</p> </li> <li> <p>\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406 \uff1a\u5f53\u6837\u672c\u91cf\u8db3\u591f\u5927\u65f6\uff0c\u6837\u672c\u5747\u503c \\(\\bar{x}\\) \u7684\u5206\u5e03\u63a5\u8fd1\u6b63\u6001\u5206\u5e03\u3002</p> <p>\u5e94\u7528\u793a\u4f8b</p> <ol> <li> <p>\u8c03\u67e5\u7814\u7a76 \uff1a\u4ece\u57ce\u5e02\u4e2d\u7684\u5c45\u6c11\u4e2d\u968f\u673a\u62bd\u53d6 1000 \u4eba\uff0c\u8c03\u67e5\u4ed6\u4eec\u7684\u6536\u5165\u5206\u5e03\u3002</p> </li> <li> <p>\u673a\u5668\u5b66\u4e60 \uff1a\u4ece\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e2d\u9009\u53d6\u5b50\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\u3002</p> </li> </ol> </li> </ul>"},{"location":"book/chapter1_Introduction/1.4statistics/#13-signal-sampling-in-mathematics","title":"1.3 \u548c \u91c7\u6837 (Signal Sampling in Mathematics) \u7684\u533a\u522b","text":"<ul> <li>\u91c7\u6837\u662f\u4ece\u4e00\u4e2a\u8fde\u7eed\u4fe1\u53f7 \\(f(t)\\) \u4e2d\uff0c\u6309\u7167\u4e00\u5b9a\u89c4\u5219\u5728\u79bb\u6563\u70b9 \\(\\{t_n\\}\\) \u4e0a\u53d6\u503c\uff0c\u5f62\u6210\u79bb\u6563\u4fe1\u53f7 \\(f[n]\\) \u7684\u8fc7\u7a0b\u3002</li> <li>\u79bb\u6563\u5316\u7684\u4fe1\u53f7\u8868\u793a\u4e3a\uff1a</li> </ul> \\[  f[n] = f(t_n), \\quad t_n = nT, \\quad n \\in \\mathbb{Z} \\] <p>\u5176\u4e2d \\(T\\) \u662f\u91c7\u6837\u95f4\u9694\uff0c\\(1/T\\) \u662f\u91c7\u6837\u9891\u7387\u3002</p> <p>\u6838\u5fc3\u533a\u522b</p> <ul> <li>\u62bd\u6837 \uff1a\u7814\u7a76\u79bb\u6563\u603b\u4f53\u7684\u5b50\u96c6\uff0c\u63a8\u65ad\u603b\u4f53\u7279\u6027\u3002</li> <li>\u91c7\u6837 \uff1a\u5c06\u8fde\u7eed\u4fe1\u53f7\u79bb\u6563\u5316\uff0c\u4e3a\u6570\u5b57\u5904\u7406\u6216\u5206\u6790\u505a\u51c6\u5907\u3002 \u64cd\u4f5c\u5bf9\u8c61</li> <li>\u62bd\u6837 \uff1a\u79bb\u6563\u603b\u4f53\uff08\u6709\u9650\u96c6\u5408\uff09\u3002</li> <li>\u91c7\u6837 \uff1a\u8fde\u7eed\u4fe1\u53f7\uff08\u65f6\u95f4\u57df\u6216\u7a7a\u95f4\u57df\u51fd\u6570\uff09\u3002 \u7406\u8bba\u4f9d\u636e</li> </ul> \u7279\u6027 \u62bd\u6837 \u91c7\u6837 \u57fa\u7840\u7406\u8bba \u5927\u6570\u5b9a\u5f8b\u3001\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406 \u5948\u594e\u65af\u7279\u91c7\u6837\u5b9a\u7406\u3001\u63d2\u503c\u7406\u8bba \u6837\u672c\u4ee3\u8868\u6027 \u968f\u673a\u6027\u548c\u8986\u76d6\u6027\u662f\u5173\u952e \u91c7\u6837\u9891\u7387\u51b3\u5b9a\u662f\u5426\u80fd\u591f\u8fd8\u539f\u4fe1\u53f7 \u6570\u5b66\u516c\u5f0f \u7279\u6027 \u62bd\u6837\u516c\u5f0f --- --- --- \u5747\u503c\u4f30\u8ba1 \\(\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i\\) \\(f[n]=f(tn),t_n = nT\\) \u8bef\u5dee\u8303\u56f4 \u6807\u51c6\u8bef\u5dee\uff1a\\(text{SE} = \\frac{\\sigma}{\\sqrt{n}}\\)\u200b \u91c7\u6837\u8bef\u5dee\uff1a\u7531\u6df7\u53e0\u9891\u7387\u5f15\u8d77"},{"location":"book/chapter1_Introduction/1.4statistics/#2-lln","title":"2. \u5927\u6570\u5b9a\u5f8b (LLN)","text":"<p>\u5728\u5927\u6570\u5b9a\u5f8b\u4e2d\uff0c\u6837\u672c\u5e73\u5747\u503c \u7684\u5b9a\u4e49\u4e3a\uff1a$ \\bar{X}n = \\frac{1}{n} \\sum{i=1}^n X_i$, \u5176\u4e2d \\(X_1, X_2, \\dots, X_n\\) \u662f\u72ec\u7acb\u540c\u5206\u5e03\u7684\u968f\u673a\u53d8\u91cf\uff0c\u5177\u6709\u671f\u671b\u503c \\(\\mu = \\mathbb{E}[X_i]\\) \u548c\u6709\u9650\u65b9\u5dee \\(\\sigma^2 = \\mathrm{Var}(X_i)\\)\u3002</p> <p>\u5927\u6570\u5b9a\u5f8b\u7684\u7ed3\u8bba\uff1a</p> \\[  \\bar{X}_n \\xrightarrow{\\text{a.s.}} \\mu, \\quad \\text{\u6216\u8005} \\quad \\mathbb{P}(|\\bar{X}_n - \\mu| &gt; \\epsilon) \\to 0 \\; (\\forall \\epsilon &gt; 0, \\; n \\to \\infty). \\] <p>\u5927\u6570\u5b9a\u5f8b\u7684\u5173\u952e\u70b9\uff1a</p> <ul> <li>\u5b83\u7814\u7a76\u7684\u662f \\(\\bar{X}_n\\) \u7684\u957f\u671f\u884c\u4e3a\uff08\u968f\u7740 \\(n \\to \\infty\\)\uff09\u3002</li> <li>\u5b83\u4e0d\u6d89\u53ca\u6837\u672c\u5e73\u5747\u503c\u7684\u6982\u7387\u5206\u5e03\u6216\u6ce2\u52a8\u6027\uff0c\u53ea\u5f3a\u8c03\\(\\bar{X}_n\\)\u4f1a\u9010\u6e10\u63a5\u8fd1 \\(\\mu\\)\u3002</li> </ul>"},{"location":"book/chapter1_Introduction/1.4statistics/#3-clt","title":"3 \u4e2d\u592e\u6781\u9650\u5b9a\u7406 (CLT)","text":"<p>\u5728\u4e2d\u592e\u6781\u9650\u5b9a\u7406\u4e2d\uff0c\u8003\u8651\u7684\u4e5f\u662f\u6837\u672c\u5e73\u5747\u503c \uff1a\\(\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\).</p> <p>\u4f46\u8fd9\u91cc\u8fdb\u4e00\u6b65\u7814\u7a76\u4e86\\(\\bar{X}_n\\) \u7684\u6ce2\u52a8\u6027\u6216\u6982\u7387\u5206\u5e03\u7684\u884c\u4e3a\u3002</p> <p>\u6807\u51c6\u5316\u5f62\u5f0f\uff1a</p> <p>\u6211\u4eec\u6784\u9020\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u968f\u673a\u53d8\u91cf \\(Z_n\\)\uff0c\u7528\u6765\u8861\u91cf \\(\\bar{X}_n\\) \u504f\u79bb\u671f\u671b\u503c\u7684\u7a0b\u5ea6\uff1a</p> \\[Z_n = \\frac{\\sqrt{n}(\\bar{X}_n - \\mu)}{\\sigma}.\\] <p>CLT \u7684\u7ed3\u8bba\uff1a \u5f53 \\(n \\to \\infty\\)\uff0c\u6807\u51c6\u5316\u7684\u53d8\u91cf \\(Z_n\\) \u7684\u5206\u5e03\u6536\u655b\u4e8e\u6807\u51c6\u6b63\u6001\u5206\u5e03\uff1a\\(Z_n \\xrightarrow{d} N(0, 1)\\)\u3002</p> <p>\u8fd9\u8bf4\u660e\uff1a</p> <ul> <li> <p>\u6837\u672c\u5e73\u5747\u503c \u5728 \\(n\\) \u8db3\u591f\u5927\u65f6\u7684\u5206\u5e03\u4f1a\u8d8b\u4e8e\u6b63\u6001\u5206\u5e03\uff0c\u4e14\u4e2d\u5fc3\u662f \\(\\mu\\)\uff0c\u6807\u51c6\u5dee\u662f \\(\\frac{\\sigma}{\\sqrt{n}}\\)\u3002</p> </li> <li> <p>\u6837\u672c\u5e73\u5747\u503c \\(\\bar{X}_n\\) \u7684\u5206\u5e03\u8fd1\u4f3c\u4e3a\uff1a</p> </li> </ul> \\[  \\bar{X}_n \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right). \\]"},{"location":"book/chapter1_Introduction/1.4statistics/#4","title":"4. \u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u548c\u5927\u6570\u5b9a\u5f8b\u4e24\u4e2a\u5e73\u5747\u503c\u7684\u6838\u5fc3\u5dee\u5f02","text":"<p>\u5dee\u5f02\u7684\u6838\u5fc3\uff1a</p> <ul> <li>LLN \u7684\u5e73\u5747\u503c \uff1a\u5173\u6ce8 \\(\\bar{X}_n\\) \u662f\u5426\u5728 \\(n \\to \\infty\\) \u65f6\u6536\u655b\u5230\u4e00\u4e2a\u56fa\u5b9a\u503c \\(\\mu\\)\u3002</li> <li>CLT \u7684\u5e73\u5747\u503c \uff1a\u5173\u6ce8 \\(\\bar{X}_n\\) \u5728\u6709\u9650 $n \u65f6\u7684\u6982\u7387\u5206\u5e03\u884c\u4e3a\uff08\u6ce2\u52a8\u6027\uff09\u3002</li> </ul> <p>\u7528\u6570\u5b66\u516c\u5f0f\u8868\u793a\uff1a</p> <ol> <li>LLN  \u63cf\u8ff0\uff1a</li> </ol> \\[  \\bar{X}_n \\xrightarrow{\\text{a.s.}} \\mu \\quad \\text{\uff08\u6536\u655b\u5230\u671f\u671b\u503c \\(\\mu\\)\uff09}. \\] <ol> <li>CLT  \u63cf\u8ff0\uff1a</li> </ol> \\[  \\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2) \\quad \\text{\uff08\u63cf\u8ff0\u6807\u51c6\u5316\u504f\u5dee\u7684\u5206\u5e03\u884c\u4e3a\uff09}. \\] <p>\u516c\u5f0f\u5dee\u5f02\u603b\u7ed3\uff1a</p> <ul> <li>\u5728 LLN \u4e2d\uff0c\u5f3a\u8c03\u7684\u662f\uff1a</li> </ul> \\[  \\bar{X}_n \\approx \\mu \\quad (\\text{\u5f53 } n \\to \\infty). \\] <p>\u5373\uff0c\u968f\u7740 \\(n\\) \u589e\u52a0\uff0c\u6837\u672c\u5e73\u5747\u503c\\(\\bar{X}_n\\) \u8d8a\u6765\u8d8a\u63a5\u8fd1\u671f\u671b\u503c \\(\\mu\\)\u3002</p> <ul> <li>\u5728 CLT \u4e2d\uff0c\u5f3a\u8c03\u7684\u662f\uff1a</li> </ul> \\[  \\bar{X}_n \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right), \\] <p>\u5373\uff0c\u6837\u672c\u5e73\u5747\u503c\u5728\u6709\u9650 \\(n\\) \u65f6\u670d\u4ece\u6b63\u6001\u5206\u5e03\uff0c\u65b9\u5dee \\(\\sigma^2 / n\\) \u4f1a\u968f\u7740 \\(n\\) \u589e\u5927\u9010\u6e10\u51cf\u5c0f\uff0c\u4f46\u5b58\u5728\u6ce2\u52a8\u6027\u3002</p> <p>\u76f4\u89c2\u89e3\u91ca\u4e24\u8005\u5dee\u5f02\uff1a</p> <ul> <li>LLN \uff1a\u544a\u8bc9\u6211\u4eec\uff0c\u5f53 \\(n\\) \u8d8a\u6765\u8d8a\u5927\u65f6\uff0c\\(\\bar{X}_n\\) \u6700\u7ec8\u4f1a\u65e0\u9650\u63a5\u8fd1\u4e8e\u671f\u671b\u503c \\(\\mu\\)\uff08\u503c\u8d8a\u6765\u8d8a\u7a33\u5b9a\uff09\u3002</li> <li>CLT \uff1a\u544a\u8bc9\u6211\u4eec\uff0c\u5bf9\u4e8e\u6709\u9650\u7684 \\(n\\)\uff0c\\(\\bar{X}_n\\) \u4ecd\u7136\u6709\u6ce2\u52a8\uff0c\u5e76\u4e14\u8fd9\u79cd\u6ce2\u52a8\u9075\u5faa\u6b63\u6001\u5206\u5e03\uff0c\u4e14\u6ce2\u52a8\u8303\u56f4\u968f \\(n\\) \u589e\u5927\u800c\u51cf\u5c0f\uff08\\(\\sim \\frac{1}{\\sqrt{n}}\\)\uff09\u3002 \u4f8b\u5b50</li> </ul> <p>\u5047\u5982\u4f60\u6d4b\u91cf\u67d0\u673a\u5668\u96f6\u4ef6\u7684\u957f\u5ea6\uff0c\u5355\u4e2a\u96f6\u4ef6\u7684\u957f\u5ea6\u662f\u968f\u673a\u53d8\u91cf \\(X\\):</p> <ul> <li>\u5927\u6570\u5b9a\u5f8b \uff1a\u5982\u679c\u4f60\u6d4b\u91cf \\(n\\) \u4e2a\u96f6\u4ef6\uff0c\u8ba1\u7b97\u5b83\u4eec\u7684\u5e73\u5747\u957f\u5ea6 \\(\\bar{X}_n\\)\uff0c\u5f53 \\(n\\) \u8db3\u591f\u5927\u65f6\uff0c\u8fd9\u4e2a\u5e73\u5747\u503c\u4f1a\u65e0\u9650\u63a5\u8fd1\u4e8e\u96f6\u4ef6\u7684\u771f\u5b9e\u5e73\u5747\u957f\u5ea6\uff08\u671f\u671b\u503c \\(\\mu\\)\uff09\u3002</li> <li>\u4e2d\u592e\u6781\u9650\u5b9a\u7406 \uff1a\u5982\u679c \\(n\\) \u4e0d\u662f\u65e0\u9650\u5927\uff08\u6bd4\u5982 \\(n = 100\\))),\u90a3\u4e48\u5e73\u5747\u957f\u5ea6 \\(\\bar{X}_n\\) \u4f1a\u5728\u67d0\u4e2a\u8303\u56f4\u5185\u6ce2\u52a8\uff0c\u4e14\u8fd9\u79cd\u6ce2\u52a8\u7684\u6982\u7387\u5206\u5e03\u662f\u6b63\u6001\u5206\u5e03\u3002</li> </ul> <p>\u603b\u7ed3\uff1aLLN \u548c CLT \u7684\u5e73\u5747\u503c\u5dee\u5f02</p> <ol> <li> <p>LLN  \u7814\u7a76\u7684\u662f \\(\\bar{X}_n\\) \u662f\u5426\u7a33\u5b9a\u5e76\u6700\u7ec8\u6536\u655b\u5230 \\(\\mu\\)\uff0c\u5f3a\u8c03\u957f\u671f\u6536\u655b\u6027\u3002</p> </li> <li> <p>CLT  \u7814\u7a76\u7684\u662f \\(\\bar{X}_n\\) \u7684\u5206\u5e03\u5982\u4f55\u6f14\u53d8\u4e3a\u6b63\u6001\u5206\u5e03\uff0c\u5f3a\u8c03\u5206\u5e03\u5f62\u72b6\u548c\u6ce2\u52a8\u6027\u3002</p> </li> </ol> <p>\u7528\u66f4\u76f4\u89c2\u7684\u6bd4\u55bb\uff1a</p> <ul> <li> <p>LLN  \u5c31\u50cf\u4f60\u6d4b\u91cf\u5f88\u591a\u6b21\uff0c\u9010\u6e10\u903c\u8fd1\u201c\u771f\u5b9e\u503c\u201d\u3002</p> </li> <li> <p>CLT  \u544a\u8bc9\u4f60\u5728\u6709\u9650\u6b21\u6d4b\u91cf\u65f6\uff0c\u8fd9\u4e2a\u201c\u903c\u8fd1\u503c\u201d\u6709\u591a\u5927\u7684\u6ce2\u52a8\u8303\u56f4\uff0c\u4ee5\u53ca\u6ce2\u52a8\u7684\u6982\u7387\u5206\u5e03\u662f\u4ec0\u4e48\u6837\u7684\u3002</p> </li> </ul>"},{"location":"book/chapter1_Introduction/1.4statistics/#5-change-of-variable","title":"5. change of variable","text":"<p>\u968f\u673a\u53d8\u91cf\u5206\u5e03\u7684 Change of Variable \uff08\u53d8\u91cf\u53d8\u6362\uff09\u662f\u6982\u7387\u8bba\u4e2d\u4e00\u4e2a\u91cd\u8981\u7684\u516c\u5f0f\uff0c\u7528\u4e8e\u63cf\u8ff0\u5f53\u968f\u673a\u53d8\u91cf\u901a\u8fc7\u67d0\u79cd\u51fd\u6570\u8fdb\u884c\u53d8\u6362\u65f6\uff0c\u5176\u6982\u7387\u5bc6\u5ea6\u5982\u4f55\u53d8\u5316\u3002</p> <p>\u516c\u5f0f\u63a8\u5bfc \u5047\u8bbe\u6211\u4eec\u6709\u4e00\u4e2a\u968f\u673a\u53d8\u91cf \\(z\\)\uff0c\u5176\u6982\u7387\u5bc6\u5ea6\u4e3a \\(p_Z(z)\\)\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e00\u4e2a\u53ef\u9006\u53d8\u6362 \\(z = f(x)\\)\uff0c\u5176\u9006\u53d8\u6362\u4e3a \\(x = f^{-1}(z)\\)\u3002\u76ee\u6807\u662f\u6c42\u53d8\u6362\u540e\u968f\u673a\u53d8\u91cf \\(x\\) \u7684\u6982\u7387\u5bc6\u5ea6 \\(p_X(x)\\)\u3002</p> <p>Change of Variable \u5b9a\u7406 \u5f53 \\(z = f(x)\\) \u4e14\u53d8\u6362 \\(f(x)\\) \u53ef\u9006\u65f6\uff0c\u53d8\u6362\u540e\u7684\u6982\u7387\u5bc6\u5ea6 \\(p_X(x)\\) \u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u516c\u5f0f\u8ba1\u7b97\uff1a$  p_X(x) = p_Z(f(x)) \\cdot \\left| \\det \\left( \\frac{\\partial f(x)}{\\partial x} \\right) \\right| $</p> <ul> <li> <p>\\(p_Z(f(x))\\)\uff1a\u662f\u539f\u59cb\u53d8\u91cf \\(z\\) \u7684\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u503c\u3002</p> </li> <li> <p>\\(\\frac{\\partial f(x)}{\\partial x}\\)\uff1a\u662f\u53d8\u6362 \\(f(x)\\) \u7684\u96c5\u53ef\u6bd4\u77e9\u9635\uff08Jacobian Matrix\uff09\u3002</p> </li> <li> <p>\\(\\det \\left( \\frac{\\partial f(x)}{\\partial x} \\right)\\)\uff1a\u662f\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u884c\u5217\u5f0f\uff0c\u8868\u793a\u53d8\u6362\u5bf9\u6982\u7387\u5bc6\u5ea6\u7684\u7f29\u653e\u7a0b\u5ea6\u3002</p> </li> <li> <p>\\(\\left| \\cdot \\right|\\)\uff1a\u662f\u884c\u5217\u5f0f\u7684\u7edd\u5bf9\u503c\uff0c\u56e0\u4e3a\u6982\u7387\u5bc6\u5ea6\u9700\u8981\u975e\u8d1f\u3002</p> </li> </ul> <p>\u76f4\u89c2\u7406\u89e3</p> <ol> <li> <p>\u53d8\u6362\u5e26\u6765\u7684\u5bc6\u5ea6\u53d8\u5316 \uff1a \u5982\u679c\u53d8\u6362 \\(f(x)\\) \u538b\u7f29\u4e86\u67d0\u4e00\u533a\u57df\u7684\u4f53\u79ef\uff08\u5373\u884c\u5217\u5f0f\u5c0f\u4e8e 1\uff09\uff0c\u5219\u8be5\u533a\u57df\u7684\u6982\u7387\u5bc6\u5ea6\u4f1a\u589e\u52a0\uff1b\u53cd\u4e4b\uff0c\u5982\u679c\u4f53\u79ef\u6269\u5927\uff0c\u5219\u6982\u7387\u5bc6\u5ea6\u4f1a\u51cf\u5c0f\u3002</p> </li> <li> <p>\u6982\u7387\u4fdd\u6301\u4e0d\u53d8 \uff1a \u968f\u673a\u53d8\u91cf\u7684\u603b\u6982\u7387\u59cb\u7ec8\u4e3a 1\uff0c\u56e0\u6b64\u53d8\u6362\u53ea\u6539\u53d8\u5bc6\u5ea6\u7684\u5206\u5e03\u5f62\u5f0f\uff0c\u800c\u4e0d\u6539\u53d8\u603b\u4f53\u79ef\u3002</p> </li> </ol> <p>\u4e00\u7ef4\u60c5\u51b5 \u5728\u4e00\u7ef4\u60c5\u51b5\u4e0b\uff08\u5373 \\(z\\) \u548c \\(x\\) \u662f\u6807\u91cf\uff09\uff0c\u96c5\u53ef\u6bd4\u77e9\u9635 \\(\\frac{\\partial f(x)}{\\partial x}\\) \u9000\u5316\u4e3a\u5bfc\u6570 \\(f'(x)\\)\uff0c\u516c\u5f0f\u7b80\u5316\u4e3a\uff1a$  p_X(x) = p_Z(f(x)) \\cdot \\left| f'(x) \\right| $</p> <p>\u4f8b\u5982\uff1a</p> <ul> <li>\u5982\u679c \\(z \\sim \\mathcal{N}(0, 1)\\) \u4e14 \\(z = f(x) = 2x + 3\\)\uff0c\u5219\uff1a $  p_X(x) = \\mathcal{N}(f(x) \\mid 0, 1) \\cdot \\left| f'(x) \\right| = \\mathcal{N}(2x + 3 \\mid 0, 1) \\cdot 2 $</li> </ul> <p>\u591a\u7ef4\u60c5\u51b5 \u5728\u591a\u7ef4\u60c5\u51b5\u4e0b\uff08\u5373 \\(z\\) \u548c \\(x\\) \u662f\u5411\u91cf\uff09\uff0c\u9700\u8981\u4f7f\u7528\u96c5\u53ef\u6bd4\u77e9\u9635\u3002\u96c5\u53ef\u6bd4\u77e9\u9635\u5b9a\u4e49\u4e3a\uff1a$  J_f(x) = \\frac{\\partial f(x)}{\\partial x} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots &amp; \\ddots &amp; \\vdots \\ \\frac{\\partial f_m}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} $</p> <p>\u591a\u7ef4\u60c5\u51b5\u4e0b\uff0c\u516c\u5f0f\u4e3a\uff1a\\(p_X(x) = p_Z(f(x)) \\cdot \\left| \\det J_f(x) \\right|\\)</p>"},{"location":"book/chapter1_Introduction/1.4statistics/#6","title":"6. \u968f\u673a\u53d8\u91cf\u548c\u5206\u5e03","text":"<p>\u8868\u793a\u4e00\u4e2a\u968f\u673a\u53d8\u91cf\u7684\u5206\u5e03\u901a\u5e38\u4f7f\u7528\u4ee5\u4e0b\u51e0\u79cd\u65b9\u5f0f\uff1a</p>"},{"location":"book/chapter1_Introduction/1.4statistics/#61-pdf-probability-density-function","title":"6.1 \u6982\u7387\u5bc6\u5ea6\u51fd\u6570(PDF, Probability Density Function)","text":"<ul> <li> <p>\u5982\u679c\u968f\u673a\u53d8\u91cf\u662f\u8fde\u7eed\u578b\u968f\u673a\u53d8\u91cf(\u5982\u6b63\u6001\u5206\u5e03),\u5176\u5206\u5e03\u53ef\u4ee5\u7528\u6982\u7387\u5bc6\u5ea6\u51fd\u6570 \\(f_X(x)\\) \u8868\u793a\u3002</p> </li> <li> <p>\u4f8b\u5982\uff0c\u5bf9\u4e8e\u6b63\u6001\u5206\u5e03:</p> </li> </ul> \\[  X \\sim \\mathcal{N}(\\mu, \\sigma^2) \\] <p>\u6216</p> \\[  f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\] <p>\u8868\u793a \\(X\\) \u670d\u4ece\u5747\u503c\u4e3a \\(\\mu\\)\u3001\u65b9\u5dee\u4e3a \\(\\sigma^2\\) \u7684\u6b63\u6001\u5206\u5e03\u3002</p>"},{"location":"book/chapter1_Introduction/1.4statistics/#62-pmf-probability-mass-function","title":"6.2. \u6982\u7387\u8d28\u91cf\u51fd\u6570(PMF, Probability Mass Function)","text":"<ul> <li> <p>\u5982\u679c\u968f\u673a\u53d8\u91cf\u662f\u79bb\u6563\u578b\u968f\u673a\u53d8\u91cf\uff0c\u5176\u5206\u5e03\u7528\u6982\u7387\u8d28\u91cf\u51fd\u6570 \\(\\(P(X = x)\\)\\) \u8868\u793a\u3002</p> </li> <li> <p>\u4f8b\u5982\uff0c\u5bf9\u4e8e\u4e00\u4e2a\u79bb\u6563\u578b\u53d8\u91cf \\(X\\) \u7684\u4f2f\u52aa\u5229\u5206\u5e03\uff1a</p> </li> </ul> \\[  P(X = x) = p^x (1-p)^{1-x}, \\quad x \\in \\{0, 1\\} \\]"},{"location":"book/chapter1_Introduction/1.4statistics/#63-cdf-cumulative-distribution-function","title":"6.3. **\u7d2f\u79ef\u5206\u5e03\u51fd\u6570(CDF, Cumulative Distribution Function)","text":"<ul> <li>\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\u8868\u793a\u968f\u673a\u53d8\u91cf\u5c0f\u4e8e\u7b49\u4e8e\u67d0\u4e2a\u503c\u7684\u6982\u7387\uff1a</li> </ul> \\[  F_X(x) = P(X \\leq x) \\] <ul> <li>\u5b83\u9002\u7528\u4e8e\u8fde\u7eed\u578b\u548c\u79bb\u6563\u578b\u968f\u673a\u53d8\u91cf\u3002</li> </ul>"},{"location":"book/chapter1_Introduction/1.4statistics/#64","title":"6.4. \u5206\u5e03\u7b26\u53f7\u8868\u793a","text":"<ul> <li>\u5728\u7b80\u5316\u573a\u666f\u4e2d\uff0c\u53ef\u4ee5\u76f4\u63a5\u7528\u7b26\u53f7 \\(\\sim\\) \u8868\u793a\u968f\u673a\u53d8\u91cf\u7684\u5206\u5e03\u7c7b\u578b\u3002\u4f8b\u5982\uff1a</li> <li>\\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\)\uff1a\u6b63\u6001\u5206\u5e03\u3002</li> <li>\\(X \\sim \\text{Bernoulli}(p)\\)\uff1a\u4f2f\u52aa\u5229\u5206\u5e03\u3002</li> <li>\\(X \\sim \\text{Uniform}(a, b)\\)\uff1a\u5747\u5300\u5206\u5e03\u3002</li> </ul> <p>\u6216\u8005\u66f4\u8fdb\u4e00\u6b65, \u53ef\u4ee5\u8868\u793a\u4e3a\uff1a\\(\\mathcal{N}(\\mathbf{X}; \\mu, \\Sigma)\\), \\(\\mathcal{N}\\) \u662f \"Normal\"\uff08\u6b63\u6001\u5206\u5e03\uff09\u7684\u9996\u5b57\u6bcd\uff0c\u7528\u4e8e\u8868\u660e\u5206\u5e03\u7684\u7c7b\u578b\u3002 \u8fd9\u662f\u4e00\u79cd\u8bb0\u53f7\u60ef\u4f8b\uff0c\u7528\u6765\u7b80\u6d01\u5730\u8868\u793a\u591a\u5143\u6b63\u6001\u5206\u5e03:</p> <ul> <li> <p>\u7b2c\u4e00\u90e8\u5206 \\(\\mathbf{X}\\) \u662f\u968f\u673a\u53d8\u91cf\u3002</p> </li> <li> <p>\u7b2c\u4e8c\u90e8\u5206 \\(0\\) \u548c \\(\\mathbf{I}\\) \u5206\u522b\u8868\u793a\u5747\u503c\u5411\u91cf\u548c\u534f\u65b9\u5dee\u77e9\u9635\u3002</p> </li> </ul>"},{"location":"book/chapter1_Introduction/1.4statistics/#65","title":"6.5. \u8054\u5408\u5206\u5e03\u4e0e\u6761\u4ef6\u5206\u5e03","text":"<ul> <li>\u5982\u679c\u6709\u591a\u4e2a\u968f\u673a\u53d8\u91cf\uff0c\u53ef\u4ee5\u7528\u8054\u5408\u5206\u5e03\u6216\u6761\u4ef6\u5206\u5e03\u6765\u63cf\u8ff0\u5b83\u4eec\u7684\u5173\u7cfb\uff1a</li> <li> <p>\u8054\u5408\u5206\u5e03\uff1a\\(P(X, Y)\\) \u6216 \\(f_{X, Y}(x, y)\\)\u3002</p> </li> <li> <p>\u6761\u4ef6\u5206\u5e03\uff1a\\(P(X | Y)\\) \u8868\u793a\u5728 \\(Y\\) \u5df2\u77e5\u7684\u6761\u4ef6\u4e0b\uff0c\\(X\\) \u7684\u5206\u5e03\u3002</p> </li> </ul>"},{"location":"book/chapter1_Introduction/1.4statistics/#7-markov","title":"7. Markov\u94fe","text":"<p>Markov\u94fe \uff08Markov Chain\uff09\u662f\u4e00\u4e2a\u6ee1\u8db3 Markov\u6027 \uff08\u6216\u79f0\u4e3a\u201c\u65e0\u540e\u6548\u6027\u201d\uff09\u7684\u968f\u673a\u8fc7\u7a0b\u3002\u7b80\u5355\u6765\u8bf4\uff0cMarkov\u6027\u6307\u7684\u662f\uff1a\u672a\u6765\u7684\u72b6\u6001\u53ea\u4e0e\u5f53\u524d\u7684\u72b6\u6001\u6709\u5173\uff0c\u800c\u4e0e\u8fc7\u53bb\u7684\u72b6\u6001\u65e0\u5173\u3002</p>"},{"location":"book/chapter1_Introduction/1.4statistics/#71-markov","title":"7.1 Markov\u94fe\u7684\u6570\u5b66\u5b9a\u4e49","text":"<p>\u8bbe \\(X_t\\) \u8868\u793a\u4e00\u4e2a\u968f\u673a\u8fc7\u7a0b\u5728\u65f6\u95f4 \\(t\\) \u65f6\u7684\u72b6\u6001\u3002\u5982\u679c\u5bf9\u4e8e\u4efb\u610f\u7684 \\(t\\) \u548c\u72b6\u6001\u5e8f\u5217 \\(x_0, x_1, \\ldots, x_t\\)\uff0c\u6ee1\u8db3\u6761\u4ef6\uff1a</p> \\[  P(X_{t+1} = x_{t+1} \\mid X_t = x_t, X_{t-1} = x_{t-1}, \\ldots, X_0 = x_0) = P(X_{t+1} = x_{t+1} \\mid X_t = x_t) \\] <p>\u5373\uff0c\u7ed9\u5b9a\u5f53\u524d\u72b6\u6001 \\(X_t\\)\uff0c\u672a\u6765\u72b6\u6001 \\(X_{t+1}\\) \u7684\u5206\u5e03\u4e0e\u8fc7\u53bb\u7684\u72b6\u6001 \\(X_{t-1}, X_{t-2}, \\ldots\\) \u65e0\u5173\uff0c\u90a3\u4e48\u8fd9\u4e2a\u968f\u673a\u8fc7\u7a0b\u5c31\u662f\u4e00\u4e2aMarkov\u8fc7\u7a0b \u3002\u5982\u679c\u72b6\u6001\u7a7a\u95f4\u662f\u79bb\u6563\u7684\uff0c\u5e76\u4e14\u65f6\u95f4\u4e5f\u662f\u79bb\u6563\u7684\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u79f0\u4e3a\u79bb\u6563\u65f6\u95f4Markov\u94fe \uff08Discrete-Time Markov Chain, DTMC\uff09\u3002</p>"},{"location":"book/chapter1_Introduction/1.4statistics/#72-markov","title":"7.2 Markov\u94fe\u7684\u8981\u7d20","text":"<ol> <li> <p>\u72b6\u6001\u7a7a\u95f4  (\\(S\\)) Markov\u94fe\u53ef\u4ee5\u5904\u4e8e\u7684\u6240\u6709\u53ef\u80fd\u72b6\u6001\u7684\u96c6\u5408\u3002\u72b6\u6001\u7a7a\u95f4\u53ef\u4ee5\u662f\u6709\u9650\u7684\uff0c\u4e5f\u53ef\u4ee5\u662f\u65e0\u9650\u7684\u3002</p> </li> <li> <p>\u8f6c\u79fb\u6982\u7387 \u4ece\u5f53\u524d\u72b6\u6001\u8f6c\u79fb\u5230\u4e0b\u4e00\u4e2a\u72b6\u6001\u7684\u6982\u7387\u3002\u901a\u5e38\u7528 *\u8f6c\u79fb\u6982\u7387* \u4ece\u5f53\u524d\u72b6\u6001\u8f6c\u79fb\u5230\u4e0b\u4e00\u4e2a\u72b6\u6001\u7684\u6982\u7387\u3002\u901a\u5e38\u7528 \u8f6c\u79fb\u6982\u7387\u77e9\u9635 \\(P\\)  \u8868\u793a\uff1a$  P_{ij} = P(X_{t+1} = j \\mid X_t = i) $ \u8fd9\u91cc \\(P_{ij}\\) \u8868\u793a\u4ece\u72b6\u6001 \\(i\\) \u8f6c\u79fb\u5230\u72b6\u6001 \\(j\\) \u7684\u6982\u7387\u3002</p> </li> <li> <p>\u521d\u59cb\u72b6\u6001\u5206\u5e03 \u6307\u968f\u673a\u8fc7\u7a0b\u5728 \\(t = 0\\) \u65f6\u5404\u4e2a\u72b6\u6001\u7684\u6982\u7387\u5206\u5e03\uff0c\u8bb0\u4e3a \\(\\pi_0\\)\u3002</p> </li> </ol>"},{"location":"book/chapter1_Introduction/1.4statistics/#73-markov","title":"7.3 Markov\u94fe\u7684\u6027\u8d28","text":"<ol> <li> <p>\u65e0\u540e\u6548\u6027\uff08Markov\u6027\uff09 \uff1a\u672a\u6765\u7684\u72b6\u6001\u53ea\u4f9d\u8d56\u4e8e\u5f53\u524d\u72b6\u6001\uff0c\u4e0e\u8fc7\u53bb\u72b6\u6001\u65e0\u5173\u3002</p> </li> <li> <p>\u72b6\u6001\u8f6c\u79fb\u7684\u6982\u7387\u5206\u5e03\u56fa\u5b9a \uff1a\u8f6c\u79fb\u6982\u7387\u77e9\u9635 \\(P\\) \u901a\u5e38\u662f\u65f6\u95f4\u4e0d\u53d8\u7684\u3002</p> </li> <li> <p>n\u6b65\u8f6c\u79fb\u6982\u7387 \uff1a\u7ecf\u8fc7 \\(n\\) \u6b65\u4ece\u72b6\u6001 \\(i\\) \u8f6c\u79fb\u5230\u72b6\u6001 \\(j\\) \u7684\u6982\u7387\u53ef\u7528\u77e9\u9635\u5e42\u8868\u793a\uff1a</p> </li> </ol> \\[  P^{(n)}_{ij} = P(X_{t+n} = j \\mid X_t = i) \\]"},{"location":"book/chapter1_Introduction/1.4statistics/#73","title":"7.3 \u5178\u578b\u5e94\u7528","text":"<ul> <li> <p>\u81ea\u7136\u8bed\u8a00\u5904\u7406 \uff1a\u5982\u8bed\u8a00\u6a21\u578b\u4e2d\u7528Markov\u94fe\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u3002</p> </li> <li> <p>\u91d1\u878d\u5efa\u6a21 \uff1a\u80a1\u7968\u4ef7\u683c\u6216\u7ecf\u6d4e\u6307\u6807\u7684\u52a8\u6001\u5efa\u6a21\u3002</p> </li> <li> <p>\u7f51\u7edc\u5206\u6790 \uff1a\u5982PageRank\u7b97\u6cd5\uff0c\u7528Markov\u94fe\u5206\u6790\u7f51\u9875\u8df3\u8f6c\u3002</p> </li> <li> <p>\u751f\u7269\u4fe1\u606f\u5b66 \uff1a\u57fa\u56e0\u5e8f\u5217\u5206\u6790\u3002</p> </li> </ul>"},{"location":"book/chapter1_Introduction/1.4statistics/#74-markov","title":"7.4 \u4e0d\u540c\u5f62\u5f0f\u7684Markov\u94fe","text":"<p>Markov\u94fe\u4e2d\u7684\u201c\u72b6\u6001\u8f6c\u79fb\u7684\u6982\u7387\u5206\u5e03\u56fa\u5b9a \u201d\u5e76\u4e0d\u662f\u4e00\u4e2a\u5fc5\u987b\u6761\u4ef6\uff0c\u800c\u662f\u9488\u5bf9\u9f50\u6b21Markov\u94fe \uff08Homogeneous Markov Chain\uff09\u7684\u5047\u8bbe\u3002\u5b9e\u9645\u4e0a\uff0cMarkov\u94fe\u4e5f\u6709\u4ee5\u4e0b\u60c5\u51b5\uff1a</p> <p>1. \u9f50\u6b21Markov\u94fe\uff08Homogeneous Markov Chain\uff09 \u8fd9\u662f\u6700\u5e38\u89c1\u7684\u60c5\u51b5\uff0c\u8f6c\u79fb\u6982\u7387\u77e9\u9635 \\(P\\) \u4e0d\u968f\u65f6\u95f4\u53d8\u5316 \uff0c\u5373\u5bf9\u4e8e\u4efb\u610f\u65f6\u95f4 \\(t\\)\uff1a$  P(X_{t+1} = j \\mid X_t = i) = P_{ij}, \\quad \\text{(\u6052\u5b9a)}\u3002 $</p> <p>\u8fd9\u79cd\u5047\u8bbe\u7b80\u5316\u4e86\u5206\u6790\uff0c\u4e14\u5728\u5f88\u591a\u5b9e\u9645\u95ee\u9898\u4e2d\u662f\u5408\u7406\u7684\uff0c\u6bd4\u5982PageRank\u7b97\u6cd5\u3001\u5929\u6c14\u6a21\u578b\u7b49\u3002</p> <p>2. \u975e\u9f50\u6b21Markov\u94fe\uff08Non-Homogeneous Markov Chain\uff09 \u5728\u4e00\u4e9b\u573a\u666f\u4e0b\uff0c\u8f6c\u79fb\u6982\u7387\u53ef\u80fd\u968f\u65f6\u95f4\u53d8\u5316 \uff0c\u5373\uff1a$  P(X_{t+1} = j \\mid X_t = i) = P_{ij}^{(t)}\uff0c \\quad \\text{(\u968f \\( t \\) \u53d8\u5316)}\u3002 $ \u8fd9\u610f\u5473\u7740\u72b6\u6001\u4ece \\(i\\) \u8f6c\u79fb\u5230 \\(j\\) \u7684\u6982\u7387\u4f9d\u8d56\u4e8e\u65f6\u95f4 \\(t\\)\u3002\u8fd9\u79cd\u60c5\u51b5\u901a\u5e38\u7528\u4e8e\u4ee5\u4e0b\u60c5\u666f\uff1a</p> <ul> <li> <p>\u5b63\u8282\u6027\u53d8\u5316 \uff1a\u6bd4\u5982\u5929\u6c14\u6a21\u578b\uff0c\u590f\u5b63\u7684\u5929\u6c14\u8f6c\u79fb\u6982\u7387\u4e0d\u540c\u4e8e\u51ac\u5b63\u3002</p> </li> <li> <p>\u52a8\u6001\u7cfb\u7edf \uff1a\u6bd4\u5982\u91d1\u878d\u5e02\u573a\uff0c\u5176\u72b6\u6001\u8f6c\u79fb\u53ef\u80fd\u968f\u65f6\u95f4\u6216\u5916\u90e8\u4e8b\u4ef6\u8c03\u6574\u3002</p> </li> </ul> <p>3. \u5176\u4ed6\u6269\u5c55\u5f62\u5f0f \u9664\u4e86\u4ee5\u4e0a\u4e24\u79cd\uff0c\u8fd8\u53ef\u4ee5\u6709\u66f4\u590d\u6742\u7684Markov\u94fe\u5f62\u5f0f\uff1a</p> <ol> <li> <p>\u534aMarkov\u8fc7\u7a0b\uff08Semi-Markov Process\uff09 \uff1a\u5141\u8bb8\u5728\u6bcf\u4e2a\u72b6\u6001\u505c\u7559\u7684\u65f6\u95f4\u4e0d\u662f\u56fa\u5b9a\u7684\uff0c\u4e5f\u4e0d\u662f\u6307\u6570\u5206\u5e03\u3002</p> </li> <li> <p>\u9ad8\u9636Markov\u94fe\uff08Higher-Order Markov Chain\uff09 \uff1a\u672a\u6765\u72b6\u6001\u4f9d\u8d56\u4e8e\u591a\u4e2a\u5386\u53f2\u72b6\u6001\uff08\u4e0d\u4ec5\u662f\u4e0a\u4e00\u4e2a\u72b6\u6001\uff09\uff0c\u5982\uff1a</p> </li> </ol> \\[  P(X_{t+1} \\mid X_t, X_{t-1}, \\ldots, X_{t-k})\u3002 \\]"},{"location":"book/chapter1_Introduction/1.4statistics/#8-monte-carlo-estimate","title":"8 Monte Carlo estimate","text":"<p>Monte Carlo estimate  \u662f\u4e00\u79cd\u5229\u7528\u968f\u673a\u62bd\u6837\u65b9\u6cd5\u6765\u8fd1\u4f3c\u8ba1\u7b97\u590d\u6742\u95ee\u9898\u7684\u6570\u503c\u89e3\u7684\u7edf\u8ba1\u6280\u672f\u3002\u5b83\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6570\u5b66\u3001\u7269\u7406\u3001\u91d1\u878d\u3001\u5de5\u7a0b\u7b49\u9886\u57df\uff0c\u5c24\u5176\u662f\u90a3\u4e9b\u89e3\u6790\u89e3\u96be\u4ee5\u83b7\u5f97\u7684\u95ee\u9898\u3002</p>"},{"location":"book/chapter1_Introduction/1.4statistics/#81","title":"8.1 \u57fa\u672c\u539f\u7406","text":"<p>Monte Carlo \u65b9\u6cd5\u57fa\u4e8e\u4ee5\u4e0b\u6838\u5fc3\u601d\u60f3\uff1a</p> <ol> <li> <p>\u4f7f\u7528\u968f\u673a\u6570\u751f\u6210\u4e00\u7cfb\u5217\u6837\u672c\u3002</p> </li> <li> <p>\u6839\u636e\u8fd9\u4e9b\u6837\u672c\uff0c\u8ba1\u7b97\u4e00\u4e2a\u51fd\u6570\u6216\u7cfb\u7edf\u7684\u5e73\u5747\u503c\u3002</p> </li> <li> <p>\u4f7f\u7528\u5927\u6570\u5b9a\u5f8b\u548c\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\uff0c\u6765\u786e\u4fdd\u901a\u8fc7\u5927\u91cf\u968f\u673a\u6837\u672c\u7684\u5e73\u5747\u503c\u53ef\u4ee5\u903c\u8fd1\u771f\u5b9e\u503c\u3002</p> </li> </ol> <p>\u7b80\u5355\u6765\u8bf4\uff0cMonte Carlo \u65b9\u6cd5\u901a\u8fc7\u591a\u6b21\u6a21\u62df\u548c\u7edf\u8ba1\u5206\u6790\uff0c\u6765\u4f30\u8ba1\u67d0\u4e9b\u590d\u6742\u95ee\u9898\u7684\u89e3\u3002</p>"},{"location":"book/chapter1_Introduction/1.4statistics/#82-monte-carlo-estimate","title":"8.2 Monte Carlo estimate \u7684\u6b65\u9aa4","text":"<p>\u4ee5\u4f30\u8ba1\u79ef\u5206 \\(\\int_a^b f(x) dx\\) \u4e3a\u4f8b\uff1a</p> <ol> <li> <p>\u5728\u533a\u95f4 \\([a, b]\\) \u5185\u751f\u6210 \\(N\\) \u4e2a\u968f\u673a\u6837\u672c \\(x_1, x_2, \\ldots, x_N\\)\u3002</p> </li> <li> <p>\u8ba1\u7b97 \\(f(x)\\) \u7684\u503c\u5728\u8fd9\u4e9b\u6837\u672c\u70b9\u4e0a\u7684\u5e73\u5747\u503c\uff1a$ \\text{\u5e73\u5747\u503c} = \\frac{1}{N} \\sum_{i=1}^N f(x_i)$</p> </li> <li> <p>\u7528\u4ee5\u4e0b\u516c\u5f0f\u4f30\u8ba1\u79ef\u5206\u503c\uff1a\\(\\int_a^b f(x) dx \\approx (b-a) \\cdot \\text{\u5e73\u5747\u503c}\\)</p> </li> </ol> <p>\u5bf9\u4e8e\u591a\u7ef4\u79ef\u5206\u3001\u6982\u7387\u5206\u5e03\u8ba1\u7b97\u6216\u590d\u6742\u7269\u7406\u7cfb\u7edf\u6a21\u62df\uff0cMonte Carlo \u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u7c7b\u4f3c\u7684\u65b9\u5f0f\u8fdb\u884c\u6269\u5c55\u3002</p>"},{"location":"book/chapter1_Introduction/1.4statistics/#83-monte-carlo-estimate","title":"8.3 Monte Carlo estimate \u7684\u5178\u578b\u5e94\u7528","text":"<ol> <li> <p>\u8ba1\u7b97\u590d\u6742\u79ef\u5206:  \u7528\u4e8e\u89e3\u51b3\u4f20\u7edf\u6570\u503c\u79ef\u5206\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u9ad8\u7ef4\u79ef\u5206\u95ee\u9898\u3002</p> </li> <li> <p>\u6a21\u62df\u968f\u673a\u8fc7\u7a0b:  \u6a21\u62df\u80a1\u7968\u4ef7\u683c\u8def\u5f84\uff08\u5982 Black-Scholes \u6a21\u578b\uff09\u3001\u7c92\u5b50\u8fd0\u52a8\u3001\u961f\u5217\u6392\u961f\u7b49\u95ee\u9898\u3002</p> </li> <li> <p>\u6982\u7387\u4f30\u8ba1:  \u4f30\u8ba1\u4e8b\u4ef6\u53d1\u751f\u7684\u6982\u7387\uff0c\u4f8b\u5982\u5728\u7edf\u8ba1\u7269\u7406\u4e2d\u6a21\u62df\u7cfb\u7edf\u7684\u72b6\u6001\u5206\u5e03\u3002</p> </li> <li> <p>\u4f18\u5316\u95ee\u9898:  \u5728\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u7528 Monte Carlo \u65b9\u6cd5\u4f18\u5316\u53c2\u6570\u6216\u6a21\u578b\u6027\u80fd\u3002</p> </li> </ol>"},{"location":"book/chapter1_Introduction/1.4statistics/#84","title":"8.4 \u4f18\u70b9\u4e0e\u7f3a\u70b9","text":""},{"location":"book/chapter1_Introduction/1.4statistics/#841","title":"8.4.1 \u4f18\u70b9","text":"<ul> <li> <p>\u901a\u7528\u6027\u5f3a:  \u53ef\u4ee5\u5e94\u7528\u4e8e\u9ad8\u7ef4\u3001\u590d\u6742\u7684\u95ee\u9898\u3002</p> </li> <li> <p>\u6613\u4e8e\u5b9e\u73b0:  \u4e0d\u4f9d\u8d56\u95ee\u9898\u7684\u5177\u4f53\u5f62\u5f0f\uff0c\u53ea\u9700\u8981\u751f\u6210\u968f\u673a\u6837\u672c\u5e76\u8ba1\u7b97\u5e73\u5747\u503c\u3002</p> </li> <li> <p>\u6e10\u8fdb\u6536\u655b\uff1a  \u968f\u7740\u6837\u672c\u6570\u91cf\u589e\u52a0\uff0c\u4f30\u8ba1\u503c\u4f1a\u9010\u6e10\u6536\u655b\u5230\u771f\u5b9e\u503c\u3002</p> </li> </ul>"},{"location":"book/chapter1_Introduction/1.4statistics/#842","title":"8.4.2 \u7f3a\u70b9","text":"<ul> <li> <p>\u6548\u7387\u8f83\u4f4e\uff1a  \u6536\u655b\u901f\u5ea6\u8f83\u6162\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u6837\u672c\u6765\u8fbe\u5230\u9ad8\u7cbe\u5ea6\u3002</p> </li> <li> <p>\u4f9d\u8d56\u968f\u673a\u6570\u8d28\u91cf\uff1a  \u751f\u6210\u9ad8\u8d28\u91cf\u7684\u968f\u673a\u6837\u672c\u5bf9\u7ed3\u679c\u81f3\u5173\u91cd\u8981\u3002</p> </li> <li> <p>\u9ad8\u7ef4\u95ee\u9898\u7684\u6837\u672c\u6548\u7387\u4f4e\uff1a  \u9ad8\u7ef4\u95ee\u9898\u4e2d\u7684\u201c\u7ef4\u5ea6\u707e\u96be\u201d\u4f1a\u663e\u8457\u589e\u52a0\u6240\u9700\u6837\u672c\u6570\u91cf\u3002</p> </li> </ul>"},{"location":"book/chapter1_Introduction/1.4statistics/#85","title":"8.5 \u76f4\u89c2\u4f8b\u5b50","text":"<p>Monte Carlo estimate  \u662f\u4e00\u79cd\u5229\u7528\u968f\u673a\u62bd\u6837\u65b9\u6cd5\u6765\u8fd1\u4f3c\u8ba1\u7b97\u590d\u6742\u95ee\u9898\u7684\u6570\u503c\u89e3\u7684\u7edf\u8ba1\u6280\u672f\u3002\u5b83\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6570\u5b66\u3001\u7269\u7406\u3001\u91d1\u878d\u3001\u5de5\u7a0b\u7b49\u9886\u57df\uff0c\u5c24\u5176\u662f\u90a3\u4e9b\u89e3\u6790\u89e3\u96be\u4ee5\u83b7\u5f97\u7684\u95ee\u9898\u3002</p>"},{"location":"book/chapter1_Introduction/1.4statistics/#9entropykl-divergenceelbo","title":"9.Entropy,KL divergence,ELBO","text":""},{"location":"book/chapter1_Introduction/1.4statistics/#91-entropy","title":"9.1 \u71b5(Entropy)","text":"<p>\u71b5\u8861\u91cf\u5206\u5e03\u7684\u5e73\u5747\u4e0d\u786e\u5b9a\u6027\u3002 \u79ef\u5206\u5f62\u5f0f \u5bf9\u4e8e\u8fde\u7eed\u5206\u5e03 \\(p(x)\\)\uff0c\u71b5\u7684\u79ef\u5206\u5f62\u5f0f\u4e3a\uff1a</p> \\[  H(X) = -\\int p(x) \\log p(x) \\, dx \\] <p>\u671f\u671b\u5f62\u5f0f \u71b5\u4e5f\u53ef\u4ee5\u8868\u793a\u4e3a\u968f\u673a\u53d8\u91cf \\(X\\) \u7684\u4fe1\u606f\u91cf\u7684\u671f\u671b\uff1a</p> \\[  H(X) = -\\mathbb{E}_{X \\sim p(x)}[\\log p(X)] \\] <p>\u5176\u4e2d \\(\\mathbb{E}_{X \\sim p(x)}\\) \u8868\u793a\u5173\u4e8e\u5206\u5e03 \\(p(x)\\) \u7684\u671f\u671b\u3002</p>"},{"location":"book/chapter1_Introduction/1.4statistics/#92klkl-divergence","title":"9.2KL\u6563\u5ea6\uff08KL Divergence","text":"<p>KL\u6563\u5ea6\u8861\u91cf\u4e24\u4e2a\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u5f02\u3002 \u79ef\u5206\u5f62\u5f0f \u5bf9\u4e8e\u8fde\u7eed\u5206\u5e03 \\(p(x)\\) \u548c \\(q(x)\\)\uff0cKL\u6563\u5ea6\u7684\u79ef\u5206\u5f62\u5f0f\u4e3a\uff1a</p> \\[  D_{\\text{KL}}(P \\| Q) = \\int p(x) \\log \\frac{p(x)}{q(x)} \\, dx \\] <p>\u671f\u671b\u5f62\u5f0f KL\u6563\u5ea6\u53ef\u4ee5\u5199\u4e3a\u5173\u4e8e\u5206\u5e03 \\(p(x)\\) \u7684\u5bf9\u6570\u6bd4\u503c\u7684\u671f\u671b\uff1a</p> \\[  D_{\\text{KL}}(P \\| Q) = \\mathbb{E}_{X \\sim p(x)} \\left[ \\log \\frac{p(X)}{q(X)} \\right] \\]"},{"location":"book/chapter1_Introduction/1.4statistics/#93-evidence-lower-bound-elbo","title":"9.3. \u8bc1\u636e\u4e0b\u754c\uff08Evidence Lower Bound, ELBO)","text":"<p>ELBO \u662f\u5728\u53d8\u5206\u63a8\u65ad\u4e2d\u7528\u6765\u4f18\u5316\u540e\u9a8c\u5206\u5e03\u7684\u76ee\u6807\u51fd\u6570\u3002</p> <p>\u79ef\u5206\u5f62\u5f0f ELBO \u7684\u79ef\u5206\u5f62\u5f0f\u4e3a\uff1a</p> \\[  \\mathcal{L}(q) = \\int q(z) \\log p(\\mathcal{D} \\mid z) \\, dz - \\int q(z) \\log \\frac{q(z)}{p(z)} \\, dz \\] <p>\u671f\u671b\u5f62\u5f0f \u5c06\u79ef\u5206\u5f62\u5f0f\u6539\u5199\u4e3a\u5173\u4e8e \\(\\(q(z)\\)\\) \u7684\u671f\u671b\uff0c\u5f97\u5230\uff1a</p> \\[  \\mathcal{L}(q) = \\mathbb{E}_{z \\sim q(z)}[\\log p(\\mathcal{D} \\mid z)] - D_{\\text{KL}}(q(z) \\| p(z)) \\] <p>\u5206\u89e3\u4e3a\u5bf9\u6570\u4f3c\u7136 \u8fb9\u9645\u5bf9\u6570\u4f3c\u7136\u53ef\u4ee5\u5206\u89e3\u4e3a ELBO \u548c KL \u6563\u5ea6\uff1a</p> \\[  \\log p(\\mathcal{D}) = \\mathcal{L}(q) + D_{\\text{KL}}(q(z) \\| p(z \\mid \\mathcal{D})) \\] <p>\u5176\u4e2d\uff1a</p> \\[\\mathcal{L}(q) = \\mathbb{E}_{z \\sim q(z)}[\\log p(\\mathcal{D}, z) - \\log q(z)]\\] <p>\u603b\u7ed3</p> \u6982\u5ff5 \u79ef\u5206\u5f62\u5f0f \u671f\u671b\u5f62\u5f0f \u71b5 \\(H(X)\\) \\(-\\int p(x) \\log p(x)\\) \\(-\\mathbb{E}_{X \\sim p(x)}[\\log p(X)]\\) KL\u6563\u5ea6 \\({\\text{KL}}(P \\| Q)\\) \\(\\int p(x) \\log \\frac{p(x)}{q(x)} \\, dx\\) \\(\\mathbb{E}_{X \\sim p(x)} \\left[ \\log \\frac{p(X)}{q(X)} \\right]\\) ELBO \\(\\mathcal{L}(q)\\) \\(\\int q(z) \\log p(\\mathcal{D} \\mid z) \\, dz - \\int q(z) \\log \\frac{q(z)}{p(z)}\\) $\\mathbb{E}{z \\sim q(z)}[\\log p(\\mathcal{D} \\mid z)] - D{\\text{KL}}(q(z) | p(z)) $ <p></p>"},{"location":"book/chapter1_Introduction/1.4statistics/#931-elbo","title":"9.3.1 ELBO","text":"<p>\u8bc1\u660e</p> \\[  \\mathcal{L}(x, \\theta, q) = \\log p_\\theta(x) - D_{\\text{KL}}(q_\\phi(z|x) \\| p_\\theta(z|x)).\\\\ =\\mathbb{E}_{z \\sim q_\\phi} [\\log p_\\theta(x, z)] + H(q_\\phi). \\] <p>\u63a8\u5bfc\u6b65\u9aa4</p> <ol> <li>KL \u6563\u5ea6\u5b9a\u4e49 \uff1a \u7531 KL \u6563\u5ea6\u7684\u5b9a\u4e49\uff1a</li> </ol> \\[  D_{\\text{KL}}(q_\\phi(z|x) \\| p_\\theta(z|x)) = \\mathbb{E}_{z \\sim q_\\phi} \\left[ \\log \\frac{q_\\phi(z|x)}{p_\\theta(z|x)} \\right]. \\] <p>\u5c06\u5176\u4ee3\u5165\u516c\u5f0f\uff1a</p> \\[  \\mathcal{L}(x, \\theta, q) = \\log p_\\theta(x) - \\mathbb{E}_{z \\sim q_\\phi} \\left[ \\log \\frac{q_\\phi(z|x)}{p_\\theta(z|x)} \\right]. \\] <ol> <li>\u8d1d\u53f6\u65af\u516c\u5f0f\u66ff\u6362 \\(p_\\theta(z|x)\\) :</li> </ol> <p>\u6839\u636e\u8d1d\u53f6\u65af\u516c\u5f0f\uff1a</p> \\[  p_\\theta(z|x) = \\frac{p_\\theta(x, z)}{p_\\theta(x)}. \\] <p>\u5c06\u5176\u4ee3\u5165\uff1a</p> \\[  \\mathcal{L}(x, \\theta, q) = \\log p_\\theta(x) - \\mathbb{E}_{z \\sim q_\\phi} \\left[ \\log \\frac{q_\\phi(z|x)}{p_\\theta(x, z) / p_\\theta(x)} \\right]. \\] <p>\u7b80\u5316\u5206\u6bcd\u4e2d\u7684\u5206\u6570\uff1a</p> \\[  \\mathcal{L}(x, \\theta, q) = \\log p_\\theta(x) - \\mathbb{E}_{z \\sim q_\\phi} \\left[ \\log \\left( q_\\phi(z|x) \\cdot \\frac{p_\\theta(x)}{p_\\theta(x, z)} \\right) \\right]. \\] <ol> <li>\u5206\u89e3\u5bf9\u6570\u9879 \uff1a \u5c55\u5f00\u5bf9\u6570\u9879\uff1a</li> </ol> \\[  \\mathcal{L}(x, \\theta, q) = \\log p_\\theta(x) - \\mathbb{E}_{z \\sim q_\\phi} \\left[ \\log q_\\phi(z|x) + \\log p_\\theta(x) - \\log p_\\theta(x, z) \\right]. \\] <p>\u5c06\u671f\u671b\u5c55\u5f00\uff1a</p> \\[  \\mathcal{L}(x, \\theta, q) = \\log p_\\theta(x) - \\mathbb{E}_{z \\sim q_\\phi} [\\log q_\\phi(z|x)] - \\mathbb{E}_{z \\sim q_\\phi} [\\log p_\\theta(x)] + \\mathbb{E}_{z \\sim q_\\phi} [\\log p_\\theta(x, z)]. \\] <p>\u6ce8\u610f\u5230 \\(\\log p_\\theta(x)\\) \u662f\u5e38\u6570\uff0c\u53ef\u4ee5\u63d0\u5230\u671f\u671b\u5916\u90e8\uff1a</p> \\[  \\mathcal{L}(x, \\theta, q) = \\mathbb{E}_{z \\sim q_\\phi} [\\log p_\\theta(x, z)] - \\mathbb{E}_{z \\sim q_\\phi} [\\log q_\\phi(z|x)]. \\] <ol> <li>\u71b5\u7684\u5b9a\u4e49\u66ff\u6362 \uff1a</li> </ol> <p>\u6839\u636e\u71b5\u7684\u5b9a\u4e49\uff1a</p> \\[  H(q_\\phi) = -\\mathbb{E}_{z \\sim q_\\phi} [\\log q_\\phi(z|x)]. \\] <p>\u5c06\u5176\u4ee3\u5165\uff1a</p> \\[  \\mathcal{L}(x, \\theta, q) = \\mathbb{E}_{z \\sim q_\\phi} [\\log p_\\theta(x, z)] + H(q_\\phi). \\]"},{"location":"book/chapter1_Introduction/1.4statistics/#reference","title":"Reference","text":"<p>[1] Book: Deep Learning, Ian Goodfellow, Yoshua Bengio, Aaron Courville</p>"},{"location":"book/chapter1_Introduction/1.5SDE/","title":"SDE","text":""},{"location":"book/chapter1_Introduction/1.5SDE/#refered-book","title":"refered book","text":"<ul> <li>AN INTRODUCTION TO STOCHASTIC DIFFERENTIAL EQUATIONS</li> </ul> <p>\u4e0b\u56fe\u6a21\u62df\u4e86\u82b1\u7c89(\u9ec4\u8272)\u6536\u5230\u5927\u91cf\u6c34\u5206\u5b50\u7684\u649e\u51fb\u8fdb\u884c\u968f\u673a\u7684\u5e03\u6717\u8fd0\u52a8\u7684\u60c5\u5f62\u3002</p>"},{"location":"book/chapter1_Introduction/1.5SDE/#reference","title":"reference","text":"<ol> <li>AN INTRODUCTION TO STOCHASTIC DIFFERENTIAL EQUATIONS</li> <li>\u6269\u6563\u6a21\u578b\u4e2d\u5e03\u6717\u8fd0\u52a8</li> </ol>"},{"location":"book/chapter1_Introduction/tutorials/","title":"Tutorials on generative AI","text":""},{"location":"book/chapter1_Introduction/tutorials/#comprehensive-tutorials-on-generative-ai","title":"Comprehensive Tutorials on generative AI","text":"<ul> <li>Tutorial on Diffusion Models for Imaging and Vision Introduced the VAE, DDPM, Score Matching, SDE and Langevin and Fokker-Planck Equations</li> </ul>"},{"location":"book/chapter1_Introduction/tutorials/#energy-based-models","title":"energy based models","text":"<ul> <li>energy-based models with contrastive divergence explanation</li> </ul>"},{"location":"book/chapter2_VAE/2.1introduction/","title":"VAE \u7406\u8bba","text":""},{"location":"book/chapter2_VAE/2.1introduction/#1-p_theta","title":"1. \\(p_\\theta\\) \u4e3a\u9ad8\u65af\u5206\u5e03","text":""},{"location":"book/chapter2_VAE/2.1introduction/#11-variational-autoencoder-vae-p_thetaxz","title":"1.1 Variational Autoencoder (VAE) \u4e2d\u5173\u4e8e \\(P_\\theta(x|z)\\) \u662f\u9ad8\u65af\u5206\u5e03\u7684\u5047\u8bbe","text":"<p>\u5728 VAE \u7684\u6846\u67b6\u4e2d\uff0c\u89e3\u7801\u5668 \\( P_\\theta(x|z) \\) \u901a\u5e38\u88ab\u5047\u8bbe\u4e3a \u9ad8\u65af\u5206\u5e03\u3002\u8fd9\u79cd\u5047\u8bbe\u662f VAE \u7684\u57fa\u7840\u4e4b\u4e00\uff0c\u5bf9\u6a21\u578b\u7684\u91cd\u5efa\u8bef\u5dee\u5b9a\u4e49\u548c\u4f18\u5316\u76ee\u6807\u81f3\u5173\u91cd\u8981\u3002\u4ee5\u4e0b\u662f\u5173\u4e8e\u8fd9\u4e2a\u5047\u8bbe\u7684\u76f8\u5173\u77e5\u8bc6\u3002</p>"},{"location":"book/chapter2_VAE/2.1introduction/#12-p_thetaxz","title":"1.2 \u4e3a\u4ec0\u4e48\u5047\u8bbe \\(P_\\theta(x|z)\\) \u4e3a\u9ad8\u65af\u5206\u5e03\uff1f","text":""},{"location":"book/chapter2_VAE/2.1introduction/#121","title":"1.2.1 \u7b80\u5316\u95ee\u9898","text":"<ul> <li>\u9ad8\u65af\u5206\u5e03\u662f\u4e00\u79cd\u8fde\u7eed\u578b\u6982\u7387\u5206\u5e03\uff0c\u6570\u5b66\u6027\u8d28\u826f\u597d\uff0c\u6613\u4e8e\u8ba1\u7b97\u3002</li> <li>\u5bf9\u4e8e\u8fde\u7eed\u578b\u6570\u636e\uff08\u5982\u56fe\u50cf\u50cf\u7d20\u503c\u3001\u97f3\u9891\u4fe1\u53f7\u7b49\uff09\uff0c\u9ad8\u65af\u5206\u5e03\u80fd\u5f88\u597d\u5730\u62df\u5408\u5927\u591a\u6570\u6570\u636e\u70b9\u7684\u6ce2\u52a8\u7279\u6027\u3002</li> </ul>"},{"location":"book/chapter2_VAE/2.1introduction/#122","title":"1.2.2 \u7b26\u5408\u91cd\u5efa\u8bef\u5dee\u7684\u5b9a\u4e49","text":"<ul> <li>\u9ad8\u65af\u5206\u5e03\u7684\u5bf9\u6570\u4f3c\u7136\u5177\u6709\u4ee5\u4e0b\u5f62\u5f0f\uff1a</li> </ul> <p>$$   \\log P_\\theta(x|z) = -\\frac{|x - \\mu_\\theta(z)|^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2)   $$</p> <ul> <li>\u8fd9\u91cc\uff0c\\(\\mu_\\theta(z)\\) \u662f\u89e3\u7801\u5668\u9884\u6d4b\u7684\u9ad8\u65af\u5206\u5e03\u5747\u503c\uff0c\\(\\sigma^2\\) \u662f\u65b9\u5dee\uff08\u901a\u5e38\u53ef\u4ee5\u5047\u8bbe\u4e3a\u5e38\u91cf\u6216\u7531\u89e3\u7801\u5668\u9884\u6d4b\uff09\u3002</li> <li> <p>\u6700\u5927\u5316 \\(\\log P_\\theta(x|z)\\) \u7b49\u4ef7\u4e8e\u6700\u5c0f\u5316\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\uff1a</p> \\[ \\|x - \\mu_\\theta(z)\\|^2 \\] </li> </ul>"},{"location":"book/chapter2_VAE/2.1introduction/#123","title":"1.2.3 \u5141\u8bb8\u5efa\u6a21\u6570\u636e\u7684\u4e0d\u786e\u5b9a\u6027","text":"<ul> <li>\u5047\u8bbe \\(P_\\theta(x|z)\\) \u4e3a\u9ad8\u65af\u5206\u5e03\uff0c\u89e3\u7801\u5668\u4e0d\u4ec5\u9884\u6d4b\u91cd\u6784\u7684\u5747\u503c \\(\\mu_\\theta(z)\\)\uff0c\u8fd8\u53ef\u4ee5\u901a\u8fc7\u65b9\u5dee \\(\\sigma^2_\\theta(z)\\) \u6355\u6349\u6570\u636e\u7684\u4e0d\u786e\u5b9a\u6027\u3002</li> <li>\u65b9\u5dee\u7684\u5f15\u5165\u6709\u52a9\u4e8e\u907f\u514d\u8fc7\u5ea6\u62df\u5408\uff0c\u5c24\u5176\u662f\u5728\u8bad\u7ec3\u6570\u636e\u5b58\u5728\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\u3002</li> </ul>"},{"location":"book/chapter2_VAE/2.1introduction/#13","title":"1.3 \u6570\u5b66\u5f62\u5f0f","text":""},{"location":"book/chapter2_VAE/2.1introduction/#131","title":"1.3.1 \u89e3\u7801\u5668\u5206\u5e03","text":"<p>\u5728 VAE \u4e2d\uff0c\u89e3\u7801\u5668\u5b9a\u4e49\u4e3a\u6761\u4ef6\u6982\u7387\u5206\u5e03 \\(P_\\theta(x|z)\\)\uff0c\u5047\u8bbe\u4e3a\u9ad8\u65af\u5206\u5e03\uff1a</p> \\[ P_\\theta(x|z) = \\mathcal{N}(x; \\mu_\\theta(z), \\sigma_\\theta^2(z)) \\] <ul> <li>\\(\\mu_\\theta(z)\\)\uff1a\u89e3\u7801\u5668\u9884\u6d4b\u7684\u91cd\u6784\u5747\u503c\uff0c\u901a\u5e38\u7531\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u3002</li> <li>\\(\\sigma_\\theta^2(z)\\)\uff1a\u89e3\u7801\u5668\u9884\u6d4b\u7684\u91cd\u6784\u65b9\u5dee\uff0c\u53ef\u4ee5\u662f\u56fa\u5b9a\u5e38\u91cf\uff0c\u4e5f\u53ef\u4ee5\u7531\u7f51\u7edc\u5efa\u6a21\u3002</li> </ul>"},{"location":"book/chapter2_VAE/2.1introduction/#2","title":"2. \u5176\u4ed6\u5206\u5e03","text":""},{"location":"book/chapter2_VAE/2.1introduction/#21-l_1","title":"2.1 \u62c9\u666e\u62c9\u65af\u5206\u5e03\u4e0e \\(L_1\\) \u8303\u6570","text":"<ul> <li>\u5047\u8bbe\u5206\u5e03:   \\(P_\\theta(x|z) = \\text{Laplace}(x; \\mu_\\theta(z), b)\\)</li> <li>\u5bf9\u6570\u4f3c\u7136:</li> </ul> <p>$$   \\log P_\\theta(x|z) = -\\frac{|x - \\mu_\\theta(z)|_1}{b} - \\log(2b)   $$</p> <ul> <li>\u635f\u5931\u51fd\u6570:</li> </ul> <p>$$   L = \\frac{1}{b}|x - \\mu_\\theta(z)|_1   $$</p> <ul> <li>\u5bf9\u5e94\u5f62\u5f0f: \u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u3002</li> </ul>"},{"location":"book/chapter2_VAE/2.1introduction/#22","title":"2.2 \u4f2f\u52aa\u5229\u5206\u5e03\u4e0e\u4ea4\u53c9\u71b5","text":"<ul> <li>\u5047\u8bbe\u5206\u5e03:   \\(P_\\theta(x|z) = \\text{Bernoulli}(x; p_\\theta(z))\\)</li> <li>\u5bf9\u6570\u4f3c\u7136:</li> </ul> <p>$$   \\log P_\\theta(x|z) = x \\log p_\\theta(z) + (1-x) \\log (1-p_\\theta(z))   $$</p> <ul> <li>\u635f\u5931\u51fd\u6570:</li> </ul> <p>$$   L = -[x \\log p_\\theta(z) + (1-x) \\log (1-p_\\theta(z))]   $$</p> <ul> <li>\u5bf9\u5e94\u5f62\u5f0f: \u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931\u3002</li> </ul>"},{"location":"book/chapter2_VAE/2.1introduction/#23","title":"2.3 \u591a\u9879\u5206\u5e03\u4e0e\u591a\u5206\u7c7b\u4ea4\u53c9\u71b5","text":"<ul> <li>\u5047\u8bbe\u5206\u5e03:   \\(P_\\theta(x|z) = \\text{Categorical}(x; \\mathbf{p}_\\theta(z))\\)</li> <li>\u5bf9\u6570\u4f3c\u7136:</li> </ul> <p>$$   \\log P_\\theta(x|z) = \\sum_i x_i \\log p_{\\theta,i}(z)   $$</p> <ul> <li>\u635f\u5931\u51fd\u6570:</li> </ul> <p>$$   L = -\\sum_i x_i \\log p_{\\theta,i}(z)   $$</p> <ul> <li>\u5bf9\u5e94\u5f62\u5f0f: \u591a\u5206\u7c7b\u4ea4\u53c9\u71b5\u3002</li> </ul>"},{"location":"book/chapter2_VAE/2.1introduction/#24","title":"2.4 \u6df7\u5408\u9ad8\u65af\u5206\u5e03","text":"<ul> <li>\u5047\u8bbe\u5206\u5e03:   \\(P_\\theta(x|z) = \\sum_k \\pi_k \\mathcal{N}(x; \\mu_k(z), \\sigma_k^2(z))\\)</li> <li>\u5bf9\u6570\u4f3c\u7136:</li> </ul> <p>$$   \\log P_\\theta(x|z) = \\log \\sum_k \\pi_k \\mathcal{N}(x; \\mu_k(z), \\sigma_k^2(z))   $$</p> <ul> <li>\u7279\u70b9: \u7528\u4e8e\u591a\u6a21\u6001\u6570\u636e\u5efa\u6a21\uff0c\u8ba1\u7b97\u635f\u5931\u9700\u8981\u6570\u503c\u8fd1\u4f3c\u3002</li> </ul>"},{"location":"book/chapter2_VAE/2.1introduction/#25","title":"2.5 \u603b\u7ed3","text":"<ul> <li> <p>\u6838\u5fc3\u601d\u60f3:   \u635f\u5931\u51fd\u6570\u4e0e\u6982\u7387\u5206\u5e03\u7684\u5173\u7cfb\u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u89c6\u89d2\uff0c\u7528\u4e8e\u8bbe\u8ba1\u548c\u4f18\u5316\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u5e38\u89c1\u635f\u5931\u51fd\u6570\uff08\u5982 MSE\u3001MAE\u3001\u4ea4\u53c9\u71b5\uff09\u5747\u53ef\u4ee5\u4ece\u5bf9\u5e94\u7684\u5206\u5e03\u5047\u8bbe\u4e2d\u63a8\u5bfc\u800c\u6765\u3002</p> </li> <li> <p>\u5b9e\u9645\u5e94\u7528:   \u6839\u636e\u6570\u636e\u7279\u6027\u9009\u62e9\u5408\u9002\u7684\u5206\u5e03\u5047\u8bbe\u548c\u635f\u5931\u51fd\u6570\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u3002\u4f8b\u5982\uff1a</p> </li> <li>\u8fde\u7eed\u503c\u6570\u636e\u9002\u7528\u9ad8\u65af\u5206\u5e03\uff08MSE\uff09\u3002</li> <li>\u4e8c\u503c\u6570\u636e\u9002\u7528\u4f2f\u52aa\u5229\u5206\u5e03\uff08\u4ea4\u53c9\u71b5\uff09\u3002</li> <li>\u7a00\u758f\u6570\u636e\u9002\u7528\u62c9\u666e\u62c9\u65af\u5206\u5e03\uff08MAE\uff09\u3002</li> </ul>"},{"location":"book/chapter2_VAE/2.1introduction/#26","title":"2.6 \u76f8\u5173\u8bba\u6587","text":"<ol> <li>\u300aAuto-Encoding Variational Bayes\u300b</li> <li>\u4f5c\u8005: Kingma, D.P., Welling, M.</li> <li>\u94fe\u63a5: https://arxiv.org/abs/1312.6114</li> <li> <p>\u5185\u5bb9: \u63d0\u51fa\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\uff0c\u63a8\u5bfc\u51fa\u91cd\u5efa\u8bef\u5dee\u9879\u4e0e\u5206\u5e03\u5047\u8bbe\u7684\u5173\u7cfb\u3002</p> </li> <li> <p>\u300aHybridised Loss Functions for Improved Neural Network Generalisation\u300b</p> </li> <li>\u4f5c\u8005: Matthew C. Malan \u7b49</li> <li>\u94fe\u63a5: https://arxiv.org/abs/2204.12241</li> <li> <p>\u5185\u5bb9: \u63a2\u8ba8\u4ea4\u53c9\u71b5\u548c\u5747\u65b9\u8bef\u5dee\u7684\u6df7\u5408\u635f\u5931\u53ca\u5176\u5f71\u54cd\u3002</p> </li> <li> <p>\u300ap-Huber\u635f\u5931\u51fd\u6570\u53ca\u5176\u9c81\u68d2\u6027\u7814\u7a76\u300b</p> </li> <li>\u4f5c\u8005: \u4f59\u535a\u5929</li> <li>\u94fe\u63a5: https://pdf.hanspub.org/AAM20201200000_75579140.pdf</li> <li>\u5185\u5bb9: \u7814\u7a76 p-Huber \u635f\u5931\u5728\u6709\u566a\u58f0\u6570\u636e\u4e2d\u7684\u8868\u73b0\u3002</li> </ol>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/","title":"From GAN to PGGAN: Some Base GAN model Introduction","text":""},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#2014gan-generative-adversarial-networks","title":"[2014]GAN : Generative Adversarial Networks","text":"<p>GANs (Generative Adversarial Networks) are based on an adversarial process framework. In this framework, there are two networks that work in opposition: a generator and a discriminator. This can be compared to a scenario where one team (generator) tries to produce counterfeit items without being detected, while the other team (discriminator) acts like law enforcement trying to detect the fakes. Overall Introduction</p> <ul> <li> <p>Base model structure    Image Source: Semi Engineering - GAN Knowledge Center</p> </li> <li> <p>The model works through two independent learning models: a Generative Model and a Discriminative Model, which learn through adversarial training to produce high-quality outputs.</p> <ul> <li>Generative Model,short as \\(G\\)\uff1a</li> <li>Takes random noise \\(z\\) as input -&gt; Generates images \\(G(z)\\)</li> <li>Aims to create images that look real enough to fool \\(D\\)</li> <li>Discriminative Model, short as \\(D\\),is a binary classifier:</li> <li>Takes an image x as input -&gt; Output \\(D(x)\\), representing the probability that \\(x\\) is a real image<ul> <li>\\(D(x)=1\\) means 100% confidence it's real</li> <li>\\(D(x)=0\\) means it's definitely fake</li> </ul> </li> </ul> </li> <li> <p>Adversarial training: The optimization process is a minimax game, with the goal of reaching Nash equilibrium</p> </li> <li> <p>The generator tries to minimize the probability of the discriminator detecting fake samples     The discriminator tries to maximize its ability to distinguish between real and fake samples</p> </li> </ul>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#loss-function","title":"Loss function:","text":"<p>\\(\\(V(D,G) = \\underset{D}{\\max} {\\underbrace{\\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1-D(G(z)))]}_{\\text{Discriminator Loss}:L_D}} +\\underset{G}{\\min} {\\underbrace{\\mathbb{E}_{z \\sim p_z(z)}[\\log(1-D(G(z)))]}_{\\text{Generator Loss: }L_G}}\\)\\)</p> <p>Where:   - \\(p_{data}(x)\\) is the real data distribution   - \\(p_z(z)\\) is the noise distribution   - \\(G(z)\\) is generator mapping from noise to synthetic data   - \\(D(x)\\) is the discriminator's estimate of the probability that \\(x\\) is real</p> <p>overall training process:    Image Source: Goodfellow et al., \"Generative Adversarial Networks\" (2014) arXiv:1406.2661</p> <ul> <li> <p>In GAN training, we iterate the Discriminator (\\(D\\)) \\(K\\) times before updating the Generator (\\(G\\)) once\uff1a</p> </li> <li> <p>because we need \\(D\\) to be powerful enough to provide accurate feedback for \\(G's\\) improvement</p> </li> <li>This iterative strategy helps maintain training stability and prevent mode collapse, although K needs to be carefully balanced <ul> <li>too large and G can't learn effectively, too small and D's feedback becomes unreliable.</li> </ul> </li> </ul>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#base-model-structure","title":"Base model structure","text":"<p>Code reference: https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py</p> <pre><code>class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.size(0), *img_shape)\n        return img\n</code></pre> <pre><code>class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )\n    def forward(self, img):\n        img_flat = img.view(img.size(0), -1)\n        validity = self.model(img_flat)\n        return validity\n</code></pre> <p>Training process and images generated:</p> <p> Image Source: Goodfellow et al., \"Generative Adversarial Networks\" (2014) arXiv:1406.2661</p> <ul> <li>Real data distribution \\(p_{data}\\)(Black dotted line)</li> <li>Generator distribution \\(p_g\\)(Green solid line )</li> <li>Discriminator output \\(D\\)(Blue dashed line)</li> <li>Noise space where \\(z\\) is sampled (Lower horizontal line)</li> <li>Data space \\(x\\)(Upper horizontal line)</li> <li>Generator G's mapping from  \\(z\\) to  \\(x\\) (Arrows connecting lines)</li> </ul> <p>From the picture, we can see the training Evolution (from a to d):</p> <ul> <li> <p>Initial Stage:</p> </li> <li> <p>The generated distribution  \\(p_g\\) (green) differs significantly from the real distribution \\(p_{data}\\) (black)</p> </li> <li> <p>Discriminator \\(D\\)  (blue) attempts to distinguish samples, but performs unstably</p> </li> <li> <p>Discriminator Training: \\(D\\) is trained to reach optimal solution: \\(D^*(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}\\)</p> </li> <li> <p>Generator Update:  \\(G\\) updates based on gradients from \\(D\\)</p> </li> <li> <p>Final Convergence: When \\(G\\) and \\(D\\) have sufficient capacity\uff08 \\(p_{data} =p_g\\)), they reach Nash equilibrium</p> </li> </ul> <p> Image Source: Goodfellow et al., \"Generative Adversarial Networks\" (2014) arXiv:1406.2661</p>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#2014cgan-conditional-generative-adversarial-nets","title":"[2014]cGAN: Conditional Generative Adversarial Nets","text":""},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#overall-introduction","title":"Overall Introduction:","text":"<p>Conditional generation Traditional GANs produce samples from random noise but can't control the output features, as they are unsupervised learning.</p> <p>While conditional GANs (cGANs) incorporate conditional information into both the generator and discriminator, enabling control over the output properties. This is achieved through a semi-supervised approach.</p> <p>The cGAN paper only shows its generated results on the MNIST dataset, where simply concatenating label embeddings might have limited impact. However, the core idea of \"guiding the generation process with conditional information\" proposed by cGAN has significantly influenced subsequent generative models. </p> <ul> <li>For example, models like DALL-E and Stable Diffusion, although utilizing different architectures like Diffusion, have adopted the principle of conditional generation: they use text embeddings as conditional information to control image generation.</li> </ul>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#base-model-structure_1","title":"Base model structure","text":"<p> Source: Mirza et al., \"Conditional Generative Adversarial Nets\" (2014) arXiv:1411.1784</p> <p>How to combine the condition into input:</p> <ul> <li> <p>First convert categorical labels into continuous vector representations using nn.Embedding</p> </li> <li> <p><code>self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)</code></p> </li> <li> <p>Then concatenates torch.cat label embeddings(y) with noise vectors(z) along dimension -1: </p> </li> <li> <p><code>gen_input = torch.cat((self.label_emb(labels), noise), -1)</code></p> </li> <li> <p>Uses this concatenated vector as input to generate images through multiple network layers</p> </li> </ul> <pre><code>class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim + opt.n_classes, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n\n    def forward(self, noise, labels):\n        # Concatenate label embedding and image to produce input\n        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n        img = self.model(gen_input)\n        img = img.view(img.size(0), *img_shape)\n        return img\n</code></pre> <p>IN Discriminator:</p> <ul> <li> <p>Flattens input images: </p> <ul> <li><code>img.view(img.size(0), -1)</code></li> </ul> </li> <li> <p>Similarly, processes labels through embedding: <code>self.label_embedding(labels)</code></p> </li> <li> <p>Concatenates flattened images and label embeddings:</p> <ul> <li><code>d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)</code></li> </ul> </li> <li> <p>Passes concatenated vector through discriminator network for real/fake classification</p> </li> </ul> <pre><code>class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n\n        self.model = nn.Sequential(\n            nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 512),\n            nn.Dropout(0.4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 512),\n            nn.Dropout(0.4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 1),\n        )\n\n    def forward(self, img, labels):\n        # Concatenate label embedding and image to produce input\n        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n        validity = self.model(d_in)\n        return validity\n</code></pre> <p>Actually, it's still a Binary Classification Task. The output of cGAN discriminator still maintains GAN's binary output (real/fake). </p> <ul> <li>Doesn't explicitly verify condition-image matching</li> <li>Output is a single scalar through Sigmoid/BCELoss or MSELoss</li> </ul> <p>For example, an input condition: number \"7\". If the generator generates an image that looks like \"3\". Although the discriminator will not directly point out \"this is not 7\", because the label of \"7\" in the training data has never been paired with the image of \"3\". So this wrong match will be identified as \"generated\" by the discriminator.</p>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#loss-function_1","title":"Loss function","text":"\\[\\min_{G} \\max_{D} V(D,G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x|y)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z|y)))]\\] <p>Where: \\(\\mathbb{E}\\): Expected value (expectation) \\(\\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x|y)]\\) : </p> <ul> <li> <p>\\(x \\sim p_{data(x)}\\):  \\(x\\)sampled from real data distribution</p> </li> <li> <p>\\(D(x|y)\\): Discriminator's output for real data  \\(x\\) given condition  \\(y\\)</p> </li> <li> <p>\\(E[\\log D(x|y)]\\) - Discriminator's ability to identify real samples</p> </li> </ul> <p>\\(\\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z|y)))]\\) : </p> <ul> <li> <p>\\(z \\sim p_z(z)\\) z sampled from noise distribution</p> </li> <li> <p>\\(G(z|y)\\): Generator's output from noise z given condition y</p> </li> <li> <p>\\(E[log(1 - D(G(z|y)))]\\) - Discriminator's ability to identify fake samples</p> </li> </ul> <pre><code>for epoch in range(opt.n_epochs):\n    for i, (imgs, labels) in enumerate(dataloader):\n        batch_size = imgs.shape[0]\n        # Adversarial ground truths\n        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n        # Configure input\n        real_imgs = Variable(imgs.type(FloatTensor))\n        labels = Variable(labels.type(LongTensor))\n        # -----------------\n        #  Train Generator\n        # -----------------\n        optimizer_G.zero_grad()\n        # Sample noise and labels as generator input\n        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n        gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size)))\n        # Generate a batch of images\n        gen_imgs = generator(z, gen_labels)\n        # Loss measures generator's ability to fool the discriminator\n        validity = discriminator(gen_imgs, gen_labels)\n        g_loss = adversarial_loss(validity, valid)\n        g_loss.backward()\n        optimizer_G.step()\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n        optimizer_D.zero_grad()\n        # Loss for real images\n        validity_real = discriminator(real_imgs, labels)\n        d_real_loss = adversarial_loss(validity_real, valid)\n        # Loss for fake images\n        validity_fake = discriminator(gen_imgs.detach(), gen_labels)\n        d_fake_loss = adversarial_loss(validity_fake, fake)\n        # Total discriminator loss\n        d_loss = (d_real_loss + d_fake_loss) / 2\n        d_loss.backward()\n        optimizer_D.step()\n\n</code></pre> <p> Source: Mirza et al., \"Conditional Generative Adversarial Nets\" (2014) arXiv:1411.1784</p>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#2015dcgandeep-convolutional-ganunsupervised-representation-learning-with-deep-convolutional-generative-adversarial-networks","title":"[2015]DCGAN(Deep Convolutional GAN\uff09\uff1aUnsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks","text":""},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#overall-introduction-deep-convolutional-gan","title":"Overall Introduction:  Deep Convolutional GAN","text":"<p>DCGAN integrates the strengths of Convolutional Neural Networks into GANs with key innovations:</p> <ol> <li> <p>Convolutional Layers: Transposed convolutions in the generator and strided convolutions in the discriminator enhance spatial information retention.</p> </li> <li> <p>Batch Normalization: Used extensively in both parts to improve stability and prevent mode collapse.</p> </li> <li> <p>Activation Functions: The generator uses ReLU with a Tanh final layer, and the discriminator employs LeakyReLU.</p> </li> <li> <p>Starts from 100-dimensional noise z, gradually generating \\(64\u00d764\\) images through multiple convolution layers</p> </li> <li> <p>Feature map progression(CHW): </p> </li> <li> <p>\\(100\\times1\\times1\\)-&gt;\\(1024\\times4\\times4\\) -&gt; \\(512\\times8\\times8\\)-&gt;\\(256\\times16\\times16\\)-&gt;\\(128\\times32\\times32\\)-&gt;\\(3\\times64\\times64\\)</p> </li> </ol>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#base-model-structure_2","title":"Base model structure","text":"<p> Source: Radford et al., \"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\" (2016) arXiv:1511.06434</p> <p>About Transposed convolutions:</p> <ul> <li> <p>Input Matrix Expansion: Initially, the input feature map undergoes an expansion by inserting zeros between each element. The number of zeros inserted depends on the stride parameter. </p> </li> <li> <p>Application of the Convolution Kernel: Next, the convolution kernel is applied to the expanded feature map. This process is similar to traditional convolution operations, where the kernel slides over the expanded feature map, computing the dot product with local regions. Unlike regular convolution, this operation results in a larger output feature map because the input has been expanded.</p> </li> <li> <p>Adjustment of Output Size: Finally, the output feature map might be cropped or padded to adjust its dimensions to the desired size. This adjustment depends on the padding parameter, which can either reduce or increase the spatial dimensions of the output</p> </li> </ul>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#loss-function_2","title":"Loss function","text":"<p>Same as GAN objective : \\(\\min_G \\max_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z\\sim p_z}[\\log(1-D(G(z)))]\\)</p>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#2017-wgan-wasserstein-gan","title":"[2017] WGAN: Wasserstein GAN","text":""},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#overall-introduction_1","title":"Overall Introduction:","text":"<p>WGAN introduces Wasserstein distance and Lipschitz constraint  in loss function to \"improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches\". WGAN replaces the JSD with the Wasserstein distance to measure the distribution distance in the original GAN.</p> <ul> <li> <p>If the discriminator is trained too well, the generator's gradients vanish, and the generator's loss cannot decrease </p> </li> <li> <p>If the discriminator is not trained well enough, the generator's gradients become inaccurate, causing it to move erratically. </p> </li> </ul> <p>The discriminator needs to be trained to just the right degree - neither too well nor too poorly - but this balance is very difficult to achieve. Moreover, this optimal balance might even vary at different stages within the same training epoch, which is why GANs are so difficult to train.</p>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#drawbacks-as-jsd","title":"Drawbacks as JSD\uff1a","text":""},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#gradient-vanishing-problem-training-instability","title":"Gradient Vanishing Problem &amp; training instability","text":"<p>We have introduced above, under an (approximately) optimal discriminator, minimizing the generator's loss is equivalent to minimizing the JS divergence between \\(P_r\\) and \\(P_g\\). Since \\(P_r\\) and \\(P_g\\) almost inevitably have negligible overlap, their JS divergence will always be the constant \\(\\log 2\\), regardless of how far apart they are. This ultimately leads to the generator's gradient (approximately) becoming 0, resulting in gradient vanishing.  Source: Arjovsky et al., \"Wasserstein GAN\" (2017) arXiv:1701.07875</p>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#model-collapse","title":"Model collapse","text":"<p>Secondly, even the previously mentioned standard KL divergence term has flaws. Because KL divergence is not a symmetric measure, \\(KL(P_g\\|P_r)\\) and \\(KL(P_r\\|P_g)\\) are different.  Taking the former as an example: \\(KL(P_g||P_r) = \\int_x P_g(x)\\log(\\frac{P_g(x)}{P_r(x)})dx\\)</p> <ul> <li> <p>When \\(P_g(x) \\to 0\\) and \\(P_{r}(x) \\to 1\\), \\(P_g(x)\\log\\frac{P_g(x)}{P_{r}(x)} \\to 0\\), contributing nearly 0 to \\(KL(P_g||P_r)\\)</p> </li> <li> <p>When \\(P_g(x) \\to 1\\) and \\(P_{r}(x) \\to 0\\), \\(P_g(x)\\log\\frac{P_g(x)}{P_{r}(x)} \\to +\\infty\\), contributing positively infinite to \\(KL(P_g||P_r)\\)</p> </li> <li> <p>In other words, \\(KL(P_g||P_r)\\) penalizes these two types of errors differently. </p> </li> <li> <p>The first type of error corresponds to \"generator failing to generate real samples\" with small penalty.</p> </li> <li> <p>The second type corresponds to \"generator generating unrealistic samples\" with large penalty. </p> </li> </ul> <p>The first type of error represents a lack of diversity, while the second type represents a lack of accuracy. As a result, the generator would rather generate some repetitive but \"safe\" samples, and is reluctant to generate diverse samples, because one small mistake could lead to the second type of error, resulting in an unacceptable loss. This phenomenon is commonly referred to as mode collapse.</p> <p> Source: Arjovsky et al., \"Wasserstein GAN\" (2017) arXiv:1701.07875</p> <p>Why Wasserstein distance?</p> <p>The superiority of the Wasserstein distance compared to KL divergence and JS divergence lies in its ability to reflect the proximity between two distributions even when they don't overlap. While KL divergence and JS divergence are discontinuous , being either maximum or minimum. The Wasserstein distance is smooth and offers a more natural way to measure distances between distributions.</p> <ol> <li> <p>Training Stability: Provides meaningful gradients even when distributions do not overlap, significantly improving the stability of GAN training.</p> </li> <li> <p>Reduced Mode Collapse: Encourages diversity in generated samples by considering the overall differences between distributions, reducing mode collapse.</p> </li> <li> <p>Intuitive Loss Function: Serves as a loss metric, where a smaller Wasserstein distance indicates closer alignment with the target distribution's statistical properties.</p> </li> <li> <p>Effective GAN Training: WGANs use Wasserstein distance to offer a more stable and effective training process, enhancing the quality and diversity of generated samples.</p> </li> </ol>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#loss-function_3","title":"Loss function","text":"<ol> <li>About Wasserstein Distance (measure distance by estimating the difference between expectation):</li> <li>Mathematical Definition \uff1a\\(W(P,Q) = \\inf_{\\gamma \\in \\Pi(P,Q)} \\mathbb{E}_{(x,y)\\sim \\gamma}[||x-y||]\\)<ul> <li>\\(\\Pi(P_r, P_g)\\) is the set of all possible joint distributions whose marginal distributions are \\(P_r\\) and \\(P_g\\). </li> <li>In other words, for each distribution in \\(\\Pi(P_r, P_g)\\), its marginal distributions are \\(P_r\\) and \\(P_g\\). </li> <li>For each possible joint distribution \\(\\gamma\\),sample \\((x,y) \\sim \\gamma\\) to get a real sample \\(x\\) and a generated sample \\(y\\), and calculate the distance between these samples \\(\\|x-y\\|\\). </li> <li>\\(\\mathbb{E}_{(x,y)\\sim\\gamma}[\\|x-y\\|]\\)calculates the expected value of the sample distance under this joint distribution. The infimum of this expected value among all possible joint distributions \\(\\inf_{\\gamma\\sim\\Pi(P_r,P_g)} \\mathbb{E}_{(x,y)\\sim\\gamma}[\\|x-y\\|]\\) is defined as the Wasserstein distance.</li> <li>Intuitively, \\(\\mathbb{E}_{(x,y)\\sim\\gamma}[\\|x-y\\|]\\) can be understood as the \"cost\" of moving \"\\(P_r\\) pile of earth\" to \" \\(P_g\\) location\" under this \"transport plan\", and \\(W(P_r, P_g)\\) is the \"minimum cost\" under the \"optimal transport plan\", which is why it's called the Earth-Mover distance.</li> </ul> </li> <li>Lipschitz Constraint</li> <li>A function f is called Lipschitz continuous if it satisfies:(\\(|f(x) - f(y)| \\leq C|x - y|\\)\\)<ul> <li>where \\(C\\) is the Lipschitz constant.</li> <li>When inputs x and y are close to each other, their corresponding outputs \\(f(x)\\) and \\(f(y)\\) must also be close. This property ensures smoothness and continuity in the function</li> </ul> </li> <li>As for a discriminator in WGAN, D is constrained to be 1-Lipschitz functions: \\(|D(x) - D(y)| \\leq |x - y|\\)</li> <li>Wasserstein Distance in WGAN:</li> <li>GAN objective function :\\(\\(\\min_{G} \\max_{D} V(D,G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]\\)\\)</li> <li>In WGAN, the objective function can be written as:(\\(W(P_r, P_g) = \\sup_{||f||_L \\leq 1} \\mathbb{E}_{x \\sim \\mathbb{P}_r}[D(x)] - \\mathbb{E}_{x \\sim \\mathbb{P}_g}[D(x)]\\)\\)   where \uff1a<ul> <li>D is constrained to be 1-Lipschitz functions.</li> <li>\\(\\mathbb{P}_r\\) represents the distribution of real data (real probability distribution)</li> <li>\\(x\\sim\\mathbb{P}_r\\) means x is sampled from the real data distribution</li> <li>\\(\\mathbb{P}_g\\) represents the distribution of generated data (generated probability distribution)</li> <li>\\({x}\\sim\\mathbb{P}_g\\) means x is sampled from the generator's distribution</li> <li>WGAN tries to minimize the Wasserstein distance between \\(\\mathbb{P}_r\\) and \\(\\mathbb{P}_g\\) </li> <li>\\(||f||_L \\leq 1\\) means that the discriminator D must satisfy the Lipschitz condition.</li> </ul> </li> </ol> <p>how do we get the loss function:</p> <ul> <li>Primal Form (Original Wasserstein Distance):</li> </ul> <p>\\(\\(W(P,Q) = \\inf_{\\gamma \\in \\Pi(P,Q)} \\mathbb{E}_{(x,y)\\sim \\gamma}[||x-y||]\\)\\)</p> <p>where \\(\\Pi(P,Q)\\) is the set of all joint distributions (couplings) whose margins are P and Q.</p> <ul> <li>Kantorovich Duality Theorem:According to the duality Theorem, this problem is equivalent to:</li> </ul> <p>\\(\\(W(P,Q) = \\sup_{f\\in Lip_1} \\left(\\int f\\,dP - \\int f\\,dQ\\right)\\)\\) where \\(Lip_1\\) is the set of 1-Lipschitz functions.</p> <ul> <li>Expectation Form:Converting the integrals to expectations:</li> </ul> \\[W(P,Q) = \\sup_{||f||_L \\leq 1} [\\mathbb{E}_{x\\sim P}[f(x)] - \\mathbb{E}_{x\\sim Q}[f(x)]]\\] <ul> <li>Application to GAN: When P = Pr (real distribution) and Q = Pg (generated distribution):</li> </ul> \\[W(P_r,P_g) = \\sup_{||f||_L \\leq 1} [\\mathbb{E}_{x\\sim P_r}[f(x)] - \\mathbb{E}_{x\\sim P_g}[f(x)]]\\] <ol> <li> <p>Weight Clipping : -&gt; implementation of Lipschitz Constraint in WGAN</p> </li> <li> <p>WGAN forces the discriminator to satisfy the Lipschitz constraint through weight clipping or gradient penalty.</p> </li> <li> <p>After each gradient update, the weights of the critic (discriminator) are clipped to a fixed range [-c, c] </p> </li> <li> <p>In the paper, c = 0.01</p> </li> </ol> <pre><code># \u5728\u6bcf\u6b21\u53c2\u6570\u66f4\u65b0\u540e\u6267\u884c\nfor param in discriminator.parameters():\n    param.data.clamp_(-c, c)  # c\u901a\u5e38\u8bbe\u4e3a0.01\nWeight Clipping -&gt; all weight values are forced to be limited to the range of [-0.01, 0.01].\n\n</code></pre> <ul> <li> <p>Any value outside this range will be \"clipped\" to the boundary value. </p> </li> <li> <p>This ensures the Lipschitz constraint of the network, but may also lead to limitations in expressiveness.</p> </li> </ul> <p>\"Weight clipping is a clearly terrible way to enforce a Lipschitz constraint.\" -- M. Arjovsky, S. Chintala and L. Bottou, \"Wasserstein Generative Adversarial Networks,\" in International Conference on Machine Learning, 2017, pp. 214-223.</p>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#training-process-wasserstein-lipschitz-gradient","title":"Training process -&gt; Wasserstein &amp; Lipschitz  Gradient","text":"<p> Source: Arjovsky et al., \"Wasserstein GAN\" (2017) arXiv:1701.07875</p> <ol> <li>Discriminator  Gradient:</li> </ol> <p>The Wasserstein loss for the Discriminator is:\\(\\(L(w) = \\mathbb{E}_{x \\sim \\mathbb{P}_r}[f_w(x)] - \\mathbb{E}_{z \\sim p(z)}[f_w(g_\\theta(z))]\\)\\) where:</p> <ul> <li> <p>\\(f_w(x)\\) Discriminator evaluates real data samples</p> </li> <li> <p>\\(x\\sim\\mathbb{P}_r\\) means x is sampled from the real data distribution</p> </li> <li> <p>\\(f_w(g_\u03b8(z))\\) Discriminator evaluates generated data samples</p> <ul> <li> <p>\\(z\\) is random noise transformed by generator \\(g_\u03b8\\) </p> </li> <li> <p>\\(g_\u03b8(z)\\) represents the Generator generated samples</p> </li> </ul> </li> </ul> <p>For a batch of size m, the empirical version becomes: \\(\\(L(w) = \\frac{1}{m}\\sum_{i=1}^m f_w(x^{(i)}) - \\frac{1}{m}\\sum_{i=1}^m f_w(g_\\theta(z^{(i)}))\\)\\)</p> <p>Therefore, the gradient with respect to Discriminator parameters w is:</p> \\[\\nabla_w L = \\frac{1}{m}\\sum_{i=1}^m \\nabla_w f_w(x^{(i)}) - \\frac{1}{m}\\sum_{i=1}^m \\nabla_w f_w(g_\\theta(z^{(i)}))\\] <ol> <li>Generator Gradient:</li> </ol> <p>The generator's objective is to minimize:</p> \\[L(\u03b8) = -\\mathbb{E}_{z \\sim p(z)}[f_w(g_\\theta(z))]\\] <p>For a batch of size m, this becomes:</p> \\[L(\u03b8) = -\\frac{1}{m}\\sum_{i=1}^m f_w(g_\\theta(z^{(i)}))\\] <p>The gradient with respect to generator parameters \u03b8 is:</p> \\[\\nabla_\\theta L = -\\frac{1}{m}\\sum_{i=1}^m \\nabla_\\theta f_w(g_\\theta(z^{(i)}))\\] <ul> <li> <p>Line 5: Discriminator gradient computation \\(g_w \u2190 \\nabla_w [\\frac{1}{m}\\sum_{i=1}^m f_w(x^{(i)}) - \\frac{1}{m}\\sum_{i=1}^m f_w(g_\\theta(z^{(i)}))]\\)</p> </li> <li> <p>Line 10: Generator gradient computation \\(g_\\theta \u2190 -\\nabla_\\theta \\frac{1}{m}\\sum_{i=1}^m f_w(g_\\theta(z^{(i)}))\\)</p> </li> </ul> <pre><code>for i, (imgs, _) in enumerate(dataloader):\n        # Configure input\n        real_imgs = Variable(imgs.type(Tensor))\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n        optimizer_D.zero_grad()\n        # Sample noise as generator input\n        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n        # Generate a batch of images\n        fake_imgs = generator(z).detach()\n        # Adversarial loss\n        loss_D = -torch.mean(discriminator(real_imgs)) + torch.mean(discriminator(fake_imgs))\n        loss_D.backward()\n        optimizer_D.step()\n        # Clip weights of discriminator\n        for p in discriminator.parameters():\n            p.data.clamp_(-opt.clip_value, opt.clip_value)\n        # Train the generator every n_critic iterations\n        if i % opt.n_critic == 0:\n            # -----------------\n            #  Train Generator\n            # -----------------\n            optimizer_G.zero_grad()\n            # Generate a batch of images\n            gen_imgs = generator(z)\n            # Adversarial loss\n            loss_G = -torch.mean(discriminator(gen_imgs))\n            loss_G.backward()\n            optimizer_G.step()\n</code></pre> <p> Source: Arjovsky et al., \"Wasserstein GAN\" (2017) arXiv:1701.07875</p>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#2017-wgan-gp-improved-training-of-wasserstein-gans","title":"[2017] WGAN-GP: Improved Training of Wasserstein GANs","text":""},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#overall-introduction-gradient-penalty","title":"Overall Introduction:  gradient penalty","text":"<p>WGAN-GP replaces the weight clipping in the original WGAN by adding a gradient penalty term at the random interpolation points between the real and generated data, thereby achieving a more stable training process and better generation effects.</p> <p>Loss function in WGAN:  \\(\\(L = \\sup_{||f||_L \\leq 1} \\mathbb{E}_{x \\sim \\mathbb{P}_r}[D(x)] - \\mathbb{E}_{x \\sim \\mathbb{P}_g}[D(x)]\\)\\)</p> <p>Loss function in WGAN-GP:  \\(\\(L = \\mathbb{E}_{x\\sim P_r}[D(x)] - \\mathbb{E}_{x\\sim P_g}[D(x)] + \\lambda \\mathbb{E}_{\\hat{x}\\sim P_{\\hat{x}}}[(||\\nabla_{\\hat{x}}D(\\hat{x})||_2 - 1)^2]\\)\\)</p> <p>where: - Wasserstein Distance Term :\\(\\mathbb{E}_{x\\sim P_r}[D(x)] - \\mathbb{E}_{x\\sim P_g}[D(x)]\\)</p> <ul> <li> <p>measure distance between real and generated distributions</p> </li> <li> <p>Gradient Penalty Term: \\(\\lambda \\mathbb{E}_{\\hat{x}\\sim P_{\\hat{x}}}[(||\\nabla_{\\hat{x}}D(\\hat{x})||_2 - 1)^2]\\)</p> </li> <li> <p>\u03bb is penalty coefficient (typically 10)</p> </li> <li> <p>Ensures gradient norm is close to 1</p> </li> <li> <p>\\(\\hat{x}\\) is a random interpolation between real samples and generated samples:</p> </li> </ul>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#drawbacks-as-weight-clipping","title":"Drawbacks as weight-clipping","text":"<p> Source: Gulrajani et al., \"Improved Training of Wasserstein GANs\" (2017) arXiv:1704.00028</p>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#capacity-underuse","title":"Capacity underuse","text":"<ul> <li> <p>Main Issues:</p> </li> <li> <p>Theoretically, this critic should maintain unit gradient magnitudes everywhere, but when using weight clipping constraints, the critic in WGAN tends to learn overly simplistic functions. </p> </li> <li> <p>Experimental Validation:</p> </li> <li> <p>To verify this, we conducted experiments using the real distribution plus random noise as the generator's output. </p> </li> <li> <p>The results showed that critics with weight clipping indeed overlook the complex features of the data, learning only simple approximations. </p> </li> </ul>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#exploding-and-vanishing-gradients","title":"Exploding and vanishing gradients","text":"<ul> <li> <p>Main Issues:</p> </li> <li> <p>WGAN faces optimization challenges during training, caused by the interaction between weight constraints and the loss function. If the clipping threshold \\(c\\) is not carefully adjusted, it may lead to either vanishing or exploding gradients.</p> </li> <li> <p>Experimental Validation:</p> </li> <li> <p>Researchers conducted experiments on the Swiss Roll toy dataset using three different clipping thresholds: 0.1, 0.01, and 0.001. With weight clipping:</p> <ul> <li> <p>At \\(c=0.1\\), gradients exhibited exponential growth (red line going up).</p> </li> <li> <p>At \\(c=0.01\\) and \\(c=0.001\\), gradients exhibited exponential decay (purple and green lines going down).</p> </li> </ul> </li> </ul> <p>The two smaller graphs on the right show differences in weight distribution:</p> <ul> <li> <p>The upper graph: Weight clipping pushes weights toward two extreme values.</p> </li> <li> <p>The lower graph: Gradient penalty results in a more normal distribution of weights.</p> </li> </ul>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#loss-function_4","title":"Loss function","text":"<p>Training process:  Source: Gulrajani et al., \"Improved Training of Wasserstein GANs\" (2017) arXiv:1704.00028</p> <p>How does the Gradient Penalty term work in WGAN-GP\uff1f</p> <ol> <li> <p>The loss function of WGAN-GP:  \\(\\min_G \\max_D \\mathbb{E}_{x\\sim P_r}[D(x)] - \\mathbb{E}_{z\\sim P_z}[D(G(z))] + \\lambda \\mathbb{E}_{\\hat{x}\\sim P_{\\hat{x}}}[(||\\nabla_{\\hat{x}}D(\\hat{x})||_2 - 1)^2]\\)</p> </li> <li> <p>The core idea of the Gradient Penalty term is to enforce the 1-Lipschitz constraint on discriminator D across the sample space\uff1a \\(|D(x_1) - D(x_2)| \\leq |x_1 - x_2|\\)</p> </li> <li> <p>The 1-Lipschitz constraint above is equivalent to having the gradient norm of the discriminator not exceeding 1 at any point:  \\(||\\nabla_x D(x)||_2 \\leq 1\\)</p> </li> <li> <p>WGAN-GP enforces the gradient norm to be equal to 1, rather than less than or equal to 1, through the penalty term:\\(\\mathcal{L}_{GP} = \\lambda \\mathbb{E}_{\\hat{x}\\sim P_{\\hat{x}}}[(||\\nabla_{\\hat{x}}D(\\hat{x})||_2 - 1)^2]\\)</p> </li> <li> <p>\\(\\hat{x} = \\epsilon x + (1-\\epsilon)G(z), \\epsilon \\sim U[0,1]\\)</p> <ul> <li> <p>So \\(\\hat{x}\\) is a linear interpolation between data points of the real data distribution \\(P_r\\) and the generated data distribution \\(P_g\\).</p> </li> <li> <p>Why sampling?</p> </li> <li> <p>According to \\(||\\nabla_x D(x)||_2 \\leq 1\\), the optimal critic forms a line between the paired points of the real and generated distributions with a gradient norm of 1. Therefore, as a compromise, the constraint is only enforced along these sampled lines.</p> </li> <li> <p>Easy to implement and worked out in experiments.</p> </li> </ul> </li> </ol> <p>\\(\\hat{x}\\):</p> <pre><code>alpha = torch.rand(real_samples.size(0), 1, 1, 1)\ninterpolates = alpha * real_samples + (1 - alpha) * fake_samples\n</code></pre> <p>\\(\\nabla_{\\hat{x}}D(\\hat{x})\\):</p> <pre><code>#d(x)\nd_interpolates = D(interpolates)  \n# \\nabla_{\\hat{x}}D(\\hat{x}):\ngradients = autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=torch.ones_like(d_interpolates),\n        create_graph=True\n    )[0]\n</code></pre> <p>\\((||\\nabla_{\\hat{x}}D(\\hat{x})||_2 - 1)^2\\):</p> <pre><code>gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n</code></pre> <ol> <li> <p>The regulatory effect of this penalty term is manifested in:</p> </li> <li> <p>When \\(||\\nabla_{\\hat{x}}D(\\hat{x})||_2 &gt; 1\\), showing:</p> <ul> <li> <p>Too Steep Gradients: Discriminators tend to be \"aggressive\" in judging real/fake samples and change too rapidly </p> </li> <li> <p>May lead to training instability:</p> </li> <li> <p>Likely to cause discriminator overfitting</p> </li> <li> <p>Provides too strong gradient signals to the generator</p> </li> </ul> </li> <li> <p>When \\(||\\nabla_{\\hat{x}}D(\\hat{x})||_2 &lt; 1\\), showing:</p> <ul> <li> <p>Too Flat Gradients: The Discriminator tends to be  insensitive to input changes </p> </li> <li> <p>Insufficient Discrimination :  The Discriminator cannot effectively distinguish real/fake samples</p> </li> <li> <p>Vanishing Gradients: Generator might not receive effective training signals</p> </li> </ul> </li> <li> <p>Only when gradient norm  \\(||\\nabla_{\\hat{x}}D(\\hat{x})||_2 = 1\\), the penalty term becomes zero</p> </li> </ol> <pre><code>def compute_gradient_penalty(D, real_samples, fake_samples):\n    # Random interpolation coefficient\n    alpha = torch.rand(real_samples.size(0), 1, 1, 1)\n    # Generate interpolated samples\n    interpolates = alpha * real_samples + (1 - alpha) * fake_samples\n    interpolates.requires_grad_(True)\n    # Compute discriminator output\n    d_interpolates = D(interpolates)  \n    # Compute gradients\n    gradients = autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=torch.ones_like(d_interpolates),\n        create_graph=True\n    )[0]\n    # Compute gradient penalty\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty\n\n# Training loop\nfor epoch in epochs:\n    for real_data in dataloader:\n        # Compute gradient penalty\n        gradient_penalty = compute_gradient_penalty(D, real_data, fake_data)\n        # Discriminator loss\n        d_loss = -torch.mean(D(real_data)) + torch.mean(D(fake_data)) + lambda_gp * gradient_penalty\n</code></pre> <p> Source: Gulrajani et al., \"Improved Training of Wasserstein GANs\" (2017) arXiv:1704.00028</p> <p> Source: Gulrajani et al., \"Improved Training of Wasserstein GANs\" (2017) arXiv:1704.00028</p>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#2018-pggan-progressive-growing-of-gans-for-improved-quality-stability-and-variation","title":"[2018] PGGAN: Progressive Growing of GANs for Improved Quality, Stability, and Variation","text":""},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#overall-introduction_2","title":"Overall Introduction:","text":"<ol> <li> <p>Progressive Growing</p> </li> <li> <p>Core Idea: Start at low resolution and progressively increase to higher resolutions.</p> </li> <li> <p>Advantages:  More stable training &amp; Higher computational efficiency &amp; Better memory utilization</p> </li> <li> <p>Implementation: Smoothly fade in new layers and synchronous growth of the generator and discriminator</p> </li> <li> <p>Minibatch Standard Deviation </p> </li> <li> <p>Purpose: Increase the diversity of generated images &amp; prevents mode collapse.</p> </li> <li> <p>Implementation: </p> <ul> <li> <p>Introduce a statistical layer late in the discriminator</p> </li> <li> <p>Calculate the standard deviation within a minibatch of samples</p> </li> <li> <p>Concatenate statistical features with the original features</p> </li> </ul> </li> <li> <p>Normalization Strategies</p> </li> <li> <p>Purpose: strategies ensure underlying training stability.</p> </li> <li> <p>Implementation: </p> <ul> <li>Generator: Uses PixelNorm </li> </ul> </li> </ol> <p>The structure of PGGAN laid an important foundation for subsequent work (such as StyleGAN).</p>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#progressive-growing-of-gans","title":"PROGRESSIVE GROWING OF GANS","text":"<p> Source: Karras et al., \"Progressive Growing of GANs for Improved Quality, Stability, and Variation\" (2018) arXiv:1710.10196</p> <p> Source: Karras et al., \"Progressive Growing of GANs for Improved Quality, Stability, and Variation\" (2018) arXiv:1710.10196</p> <ul> <li> <p>Each resolution stage has two phases:</p> </li> <li> <p>Fade-in Phase:</p> <ul> <li> <p>The new layer is gradually blended in using alpha parameter</p> </li> <li> <p>Alpha increases linearly from 0 to 1</p> </li> <li> <p>In PGGAN, the growth of the \u03b1 parameter is linear and is controlled by the number of training iterations. This is achieved as follows:</p> </li> </ul> </li> </ul> <pre><code># \u5047\u8bbefade_in_iters\u662ffade-in\u9636\u6bb5\u7684\u603b\u8fed\u4ee3\u6b21\u6570\nfade_in_iters = 600000  # 600k images\n# \u5f53\u524d\u8fed\u4ee3\u6b21\u6570current_iter\nalpha = min(current_iter / fade_in_iters, 1.0)\n\ndef fade_in(self, alpha, upscaled, generated):\n    return torch.tanh(alpha * generated + (1-alpha) * upscaled)\n......\nfinal_upscaled = self.rgb_layers[steps-1](upscaled)\nfinal_out = self.rgb_layers[steps](out)\nreturn self.fade_in(alpha, final_upscaled, final_out)\n</code></pre> <ol> <li> <p>Stabilization Phase: </p> <ul> <li> <p>Train network with new layers fully active</p> </li> <li> <p>Old paths are removed</p> </li> <li> <p>Network stabilizes at new resolution</p> </li> </ul> </li> </ol> <p>Time Allocation:</p> <ul> <li> <p>Fade-in Phase: 600k images</p> </li> <li> <p>Stabilization Phase: 600k images</p> </li> <li> <p>Total per resolution: 1.2M images (600k + 600k)</p> </li> </ul> <pre><code>Complete Training Process Example (from 4\u00d74 to 1024\u00d71024):\n4\u00d74:   Only Stabilization     600k images\n8\u00d78:   Fade-in + Stabilization 1.2M images\n16\u00d716: Fade-in + Stabilization 1.2M images\n32\u00d732: Fade-in + Stabilization 1.2M images\n64\u00d764: Fade-in + Stabilization 1.2M images\n128\u00d7128: Fade-in + Stabilization 1.2M images\n256\u00d7256: Fade-in + Stabilization 1.2M images\n512\u00d7512: Fade-in + Stabilization 1.2M images\n1024\u00d71024: Fade-in + Stabilization 1.2M images\n</code></pre> <p>Smooth Layer Transitions:</p> <pre><code>def forward(self, x, alpha):\n    # Old path (lower resolution)\n    old_rgb = self.from_rgb_old(x)\n    old_rgb = self.upsample(old_rgb)\n\n    # New path (higher resolution)\n    new_x = self.upsample(x)\n    new_x = self.conv(new_x)\n    new_rgb = self.to_rgb_new(new_x)\n\n    # Smooth blending\n    return (1 - alpha) * old_rgb + alpha * new_rgb\n</code></pre> <ul> <li> <p>toRGB: 1\u00d71 convolution to convert features to RGB</p> </li> <li> <p>fromRGB: 1\u00d71 convolution to convert RGB to features</p> </li> <li> <p>2\u00d7: Upsampling (nearest neighbor)</p> </li> <li> <p>0.5\u00d7: Downsampling (average pooling)</p> </li> </ul>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#increasing-variation-using-minibatch-standard-deviation","title":"INCREASING VARIATION USING MINIBATCH STANDARD DEVIATION","text":"<p> Source: Wang et al., \"Citrus Disease Image Generation and Classification Based on Improved FastGAN and EfficientNet-B5\" (2023) Electronics, 12(5), 1232</p> <ol> <li>For each feature and spatial location i, compute standard deviation across the batch:   \\(\\sigma_i(x) = \\sqrt{\\frac{1}{N}\\sum_{k=1}^{N}(x_{ik} - \\mu_i)^2}\\) where:</li> <li> <p>\\(x_{ik}\\) is the feature value for sample k at position i</p> </li> <li> <p>\\(\\mu_i = \\frac{1}{N}\\sum_{k=1}^{N}x_{ik}\\) is the mean across the batch</p> </li> <li> <p>N is the batch size</p> </li> <li> <p>Average the standard deviations across features and spatial dimensions:   \\(\\sigma = \\frac{1}{C \\times H \\times W}\\sum_{i}\\sigma_i(x)\\)</p> </li> </ol> <p>where:</p> <ul> <li>\\(C\\) : channels. \\(H\\): height. \\(W\\) :width</li> </ul> <p>These statistics are then:</p> <ol> <li> <p>Replicated into a \\([1\u00d71\u00d7H\u00d7W]\\) tensor</p> </li> <li> <p>Further replicated N times to match batch size: \\([N\u00d71\u00d7H\u00d7W]\\)</p> </li> <li> <p>Concatenated with original input along channel dimension to get final output of shape \\([N\u00d7(C+1)\u00d7H\u00d7W]\\)</p> </li> </ol> <pre><code>def minibatch_stddev_layer(x, group_size=4):\n    N, C, H, W = x.shape\n    G = min(group_size, N)  # \u5206\u7ec4\u5927\u5c0f\n\n    # [NCHW] -&gt; [GMCHW] \u5206\u6210G\u7ec4\n    y = x.reshape(G, -1, C, H, W)\n\n    # \u8ba1\u7b97\u6807\u51c6\u5dee\n    y = torch.sqrt(y.var(0) + 1e-8)  # [-MCHW]\n\n    # \u53d6\u5e73\u5747\u5f97\u5230\u5355\u4e2a\u503c\n    y = y.mean(dim=[0,1,2,3])  # []\n\n    # \u5e7f\u64ad\u56de\u539f\u59cb\u5f62\u72b6\n    y = y.reshape(1, 1, 1, 1)\n    y = y.repeat(N, 1, H, W)  # [N1HW]\n\n    # \u8fde\u63a5\u5230\u8f93\u5165\u7279\u5f81\u56fe\n    return torch.cat([x, y], 1)  # [N(C+1)HW]\n</code></pre> <p>The main advantages of this technique are:</p> <ol> <li>Helps the discriminator identify the statistical characteristics of the generated images</li> <li>Encourages the generator to produce more diverse outputs</li> <li>Helps avoid mode collapse</li> </ol>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#normalization-in-generator-and-discriminator","title":"NORMALIZATION IN GENERATOR AND DISCRIMINATOR","text":""},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#normalization-in-passed-gan-related-model","title":"Normalization in passed GAN-related-model","text":"GAN model Normalization applied Implementation detail GAN batch normalization Typically employs basic batch normalization in both the generator and discriminator. cGAN batch normalization Typically employs basic batch normalization in both the generator and discriminator. DCGAN batch normalization Generator: BN is used in all layers except the output layer. Discriminator: BN is used in all layers except the input and output layers. WGAN Remove BN in the discriminator Advises against using batch normalization due to its impact on the Lipschitz constraint. Completely removes BN in the discriminator. The generator may use BN, but it is often omitted in practice. WGAN-GP Remove BN in the discriminator Discriminator: Recommends using layer normalization or instance normalization instead of batch normalization. This is because BN introduces correlations between samples, affecting the calculation of the gradient penalty. PGGAN Completely Remove BN and Pixel-wise normalization in the generator and new weight initialization Generator: BN is used in all layers except the output layer. Discriminator: BN is used in all layers except the input and output layers."},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#pixelwise-feature-vector-normalization-in-generator","title":"PIXELWISE FEATURE VECTOR NORMALIZATION IN GENERATOR","text":"<p>Applied after each convolutional layer in the generator at each pixel position independently:</p> \\[b_{x,y} = \\frac{a_{x,y}}{\\sqrt{\\frac{1}{N}\\sum_{j=0}^{N-1}(a_{x,y}^j)^2 + \\epsilon}}\\] <p>Where: - \\(\\epsilon = 10^{-8}\\)</p> <ul> <li> <p>\\(a_{x,y} \\text{ is the original feature vector at pixel position } (x,y)\\)</p> </li> <li> <p>\\(b_{x,y} \\text{ is the normalized feature vector at pixel position } (x,y)\\)</p> </li> <li> <p>\\(N \\text{ is the number of feature maps (channels)}\\)</p> </li> <li> <p>\\(\\epsilon = 10^{-8} \\text{ is a small constant to prevent division by zero}\\)</p> </li> <li> <p>\\(\\text{The sum } \\sum_{j=0}^{N-1} \\text{ is taken over all } N \\text{ feature maps for that pixel position}\\)</p> </li> </ul> <pre><code>class PixelNorm(nn.Module):\n    def forward(self, x):\n        norm = torch.mean(x ** 2, dim=1, keepdim=True)\n        norm = torch.sqrt(norm + self.epsilon)\n        return x / norm    \n</code></pre> <ul> <li> <p>\\(x ** 2\\):  Calculate the squared values of all features at each pixel \\((a_{x,y}^j)^2\\)</p> </li> <li> <p><code>torch.mean(..., dim=1)</code>:  Average these squares across all feature maps \\({\\frac{1}{N}\\sum_{j=0}^{N-1}(a_{x,y}^j)^2 }\\)</p> </li> <li> <p>torch.sqrt(... + epsilon): Take the square root of the average (plus \u03b5) \\({\\sqrt{\\frac{1}{N}\\sum_{j=0}^{N-1}(a_{x,y}^j)^2 + \\epsilon}}\\)</p> </li> <li> <p>\\(x / norm\\):  normalization \\(\\frac{a_{x,y}}{\\sqrt{\\frac{1}{N}\\sum_{j=0}^{N-1}(a_{x,y}^j)^2 + \\epsilon}}\\)$</p> </li> </ul>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#qualized-learning-rate","title":"QUALIZED LEARNING RATE","text":"<p>Problem: In traditional neural network training, parameters of different layers may have different dynamic ranges. When using adaptive optimizers like RMSProp or Adam, they normalize gradient updates based on the standard deviation of parameters. This results in parameters with larger dynamic ranges requiring more time to adjust properly.</p> <p>Specific Implementation:</p> <ol> <li>Traditional Weight Initialization </li> <li> <p>In standard neural networks, weights are typically initialized using methods </p> <ul> <li>like He initialization N(0, sqrt(2/n)), Better suited for ReLU</li> </ul> </li> </ol> <pre><code># Standard He initialization (for comparison)\nweight_shape = (out_channels, in_channels, kernel, kernel)\nstd = np.sqrt(2.0 / (in_channels * kernel * kernel))\nweights = np.random.normal(0, std, weight_shape)\n</code></pre> <ul> <li> <p>This can lead to different layers learning at different rates, causing training instability</p> </li> <li> <p>The variance of the gradients can differ significantly between layers</p> </li> <li> <p>The Equalized Learning Rate Solution: </p> </li> <li> <p>initialization (happens only once when the model is created):</p> <ul> <li>Instead of using usual standard initialization, weights are initialized from N(0,1)</li> </ul> </li> </ul>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#equalized-learning-rate-approach","title":"Equalized learning rate approach","text":"<pre><code>weights = np.random.normal(0, 1.0, weight_shape)\nruntime_coef = std  # Applied during forward pass\n</code></pre> <ul> <li> <p>During training (happens every forward pass): scaling process</p> </li> <li> <p>Each layer's weights are explicitly scaled during runtime by a layer-specific constant </p> </li> <li> <p>The scaling factor is the per-layer normalization constant from He initialization</p> </li> </ul>"},{"location":"book/chapter3_GAN/3.1from_gan_to_stylegan/paper/#in-equalizedconv2d","title":"In EqualizedConv2d","text":"<pre><code>self.scale = np.sqrt(2) / np.sqrt(fan_in)  # Calculate He scaling factor\nscaled_weight = self.weight * self.scale    # Apply scaling at runtime\n</code></pre> <ul> <li>This ensures that the dynamic range and learning speed are similar for all weights</li> <li>Standard layers: Apply scaling during initialization<ul> <li>Equalized layers: Apply scaling during each forward pass</li> <li>This ensures the gradient updates remain properly scaled throughout training</li> </ul> </li> </ul> <p>Why It's Useful:</p> <ol> <li> <p>Ensures that all weights have a similar dynamic range.</p> </li> <li> <p>Makes the learning process more balanced, avoiding slow learning for some parameters due to large ranges.</p> </li> <li> <p>Better adapts to c</p> </li> <li> <p>In standard neural networks, weights are typically initialized using methods </p> </li> <li> <p>like He initialization \\(N(0, sqrt(2/n))\\), Better suited for ReLU</p> </li> </ol> <p>RMSProp or Adam, they normalize gradient updates based on the standard deviation of parameters\uff1a</p> <p>RMSprop: \\(\\(\\theta_t = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{v_t + \\epsilon}} g_t\\)\\) Adam:\\(\\(\\theta_t = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\\)\\)</p> <ol> <li>Standard Deviation Estimation:</li> <li> <p>\\(v_t\\) actually estimates the exponential moving average of squared gradients:\\(v_t = \\beta v_{t-1} + (1-\\beta)g_t^2\\)</p> </li> <li> <p>This accumulated squared gradient \\(v_t\\) is essentially estimating the second moment of the gradient, and its square root \\(\\sqrt v_t\\) approximates the standard deviation of the gradient</p> </li> <li> <p>Normalization Effect:</p> </li> <li> <p>In the update formula, the gradient term (\\(g_t\\) or \\(\u0125_t\\)) is divided by \\(\\sqrt v_t\\)</p> </li> <li> <p>This is equivalent to normalizing the gradient update by the gradient's standard deviation</p> </li> <li> <p>Mathematically equivalent to:\\(\\(\\text{normalized update} = \\frac{g_t}{\\sqrt{v_t + \\epsilon}}\\)\\)</p> </li> <li> <p>Why This Is Standard Deviation Normalization:</p> </li> <li> <p>If a parameter has large gradient variations (high standard deviation), \\(\\sqrt v_t\\) will become larger</p> </li> <li> <p>This will make the actual update step smaller</p> </li> <li> <p>Conversely, if gradient variations are small (low standard deviation), the update step will become larger accordingly</p> </li> <li> <p>This achieves adaptive standard deviation normalization</p> <ul> <li>This is also why EQUALIZED LEARNING RATE solves this problem by explicitly controlling the dynamic range of parameters (\\(\u0175\u1d62 = w\u1d62/c\\)).</li> </ul> </li> </ol> <p>Main Advantages:</p> <ol> <li> <p>Keeps the learning speed consistent for all weights.</p> </li> <li> <p>Avoids the issue of having both too high and too low learning rates at the same time.</p> </li> <li> <p>By dynamically scaling during runtime rather than statically at initialization, it makes the training process more stable.</p> </li> </ol>"},{"location":"book/chapter3_GAN/3.3stylegan/paper/","title":"StyleGAN: A Style-Based Generator Architecture for Generative Adversarial Networks","text":"<p>Paper Link </p>"},{"location":"book/chapter3_GAN/3.3stylegan/paper/#_1","title":"\u6838\u5fc3\u521b\u65b0","text":"<ol> <li>\u57fa\u4e8e\u98ce\u683c\u7684\u751f\u6210\u5668\u67b6\u6784 </li> <li>\u5c06\u4f20\u7edf\u7684\u8f93\u5165\u5c42\u66ff\u6362\u4e3a\u5e38\u91cf\u8f93\u5165</li> <li>\u901a\u8fc7AdaIN\u5c42\u5c06\u6f5c\u5728\u7f16\u7801\u6ce8\u5165\u5230\u4e0d\u540c\u7684\u5c42\u7ea7</li> <li> <p>\u5b9e\u73b0\u4e86\u5bf9\u4e0d\u540c\u5c3a\u5ea6\u7279\u5f81\u7684\u7cbe\u786e\u63a7\u5236</p> </li> <li> <p>\u4e2d\u95f4\u6f5c\u5728\u7a7a\u95f4W</p> </li> <li>\u5f15\u5165\u6620\u5c04\u7f51\u7edcF\u5c06\u6f5c\u5728\u7a7a\u95f4Z\u6620\u5c04\u5230\u4e2d\u95f4\u6f5c\u5728\u7a7a\u95f4W</li> <li>W\u7a7a\u95f4\u5177\u6709\u66f4\u597d\u7684\u89e3\u8026\u7279\u6027</li> <li> <p>\u652f\u6301\u66f4\u597d\u7684\u7279\u5f81\u6df7\u5408\u548c\u7f16\u8f91\u64cd\u4f5c</p> </li> <li> <p>\u81ea\u9002\u5e94\u5b9e\u4f8b\u5f52\u4e00\u5316(AdaIN)</p> </li> <li>\u901a\u8fc7\u98ce\u683c\u5411\u91cf\u8c03\u5236\u6bcf\u4e2a\u901a\u9053\u7684\u5747\u503c\u548c\u65b9\u5dee</li> <li>\u5b9e\u73b0\u4e86\u5bf9\u7279\u5b9a\u7279\u5f81\u7684\u7cbe\u786e\u63a7\u5236</li> </ol> <p><code>python    x = x * (style_split[:, 0] + 1) + style_split[:, 1]</code></p> <ul> <li>\u6bcf\u4e2achannel\u7684feature map\u663e\u5f0f\u8c03\u8282\u65b9\u5dee\u548c\u5747\u503c\uff0c\u8fd9\u4e24\u4e2a\u7edf\u8ba1\u91cf\u8868\u793a\u56fe\u7247\u7684\u98ce\u683c</li> </ul>"},{"location":"book/chapter3_GAN/3.3stylegan/paper/#_2","title":"\u6280\u672f\u7ec6\u8282","text":"<ol> <li>LOD (Level of Detail) \u8ba1\u7b97:</li> <li>\u76ee\u7684: \u63a7\u5236\u8bad\u7ec3\u671f\u95f4\u7684\u5206\u8fa8\u7387\u7ea7\u522b\uff0c\u652f\u6301\u6e10\u8fdb\u5f0f\u589e\u957f\u3002</li> <li>\u8ba1\u7b97:<ul> <li><code>lod</code> \u57fa\u4e8e\u5df2\u89c1\u56fe\u50cf\u6570\u91cf (<code>seen_img</code>) \u548c\u6bcf\u4e2a\u7ea7\u522b\u7684\u6301\u7eed\u65f6\u95f4 (<code>lod_duration</code>) \u8ba1\u7b97\u3002</li> <li>\u968f\u8bad\u7ec3\u8fdb\u5c55\u800c\u51cf\u5c11\uff0c\u5e73\u6ed1\u8fc7\u6e21\u4e8e\u4e0d\u540c\u5206\u8fa8\u7387\u3002</li> </ul> </li> <li> <p>\u9636\u6bb5:</p> <ul> <li>\u7a33\u5b9a\u9636\u6bb5\uff1a<code>lod</code> \u4fdd\u6301\u4e0d\u53d8\u3002</li> <li>\u8fc7\u6e21\u9636\u6bb5\uff1a<code>lod</code> \u53d6\u5206\u6570\u503c\uff0c\u5e73\u6ed1\u5206\u8fa8\u7387\u53d8\u5316\u3002</li> </ul> </li> <li> <p>\u5408\u6210\u7f51\u7edc (Synthesis Network):</p> </li> <li>\u7ed3\u6784: \u7531\u591a\u4e2a\u5206\u8fa8\u7387\u5c42\u7ec4\u6210\uff0c\u6bcf\u5c42\u5305\u542b\u4e24\u4e2a\u5377\u79ef\u5757\u3002</li> <li>\u5206\u8fa8\u7387\u5c42:<ul> <li>\u521d\u59cb\u5c42: \u5c06\u5e38\u91cf\u8f93\u5165\u8f6c\u6362\u4e3a\u7279\u5f81\u56fe\u3002</li> <li>\u540e\u7eed\u5c42: \u6bcf\u5c42\u5206\u8fa8\u7387\u52a0\u500d\uff0c\u901a\u9053\u51cf\u534a\uff0c\u4fdd\u6301\u53c2\u6570\u4e00\u81f4\u3002</li> </ul> </li> <li> <p>\u5377\u79ef\u5757 (ConvBlock):</p> <ul> <li>\u4e0a\u91c7\u6837: \u4f7f\u7528\u6700\u8fd1\u90bb\u63d2\u503c\u589e\u52a0\u5206\u8fa8\u7387\u3002</li> <li>\u5377\u79ef: \u53d7 PGGAN \u542f\u53d1\u7684\u7f29\u653e\u6743\u91cd\uff0c\u7a33\u5b9a\u8bad\u7ec3\u3002</li> <li>\u566a\u58f0\u6dfb\u52a0: \u7a7a\u95f4\u6216\u901a\u9053\u566a\u58f0\uff0c\u589e\u5f3a\u7ec6\u8282\u591a\u6837\u6027\u3002</li> <li>\u6fc0\u6d3b: LeakyReLU \u9632\u6b62\u795e\u7ecf\u5143\u6b7b\u4ea1\uff0c\u6539\u5584\u68af\u5ea6\u6d41\u52a8\u3002</li> <li>\u5f52\u4e00\u5316: \u5b9e\u4f8b\u5f52\u4e00\u5316\u6807\u51c6\u5316\u7279\u5f81\u56fe\u3002</li> <li>\u98ce\u683c\u5316: \u4f7f\u7528\u5b66\u4e60\u7684\u98ce\u683c\u5411\u91cf\u8c03\u5236\u7279\u5f81\u56fe\uff0c\u8c03\u6574\u6bcf\u4e2a\u7279\u5f81\u56fe\u7684\u5747\u503c\u548c\u65b9\u5dee\u3002</li> </ul> </li> <li> <p>\u98ce\u683c\u8c03\u5236 (Style Modulation):</p> </li> <li>\u8fc7\u7a0b: \u6bcf\u4e2a\u7279\u5f81\u56fe\u901a\u9053\u4f7f\u7528\u98ce\u683c\u5411\u91cf\u8c03\u5236\u3002</li> <li> <p>\u65b9\u7a0b:</p> \\[  x = x \\times (\\text{style\\_split}[:, 0] + 1) + \\text{style\\_split}[:, 1] \\] </li> <li> <p>\u76ee\u7684: \u663e\u5f0f\u8c03\u6574\u7279\u5f81\u56fe\u65b9\u5dee\u548c\u5747\u503c\uff0c\u8868\u793a\u56fe\u7247\u98ce\u683c\u3002</p> </li> <li> <p>\u622a\u65ad\u6280\u5de7 (Truncation Trick):</p> </li> <li>\u76ee\u7684: \u63a7\u5236\u751f\u6210\u56fe\u50cf\u7684\u591a\u6837\u6027\u4e0e\u7a33\u5b9a\u6027\u3002</li> <li> <p>\u8fc7\u7a0b: \u4f7f\u7528\u4e2d\u5fc3\u5e73\u5747\u503c (<code>w_{avg}</code>) \u548c\u7f29\u653e\u56e0\u5b50 (<code>trunc_psi</code>) \u8c03\u6574\u98ce\u683c\u5411\u91cf\u3002\u5b83\u66f4\u76f8\u5f53\u4e8e\u4e00\u4e2a\u7f29\u653e\u64cd\u4f5c\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u610f\u4e49\u4e0a\u7684\u622a\u65ad\u64cd\u4f5c\u3002</p> </li> <li> <p>\u566a\u58f0\u6ce8\u5165 (Noise Injection):</p> </li> <li>\u7c7b\u578b: \u7a7a\u95f4\u6216\u901a\u9053\u3002</li> <li>\u76ee\u7684: \u4e3a\u7279\u5f81\u56fe\u6dfb\u52a0\u968f\u673a\u53d8\u5316\uff0c\u589e\u5f3a\u7ec6\u8282\u591a\u6837\u6027\u3002</li> </ol>"},{"location":"book/chapter3_GAN/3.3stylegan/paper/#_3","title":"\u4ee3\u7801\u89e3\u8bfb","text":""},{"location":"book/chapter3_GAN/3.3stylegan/paper/#1-generator","title":"1. Generator","text":"<pre><code>\n    def forward(self,\n                z,\n                label=None,\n                lod=None,\n                w_moving_decay=0.995,\n                style_mixing_prob=0.9,\n                trunc_psi=None,\n                trunc_layers=None,\n                randomize_noise=False,\n                **_unused_kwargs):\n        mapping_results = self.mapping(z, label) # label \u662f\u7528\u4e8e\u6761\u4ef6\u751f\u6210\u7684\u6807\u7b7e\n        w = mapping_results['w'] #  W\u7a7a\u95f4\n        # w shape : [batch_size, w_space_dim] = [B,512]\n\n        if self.training and w_moving_decay &lt; 1:\n            batch_w_avg = all_gather(w).mean(dim=0)\n            self.truncation.w_avg.copy_(\n                self.truncation.w_avg * w_moving_decay +\n                batch_w_avg * (1 - w_moving_decay))\n            # \u8ba1\u7b97\u4e00\u4e2aW\u7a7a\u95f4\u4e2d\u7684\u4e2d\u5fc3\u70b9,\u7528\u4e8e\u622a\u65ad\u64cd\u4f5c\n        if self.training and style_mixing_prob &gt; 0:\n            new_z = torch.randn_like(z)\n            new_w = self.mapping(new_z, label)['w']\n            lod = self.synthesis.lod.cpu().tolist() if lod is None else lod\n            # \u8fd9\u91cc\u7684 tolist() \u53ef\u80fd\u9020\u6210\u6df7\u6dc6\uff0c\u4f46\u5bf9\u4e8e0\u7ef4tensor\uff08\u6807\u91cftensor\uff09\uff0c\u5b83\u8fd4\u56de\u7684\u662f\u4e00\u4e2a\u6807\u91cf\u503c\u3002\u4f8b\u5982\uff1a\n            # \u6240\u4ee5\u5728\u8fd9\u91cc\uff0clod \u5c31\u662f\u4e00\u4e2a\u6570\u503c\uff0c\u6bd4\u5982 0\u30011\u30012 \u6216\u8005 1.5 \u8fd9\u6837\u7684\u503c\uff0c\u7528\u4e8e\u63a7\u5236\u751f\u6210\u56fe\u50cf\u7684\u5206\u8fa8\u7387\u7ea7\u522b\u3002\u8fd9\u4e2a\u503c\uff1a\n\n            # \u53ef\u4ee5\u662f\u6574\u6570\uff1a\u76f4\u63a5\u5bf9\u5e94\u67d0\u4e2a\u5206\u8fa8\u7387\u7ea7\u522b\n            # \u4e5f\u53ef\u4ee5\u662f\u5c0f\u6570\uff1a\u8868\u793a\u5728\u4e24\u4e2a\u5206\u8fa8\u7387\u7ea7\u522b\u4e4b\u95f4\u8fdb\u884c\u63d2\u503c\n\n            current_layers = self.num_layers - int(lod) * 2\n            if np.random.uniform() &lt; style_mixing_prob:\n                mixing_cutoff = np.random.randint(1, current_layers)\n                # \u524d\u9762\u7684current_layers \u8868\u793a\u5f53\u524d\u7684\u5206\u8fa8\u7387\u7ea7\u522b\n                # \u540e\u9762\u7684 mixing_cutoff \u8868\u793a\u622a\u65ad\u70b9\n                # \u8fd9\u91cc\u7684 np.random.randint(1, current_layers) \u8868\u793a\u968f\u673a\u9009\u62e9\u4e00\u4e2a\u622a\u65ad\u70b9\n\n                w = self.truncation(w)\n                new_w = self.truncation(new_w)\n                w[:, mixing_cutoff:] = new_w[:, mixing_cutoff:]\n                # \u524d\u534a\u90e8\u5206\u7684 W \u7a7a\u95f4\u4e0d\u53d8\uff0c\u540e\u534a\u90e8\u5206\u7684 W \u7a7a\u95f4\u8fdb\u884c\u4e86\u66ff\u6362\n\n        wp = self.truncation(w, trunc_psi, trunc_layers)\n        # wp \u4e5f\u8fdb\u884c\u4e86\u622a\u65ad\u3002\n        # \u5f62\u72b6\u53d8\u5316  \uff1a\n        # \u8f93\u5165 w: [batch_size, w_space_dim] = [B, 512]\n        # \u8f93\u51fa wp: [batch_size, num_layers, w_space_dim] = [B, num_layers, 512]\n        synthesis_results = self.synthesis(wp, lod, randomize_noise)\n\n        return {**mapping_results, **synthesis_results}\n</code></pre> <p>generator \u5206\u6210\u51e0\u4e2a\u6b65\u9aa4</p>"},{"location":"book/chapter3_GAN/3.3stylegan/paper/#1-mapping-network","title":"1. \u6620\u5c04\u7f51\u7edc (Mapping Network)","text":"<pre><code>mapping_results = self.mapping(z, label) # label \u662f\u7528\u4e8e\u6761\u4ef6\u751f\u6210\u7684\u6807\u7b7e\nw = mapping_results['w']\n</code></pre> <p>\u8f93\u5165 :</p> <ul> <li>\\(z\\): \u6f5c\u5728\u5411\u91cf\uff0c\u901a\u5e38\u662f\u4ece\u6807\u51c6\u6b63\u6001\u5206\u5e03\u4e2d\u91c7\u6837\uff0c\u5f62\u72b6\u4e3a \\([B, z\\_dim]\\)</li> <li>\\(\\text{label}\\): \u6761\u4ef6\u751f\u6210\u7684\u6807\u7b7e\uff0c\u53ef\u9009\uff0c\u7528\u4e8e\u63a7\u5236\u751f\u6210\u5185\u5bb9\u3002</li> <li>\u8fc7\u7a0b :</li> <li> <p>\u4f7f\u7528\u4e00\u4e2a\u591a\u5c42\u611f\u77e5\u673a (MLP) \u5c06\u6f5c\u5728\u5411\u91cf \\(z\\) \u6620\u5c04\u5230\u6837\u5f0f\u7a7a\u95f4 \\(w\\)\uff1a</p> \\[w = f(z) \\] <p>\u5176\u4e2d \\(f\\) \u662f\u6620\u5c04\u7f51\u7edc\uff0c\u8f93\u51fa \\(w\\) \u7684\u5f62\u72b6\u4e3a \\([B, w\\_dim]\\)\uff0c  \u901a\u5e38 \\(w\\_dim = 512\\)\u3002 - \u8f93\u51fa :   - \\(w\\): \u6837\u5f0f\u5411\u91cf\uff0c\u4f5c\u4e3a\u540e\u7eed\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u7279\u5f81\u63a7\u5236\u53c2\u6570\u3002</p> </li> </ul>"},{"location":"book/chapter3_GAN/3.3stylegan/paper/#2-w","title":"2. W \u7a7a\u95f4\u5747\u503c\u7684\u79fb\u52a8\u5e73\u5747\u66f4\u65b0","text":"<pre><code>if self.training and w_moving_decay &lt; 1:\n    batch_w_avg = all_gather(w).mean(dim=0)\n    self.truncation.w_avg.copy_(\n        self.truncation.w_avg * w_moving_decay +\n        batch_w_avg * (1 - w_moving_decay))\n</code></pre> <ul> <li>\u76ee\u7684 :</li> <li> <p>\u5728\u8bad\u7ec3\u9636\u6bb5\uff0c\u8ba1\u7b97 \\(w\\) \u7684\u5747\u503c\u5e76\u8fdb\u884c\u79fb\u52a8\u5e73\u5747\u66f4\u65b0\uff0c\u5f97\u5230\u6837\u5f0f\u7a7a\u95f4\u7684\u4e2d\u5fc3\u70b9 \\(w_{avg}\\)\u3002</p> </li> <li> <p>\\(w_{avg}\\) \u7528\u4e8e\u540e\u7eed\u7684\u622a\u65ad\u64cd\u4f5c\uff08truncation trick\uff09\uff0c\u4ee5\u63a7\u5236\u751f\u6210\u5185\u5bb9\u7684\u591a\u6837\u6027\u548c\u5e73\u8861\u6027\u3002</p> </li> <li> <p>\u8fc7\u7a0b :</p> </li> <li> <p>\u8ba1\u7b97\u5f53\u524d\u6279\u6b21\u7684\u6837\u5f0f\u5411\u91cf\u5e73\u5747\u503c \\(\\text{batch\\_w\\_avg}\\)\u3002</p> </li> <li> <p>\u4f7f\u7528\u6307\u6570\u79fb\u52a8\u5e73\u5747 (EMA) \u66f4\u65b0\u5168\u5c40\u6837\u5f0f\u5747\u503c \\(w\\_avg\\):</p> \\[ w_{avg} \\gets w_{avg} \\cdot \\text{decay} + \\text{batch-w-avg} \\cdot (1 - \\text{decay}) \\] </li> </ul>"},{"location":"book/chapter3_GAN/3.3stylegan/paper/#3-style-mixing","title":"3. \u6837\u5f0f\u6df7\u5408 (Style Mixing)","text":"<pre><code>if self.training and style_mixing_prob &gt; 0:\n    new_z = torch.randn_like(z)\n    new_w = self.mapping(new_z, label)['w']\n    lod = self.synthesis.lod.cpu().tolist() if lod is None else lod\n    current_layers = self.num_layers - int(lod) * 2\n    if np.random.uniform() &lt; style_mixing_prob:\n        mixing_cutoff = np.random.randint(1, current_layers)\n        # \u524d\u9762\u7684current_layers \u8868\u793a\u5f53\u524d\u7684\u5206\u8fa8\u7387\u7ea7\u522b\n        # \u540e\u9762\u7684 mixing_cutoff \u8868\u793a\u622a\u65ad\u70b9\n        # \u8fd9\u91cc\u7684 np.random.randint(1, current_layers) \u8868\u793a\u968f\u673a\u9009\u62e9\u4e00\u4e2a\u622a\u65ad\u70b9\n\n        w = self.truncation(w)\n        new_w = self.truncation(new_w)\n        w[:, mixing_cutoff:] = new_w[:, mixing_cutoff:]\n        # \u524d\u534a\u90e8\u5206\u7684 W \u7a7a\u95f4\u4e0d\u53d8\uff0c\u540e\u534a\u90e8\u5206\u7684 W \u7a7a\u95f4\u8fdb\u884c\u4e86\u66ff\u6362\n\n        # \u95ee\u9898\uff1a\u540c\u4e00\u4e2abatch\u91cc\u7684mixing_point\u662f\u4e00\u6837\u7684\u3002\u5982\u679c\u5bf9\u540c\u4e00\u4e2abatch\u91cc\u7684\u4e0d\u540c\u6837\u672c\u505a\u4e0d\u4e00\u6837\u7684\u6837\u5f0f\u6df7\u5408\uff0c\u4f1a\u4e0d\u4f1a\u6709\u95ee\u9898\uff1f\n\n#### 4. \u622a\u65ad\u64cd\u4f5c (Truncation Trick)\n```python\nwp = self.truncation(w, trunc_psi, trunc_layers)\n</code></pre> <p>\u5728self.truncation\u91cc\u9762 \u4f1a\u5148\u5bf9\\(w\\)\u8fdb\u884c\u590d\u5236\uff0c\u590d\u5236\u540e\u7684shape \u4e3a \\([B, \\text{num\\_layers}, w\\_dim]\\)\uff0c \u6bcf\u4e00\u5c42\u7684\u6837\u5f0f\u5411\u91cf\u90fd\u662f\u76f8\u540c\u7684\u3002 \u7136\u540e\u518d\u8fdb\u884c\u622a\u65ad\u64cd\u4f5c.</p> <pre><code>#self traucation\nif w.ndim == 2:\n    if self.repeat_w and w.shape[1] == self.w_space_dim:\n        w = w.view(-1, 1, self.w_space_dim)\n        wp = w.repeat(1, self.num_layers, 1)\n    else:\n        assert w.shape[1] == self.w_space_dim * self.num_layers\n        wp = w.view(-1, self.num_layers, self.w_space_dim)\n</code></pre> <ul> <li>\u76ee\u7684 :</li> <li> <p>\u63a7\u5236\u751f\u6210\u5185\u5bb9\u7684\u591a\u6837\u6027\u4e0e\u7a33\u5b9a\u6027\u3002</p> </li> <li> <p>\u8fc7\u7a0b :</p> </li> <li> <p>\u4f7f\u7528\u6837\u5f0f\u7a7a\u95f4\u7684\u4e2d\u5fc3\u70b9 \\(w\\_avg\\) \u548c\u622a\u65ad\u56e0\u5b50 \\(\\psi\\) \u5bf9\u6837\u5f0f\u5411\u91cf \\(w\\) \u8fdb\u884c\u7f29\u653e\uff1a</p> \\[ w'_i = w_{avg} + \\psi \\cdot (w_i - w_{avg}), \\quad \\text{for layers } i &lt; \\text{trunclayers} \\] </li> <li> <p>\u622a\u65ad\u64cd\u4f5c\u53ef\u4ee5\u51cf\u5c0f\u6837\u5f0f\u7a7a\u95f4\u4e2d\u6837\u5f0f\u5411\u91cf\u7684\u504f\u79bb\u7a0b\u5ea6\uff0c\u4ece\u800c\u751f\u6210\u66f4\u7a33\u5b9a\u7684\u56fe\u50cf\u3002</p> </li> <li>\u8f93\u51fa :</li> <li>\\(wp\\): \u6bcf\u4e00\u5c42\u7684\u6837\u5f0f\u5411\u91cf\uff0c\u5f62\u72b6\u4e3a \\([B, \\text{num\\_layers}, w\\_dim]\\)\u3002</li> <li> <p>\u95ee\u9898 : \u4e25\u683c\u6765\u8bf4\uff0c\u8fd9\u4e2a\u64cd\u4f5c\u786e\u5b9e\u66f4\u63a5\u8fd1\u4e8e \u7f29\u653e (scaling)  \u800c\u4e0d\u662f\u4f20\u7edf\u610f\u4e49\u4e0a\u7684 \u622a\u65ad (truncation) \uff0c\u5c3d\u7ba1\u5728 StyleGAN \u7684\u672f\u8bed\u4e2d\u88ab\u79f0\u4e3a \"truncation trick\"\u3002</p> <p>\u4e3a\u4ec0\u4e48\u662f\u7f29\u653e\uff1f \u622a\u65ad\u64cd\u4f5c\u7684\u516c\u5f0f\u4e3a\uff1a</p> \\[ wp = w_{avg} + (wp - w_{avg}) \\cdot \\text{trunc-psi} \\] <p>\u89c2\u5bdf\u516c\u5f0f\u53ef\u4ee5\u770b\u51fa\uff1a 1. \u6837\u5f0f\u5411\u91cf \\(wp\\) \u88ab\u91cd\u65b0\u8c03\u6574\u5230\u4e00\u4e2a\u4ee5 \\(w_{avg}\\) \u4e3a\u4e2d\u5fc3\u7684\u8303\u56f4\u5185\u3002</p> <ol> <li>\\(\\text{trunc-psi}\\) \u662f\u4e00\u4e2a\u7f29\u653e\u56e0\u5b50\uff0c\u51b3\u5b9a\u4e86 \\(wp\\) \u504f\u79bb \\(w_{avg}\\) \u7684\u7a0b\u5ea6\u3002</li> </ol> <p>\u7269\u7406\u610f\u4e49 \uff1a   - \u5982\u679c \\(\\text{trunc-psi} &lt; 1\\)\uff0c\u6837\u5f0f\u5411\u91cf\u7684\u504f\u79bb\u88ab\u7f29\u5c0f\uff0c\u751f\u6210\u7684\u56fe\u50cf\u66f4\u63a5\u8fd1\u4e8e\u5168\u5c40\u6837\u5f0f\u7684\u201c\u5e73\u5747\u503c\u201d\u3002</p> </li> <li> <p>\u5982\u679c \\(\\text{trunc-psi} &gt; 1\\)\uff0c\u6837\u5f0f\u5411\u91cf\u7684\u504f\u79bb\u88ab\u653e\u5927\uff0c\u751f\u6210\u7684\u56fe\u50cf\u4f1a\u66f4\u6781\u7aef\uff0c\u66f4\u504f\u79bb\u5e73\u5747\u98ce\u683c\u3002</p> <p>\u4f20\u7edf\u622a\u65ad (Clipping) \u5728\u4f20\u7edf\u7684\u201c\u622a\u65ad\u201d\u6982\u5ff5\u4e2d\uff0c\u6211\u4eec\u4f1a\u5c06\u67d0\u4e9b\u8d85\u51fa\u6307\u5b9a\u8303\u56f4\u7684\u503c \u76f4\u63a5\u9650\u5236  \u5728\u8fd9\u4e2a\u8303\u56f4\u5185\uff0c\u4f8b\u5982\uff1a$ x = \\max(\\min(x, \\text{upper_bound}), \\text{lower_bound}) $</p> <p>leGAN \u7684\u8fd9\u4e2a\u5b9e\u73b0\u4e2d\uff1a</p> </li> <li> <p>\u8fc7\u7f29\u653e\u64cd\u4f5c\u8c03\u6574\u6837\u5f0f\u5411\u91cf\u7684\u5e45\u5ea6\u3002</p> </li> <li> <p>\u8fd9\u79cd\u65b9\u5f0f\u4e0d\u4f1a\u5b8c\u5168\u53bb\u9664\u591a\u6837\u6027\uff0c\u800c\u662f\u63a7\u5236\u591a\u6837\u6027\u7684\u5e45\u5ea6\u3002     \u56e0\u6b64\uff0cStyleGAN \u7684\u201c\u622a\u65ad\u6280\u5de7\u201d\u5b9e\u9645\u4e0a\u662f\u4e00\u79cd\u52a8\u6001\u7684 \u6837\u5f0f\u5411\u91cf\u7f29\u653e\u64cd\u4f5c \u3002</p> <p>\u4e3a\u4ec0\u4e48\u79f0\u4e3a\u622a\u65ad\uff1f \u5c3d\u7ba1\u64cd\u4f5c\u672c\u8d28\u662f\u7f29\u653e\uff0c\u4f46\u88ab\u79f0\u4e3a\u622a\u65ad\u53ef\u80fd\u6709\u4ee5\u4e0b\u539f\u56e0\uff1a</p> <ol> <li>\u7ed3\u679c\u4e0a\u7684\u7c7b\u4f3c\u6027 \uff1a</li> <li>\u7f29\u653e\u64cd\u4f5c\u548c\u4f20\u7edf\u622a\u65ad\u5728\u51cf\u5c11\u751f\u6210\u6837\u5f0f\u7684\u591a\u6837\u6027\u4e0a\u6709\u76f8\u4f3c\u6548\u679c\u3002</li> </ol> </li> <li> <p>\u5f53 \\(\\text{trunc\\_psi}\\) \u63a5\u8fd1 0 \u65f6\uff0c\u6837\u5f0f\u5411\u91cf\u8d8b\u8fd1 \\(w\\_avg\\)\uff0c\u6b64\u65f6\u7684\u884c\u4e3a\u7c7b\u4f3c\u4e8e\u5c06\u6240\u6709\u6837\u5f0f\u5411\u91cf\u201c\u622a\u65ad\u201d\u5230\u4e00\u4e2a\u72ed\u7a84\u7684\u8303\u56f4\u3002</p> <ol> <li>\u672f\u8bed\u6cbf\u88ad \uff1a</li> <li>\u5728 GAN \u793e\u533a\uff0c\"truncation trick\" \u662f\u4e00\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u672f\u8bed\uff0c\u5c3d\u7ba1\u5177\u4f53\u5b9e\u73b0\u53ef\u80fd\u4f1a\u6709\u6240\u4e0d\u540c\u3002</li> </ol> </li> </ul> <p>\u603b\u7ed3</p> <ol> <li> <p>StyleGAN \u7684\u64cd\u4f5c\u662f\u7f29\u653e \uff1a</p> </li> <li> <p>\u5b83\u6839\u636e \\(w\\_avg\\) \u52a8\u6001\u8c03\u6574\u6837\u5f0f\u5411\u91cf \\(wp\\) \u7684\u5e45\u5ea6\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u5730\u5bf9\u5176\u503c\u8fdb\u884c\u88c1\u526a(\u7f29\u653e)\u3002</p> </li> <li> <p>\u79f0\u4e3a\u622a\u65ad\u7684\u539f\u56e0 \uff1a</p> </li> <li> <p>\u7ed3\u679c\u4e0a\u7c7b\u4f3c\u4f20\u7edf\u622a\u65ad\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u751f\u6210\u6837\u5f0f\u7684\u591a\u6837\u6027\u3002</p> </li> <li> <p>\u672f\u8bed\u5ef6\u7eed\u4e86 GAN \u793e\u533a\u7684\u4e60\u60ef\u7528\u6cd5\u3002</p> </li> <li> <p>\u672c\u8d28 \uff1a</p> </li> <li> <p>\u5b83\u662f \u6837\u5f0f\u5411\u91cf\u7684\u5e45\u5ea6\u7f29\u653e \uff0c\u8c03\u63a7\u751f\u6210\u56fe\u50cf\u7684\u591a\u6837\u6027\u4e0e\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002</p> </li> </ol>"},{"location":"book/chapter3_GAN/3.3stylegan/paper/#5","title":"5. \u56fe\u50cf\u751f\u6210","text":"<pre><code>synthesis_results = self.synthesis(wp, lod, randomize_noise)\n</code></pre> <ul> <li>\u8f93\u5165 :</li> <li>\\(wp\\): \u5206\u5c42\u6837\u5f0f\u5411\u91cf\u3002</li> <li>\\(\\text{lod}\\): \u5206\u8fa8\u7387\u7ea7\u522b\uff08Level of Detail\uff09\uff0c\u7528\u4e8e\u63a7\u5236\u751f\u6210\u56fe\u50cf\u7684\u7ec6\u8282\u5c42\u6b21\u3002</li> <li>\\(\\text{randomize\\_noise}\\): \u662f\u5426\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u968f\u673a\u5316\u566a\u58f0\u3002</li> <li>\u8fc7\u7a0b :</li> <li> <p>\u4f7f\u7528\u6837\u5f0f\u5411\u91cf \\(wp\\) \u548c\u566a\u58f0\u63a7\u5236\u56fe\u50cf\u7684\u751f\u6210\u8fc7\u7a0b\u3002</p> </li> <li> <p>\\(\\text{lod}\\) \u53ef\u4ee5\u8c03\u6574\u751f\u6210\u56fe\u50cf\u7684\u5206\u8fa8\u7387\uff0c\u652f\u6301\u9010\u6b65\u589e\u52a0\u7ec6\u8282\uff08\u6e10\u8fdb\u5f0f\u751f\u6210\uff09\u3002</p> </li> <li>\u8f93\u51fa :</li> <li>\u751f\u6210\u7684\u56fe\u50cf\u53ca\u5176\u76f8\u5173\u7279\u5f81\u3002</li> </ul>"},{"location":"book/chapter3_GAN/3.3stylegan/paper/#2-synthesis-network","title":"2. synthesis network","text":"<p>synthesis network \u5b9a\u4e49\u5728class SynthesisModule \u91cc\u9762\uff0c\u662f\u4e00\u4e2a\u751f\u6210\u5668\u7684\u4e3b\u8981\u90e8\u5206\uff0c\u8d1f\u8d23\u751f\u6210\u56fe\u50cf\u7684\u8fc7\u7a0b\u3002 \u5b83\u7684\u53c2\u6570\u610f\u4e49</p> <pre><code>resolution (\u9ed8\u8ba41024): \u751f\u6210\u56fe\u50cf\u7684\u6700\u7ec8\u8f93\u51fa\u5206\u8fa8\u7387\u3002\u4f8b\u5982\u8bbe\u7f6e\u4e3a1024\u65f6\uff0c\u6700\u7ec8\u751f\u62101024x1024\u50cf\u7d20\u7684\u56fe\u50cf\u3002\ninit_resolution (\u9ed8\u8ba44): \u751f\u6210\u5668\u7684\u521d\u59cb\u7279\u5f81\u56fe\u5206\u8fa8\u7387\u3002StyleGAN\u4ece\u8fd9\u4e2a\u4f4e\u5206\u8fa8\u7387(\u901a\u5e38\u662f4\u62168)\u5f00\u59cb\uff0c\u7136\u540e\u901a\u8fc7\u4e00\u7cfb\u5217\u4e0a\u91c7\u6837\u5c42\u9010\u6b65\u589e\u52a0\u5206\u8fa8\u7387\u76f4\u5230\u8fbe\u5230\u76ee\u6807\u5206\u8fa8\u7387\u3002\nw_space_dim (\u9ed8\u8ba4512): W\u6f5c\u5728\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002\u8fd9\u662fStyleGAN\u7684\u4e00\u4e2a\u5173\u952e\u7279\u5f81\uff0c\u5b83\u662f\u5c06\u8f93\u5165\u7684Z\u7a7a\u95f4\u901a\u8fc7\u6620\u5c04\u7f51\u7edc\u8f6c\u6362\u540e\u7684\u4e2d\u95f4\u6f5c\u5728\u7a7a\u95f4\uff0c\u7528\u4e8e\u66f4\u597d\u5730\u63a7\u5236\u4e0d\u540c\u5c3a\u5ea6\u7684\u56fe\u50cf\u7279\u5f81\u3002\nimage_channels (\u9ed8\u8ba43): \u8f93\u51fa\u56fe\u50cf\u7684\u989c\u8272\u901a\u9053\u6570\uff0c\u5bf9\u4e8eRGB\u5f69\u8272\u56fe\u50cf\u662f3\u3002\nfinal_tanh (\u9ed8\u8ba4False): \u662f\u5426\u5728\u7f51\u7edc\u6700\u540e\u4e00\u5c42\u4f7f\u7528tanh\u6fc0\u6d3b\u51fd\u6570\u3002\u4f7f\u7528tanh\u53ef\u4ee5\u5c06\u8f93\u51fa\u4e25\u683c\u9650\u5236\u5728[-1, 1]\u8303\u56f4\u5185\u3002\nconst_input (\u9ed8\u8ba4True): \u662f\u5426\u4f7f\u7528\u5e38\u91cf\u8f93\u5165\u3002StyleGAN\u7684\u4e00\u4e2a\u521b\u65b0\u662f\u4f7f\u7528\u5b66\u4e60\u7684\u5e38\u91cf\u4f5c\u4e3a\u751f\u6210\u5668\u7684\u8d77\u70b9\uff0c\u800c\u4e0d\u662f\u968f\u673a\u566a\u58f0\u3002\nfused_scale (\u9ed8\u8ba4'auto'): \u63a7\u5236\u662f\u5426\u4f7f\u7528\u878d\u5408\u7684\u4e0a\u91c7\u6837\u5377\u79ef\u64cd\u4f5c\u3002\u878d\u5408\u64cd\u4f5c\u53ef\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002\nuse_wscale (\u9ed8\u8ba4True): \u662f\u5426\u4f7f\u7528\u6743\u91cd\u7f29\u653e\u3002\u8fd9\u662f\u4e00\u79cd\u8bad\u7ec3\u6280\u5de7\uff0c\u901a\u8fc7\u5bf9\u6743\u91cd\u8fdb\u884c\u9002\u5f53\u7684\u7f29\u653e\u6765\u5e2e\u52a9\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\nnoise_type (\u9ed8\u8ba4'spatial'): \u63a7\u5236\u6dfb\u52a0\u5230\u6bcf\u4e00\u5c42\u7684\u968f\u673a\u566a\u58f0\u7684\u7c7b\u578b\u3002\u7a7a\u95f4\u566a\u58f0\u53ef\u4ee5\u5e2e\u52a9\u751f\u6210\u66f4\u591a\u7684\u7ec6\u8282\u53d8\u5316\u3002\nfmaps_base (\u9ed8\u8ba416&lt;&lt;10): \u57fa\u7840\u7279\u5f81\u56fe\u6570\u91cf\uff0c\u7528\u4e8e\u63a7\u5236\u7f51\u7edc\u5404\u5c42\u7684\u901a\u9053\u6570\u3002\u9ed8\u8ba4\u503c\u662f16384\u3002\nfmaps_max (\u9ed8\u8ba4512): \u9650\u5236\u7f51\u7edc\u4e2d\u4efb\u4f55\u5c42\u7684\u6700\u5927\u7279\u5f81\u56fe\u6570\u91cf\uff0c\u9632\u6b62\u7f51\u7edc\u53d8\u5f97\u8fc7\u5927\u3002\n</code></pre> <p>forward \u6b65\u9aa4</p> <pre><code>    def forward(self, wp, lod=None, randomize_noise=False):\n        # wp \u4ee3\u8868\u7ecf\u8fc7\u6620\u5c04\u7f51\u7edc\u5904\u7406\u540e\u7684\u98ce\u683c\u5411\u91cf\uff08style vector\uff09\u3002\n        # Z \u7a7a\u95f4\uff08\u521d\u59cb\u6f5c\u5728\u7a7a\u95f4\uff09\n        # \u8fd9\u662f\u8f93\u5165\u7684\u968f\u673a\u566a\u58f0\u5411\u91cf\n        # \u901a\u5e38\u662f\u4ece\u6807\u51c6\u6b63\u6001\u5206\u5e03\u91c7\u6837\u5f97\u5230\n        # W \u7a7a\u95f4\uff08\u4e2d\u95f4\u6f5c\u5728\u7a7a\u95f4\uff09\n        # \u901a\u8fc7\u6620\u5c04\u7f51\u7edc(Mapping Network)\u5c06 Z \u7a7a\u95f4\u5411\u91cf\u8f6c\u6362\u5f97\u5230\n        # \u76f8\u6bd4 Z \u7a7a\u95f4\uff0cW \u7a7a\u95f4\u5177\u6709\u66f4\u597d\u7684\u89e3\u8026\u7279\u6027\n        # W+ \u7a7a\u95f4\uff08\u6269\u5c55\u7684 W \u7a7a\u95f4\uff09\n        # \u8fd9\u5c31\u662f\u4ee3\u7801\u4e2d\u7684 wp\uff08w plus\uff09\n        # \u5b83\u662f\u5c06 W \u7a7a\u95f4\u7684\u5411\u91cf\u6269\u5c55\u5230\u6bcf\u4e00\u5c42\u90fd\u6709\u4e00\u4e2a\u72ec\u7acb\u7684\u98ce\u683c\u5411\u91cf\n        # \u5982\u679c\u7f51\u7edc\u6709 n \u5c42\uff0c\u90a3\u4e48 wp \u7684\u5f62\u72b6\u5c31\u662f [batch_size, n, w_dim]\n        lod = self.lod.cpu().tolist() if lod is None else lod\n        # lod \u7684\u8ba1\u7b97\u8fc7\u7a0b\uff1a\n        # \u57fa\u4e8e\u5df2\u8bad\u7ec3\u7684\u56fe\u50cf\u6570\u91cf\uff08total_img\uff09\u8ba1\u7b97\n        # \u6bcf\u4e2a\u5206\u8fa8\u7387\u7ea7\u522b\u7684\u6301\u7eed\u65f6\u95f4 = \u8bad\u7ec3\u56fe\u50cf\u6570 + \u8fc7\u6e21\u56fe\u50cf\u6570\n        # \u5f53\u524d\u7684 lod \u503c\u51b3\u5b9a\u4e86\u751f\u6210\u56fe\u50cf\u7684\u5206\u8fa8\u7387\n        # \u4f8b\u5982\uff0c\u5bf9\u4e8e512x512\u7684\u6700\u7ec8\u5206\u8fa8\u7387\uff1a\n        # \u5206\u8fa8\u7387\u5e8f\u5217\uff1a8 \u2192 16 \u2192 32 \u2192 64 \u2192 128 \u2192 256 \u2192 512\n        # LOD\u5e8f\u5217\uff1a6 \u2192 5 \u2192 4 \u2192 3 \u2192 2 \u2192 1 \u2192 0\n        if lod + self.init_res_log2 &gt; self.final_res_log2: # lod \u5f02\u5e38\n            raise ValueError(f'Maximum level-of-detail (lod) is '\n                             f'{self.final_res_log2 - self.init_res_log2}, '\n                             f'but `{lod}` is received!')\n\n        results = {'wp': wp}\n        for res_log2 in range(self.init_res_log2, self.final_res_log2 + 1):\n            current_lod = self.final_res_log2 - res_log2\n            if lod &lt; current_lod + 1:\n                block_idx = res_log2 - self.init_res_log2\n                if block_idx == 0: # \u6700\u4f4e\u5206\u8fa8\u7387\n                    if self.const_input:\n                        x, style = self.layer0(None, wp[:, 0], randomize_noise)\n                    else:\n                        x = wp[:, 0].view(-1, self.w_space_dim, 1, 1)\n                        x, style = self.layer0(x, wp[:, 0], randomize_noise)\n                else:\n                    x, style = self.__getattr__(f'layer{2 * block_idx}')(\n                        x, wp[:, 2 * block_idx], randomize_noise)\n                    results[f'style{2 * block_idx:02d}'] = style\n                    x, style = self.__getattr__(f'layer{2 * block_idx + 1}')(\n                        x, wp[:, 2 * block_idx + 1], randomize_noise)\n                    results[f'style{2 * block_idx + 1:02d}'] = style\n            if current_lod - 1 &lt; lod &lt;= current_lod:\n                image = self.__getattr__(f'output{block_idx}')(x, None)\n            elif current_lod &lt; lod &lt; current_lod + 1:\n                alpha = np.ceil(lod) - lod\n                image = (self.__getattr__(f'output{block_idx}')(x, None) * alpha\n                         + self.upsample(image) * (1 - alpha))\n            elif lod &gt;= current_lod + 1:\n                image = self.upsample(image)\n        results['image'] = self.final_activate(image)\n        return results\n</code></pre> <p>lod \u8868\u793a level of details.\u8868\u793a\u5206\u8fa8\u7387\u3002</p> <pre><code>        # Compute level-of-details.\n        phase, subphase = divmod(runner.seen_img, self.lod_duration)\n        lod = self.init_lod - phase\n        if self.lod_transition_img:\n            transition_img = max(subphase - self.lod_training_img, 0)\n            lod = lod - transition_img / self.lod_transition_img\n        lod = max(lod, 0.0)\n        resolution = self.init_res * (2 ** int(np.ceil(self.init_lod - lod)))\n        return lod, resolution\n</code></pre> <p>lod \u8be6\u7ec6\u8ba1\u7b97\u8fc7\u7a0b\u3002\u5047\u8bbe \u5176\u4e2d\uff0cinit_res \u662f\u521d\u59cb\u5206\u8fa8\u7387\uff08\u901a\u5e38\u662f4\u62168\uff09\u3002 \u4e3e\u4f8b\u8bf4\u660e\uff1a</p> <pre><code>\u5047\u8bbe init_lod = 8\uff08\u4ece8x8\u52302048x2048\uff09\nlod_duration = 600,000\uff08\u6bcf\u4e2a\u5206\u8fa8\u7387\u7ea7\u522b60\u4e07\u5f20\u56fe\u50cf\uff09,\nlod_training_img = 300,000\uff08\u7a33\u5b9a\u9636\u6bb530\u4e07\u5f20\u56fe\u50cf\uff09\u8fd9\u4e2a\u9636\u6bb5lod\u4e00\u76f4\u4fdd\u6301\u4e3a8,\u5206\u8fa8\u7387\u4e3a8x8,\nlod_transition_img = 300,000\uff08\u8fc7\u6e21\u9636\u6bb530\u4e07\u5f20\u56fe\u50cf\uff09,\n\u5230\u4e86\u8fd9\u4e2a\u9636\u6bb5\u4e4b\u540e, lod \u4ece 8.0 \u9010\u6e10\u964d\u4f4e\u5230 7.0\uff0c\u5206\u8fa8\u7387\u4e00\u76f4\u662f16x16\u3002,\n\u76f4\u89c2\u4e0a\u770b \u6bcf\u4e2a\u5206\u8fa8\u7387\u8bad\u7ec3\u5206\u6210\u4e24\u4e2a\u9636\u6bb5\uff0c\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u5f53\u524d\u5206\u8fa8\u7387\u7684\u7a33\u5b9a\u9636\u6bb5\uff0c\u7b2c\u4e8c\u9636\u6bb5\u8bad\u7ec3\u4e0b\u4e00\u4e2a\u5206\u8fa8\u7387\u7684\u8fc7\u6e21\u9636\u6bb5\uff0clod\u7ebf\u6027\u9012\u51cf\u5230\u4e0b\u4e00\u4e2a\u5206\u8fa8\u7387\u5bf9\u5e94\u7684lod\u3002\n</code></pre> <p>synthesis \u7f51\u7edc\u7684\u6bcf\u4e00\u4e2a\u5206\u8fa8\u7387\u5c42\u90fd\u6709\u4e24\u4e2a\u5377\u79ef\u5c42\u3002\u521d\u59cb\u5206\u8fa8\u7387\u7684\u7b2c\u4e00\u4e2a\u5377\u79ef\u5c42\u6709\u6240\u533a\u522b\u3002</p>"},{"location":"book/chapter3_GAN/3.3stylegan/paper/#_4","title":"\u7b2c\u4e00\u4e2a\u5206\u8fa8\u7387\u5c42","text":""},{"location":"book/chapter3_GAN/3.3stylegan/paper/#layer0","title":"layer0","text":"<p>layer0 \u7684\u7f51\u7edc\u5b9a\u4e49\u4e3a</p> <pre><code>self.layer0 =ConvBlock(\n    in_channels=self.get_nf(res),\n    out_channels=self.get_nf(res),\n    resolution=self.init_res,\n    w_space_dim=self.w_space_dim,\n    position='const_init', # \u6807\u8bb0\u4e3aconst_init\n    use_wscale=self.use_wscale,\n    noise_type=self.noise_type)\n</code></pre> <p>\u8f93\u5165\u4e3a</p> <pre><code>x, style = self.layer0(None, wp[:, 0], randomize_noise)\n</code></pre> <p>\u7b2c\u4e00\u4e2a\u5206\u8fa8\u7387\u53ea\u6709\u4e00\u4e2aconvblock\uff0c\u5c06\u539f\u59cb\u7684\u8f93\u51654x4x512 \u8f6c\u5316\u4e3a 4x4x512</p>"},{"location":"book/chapter3_GAN/3.3stylegan/paper/#k","title":"\u7b2ck\u4e2a\u5206\u8fa8\u7387\u5c42","text":"<p>\u6b21\u540e\u7684\u6bcf\u4e2a\u5206\u8fa8\u7387\u5c42\u5305\u542b\u4e24\u4e2aconvblock \u7b2c\u4e00\u4e2aconvblock \u589e\u52a0\u4e86\u4e0a\u91c7\u6837\u7684\u64cd\u4f5c\u3002\u5176\u4f59\u7684\u4e00\u6837\u3002 \u6700\u540e\u4e00\u4e2a\u5206\u8fa8\u7387\u53ea\u6709\u4e00\u4e2aconvblock\uff0c\u5c06512x512x32 \u8f6c\u5316\u4e3a 1024x1024x16 \u5206\u8fa8\u7387\u589e\u52a0\u4e00\u500d\uff0coutput channel \u51cf\u534a\uff0c\u4fdd\u6301\u5377\u79ef\u6574\u4f53\u7684\u53c2\u6570\u91cf\u6bcf\u4e00\u5c42\u4e00\u81f4\u3002</p> <pre><code>self.layer{2k-1} =  ConvBlock(\n    in_channels=self.get_nf(res // 2),\n    out_channels=self.get_nf(res),\n    resolution=res,\n    w_space_dim=self.w_space_dim,\n    upsample=True,\n    fused_scale=fused_scale,\n    use_wscale=self.use_wscale,\n    noise_type=self.noise_type\n)\n\nself.layer{2k} =ConvBlock(\n    in_channels=self.w_space_dim,\n    out_channels=self.get_nf(res),\n    resolution=self.init_res,\n    w_space_dim=self.w_space_dim,\n    kernel_size=self.init_res,\n    padding=self.init_res - 1,\n    use_wscale=self.use_wscale,\n    noise_type=self.noise_type\n)\n\n</code></pre> <p>\u8f93\u5165</p> <pre><code>x, style = f\"self.layer{2k-11}\"(x, wp[:, 2 * block_idx - 1], randomize_noise)\nx, style = f\"self.layer{2k}\"(x, wp[:, 2 * block_idx],randomize_noise)\n</code></pre> <p>\u6bcf\u4e2aconvblock \u7684\u8f93\u5165\u90fd\u5305\u542b\u4e0a\u4e00\u5c42\u7684\u8f93\u5165,\u672c\u5c42\u7684\u98ce\u683c\u5411\u91cf\uff0c\u4ee5\u53ca\u662f\u5426\u9700\u8981\u6dfb\u52a0\u566a\u58f0\u3002</p> <p>\u6bcf\u4e00\u4e2a\u5206\u8fa8\u7387\u7684\u7b2c\u4e00\u4e2a\u5377\u79ef\u5c42\u548c\u7b2c\u4e8c\u4e2a\u5377\u79ef\u5c42\u533a\u522b\u5728\u4e8e\u7b2c\u4e8c\u4e2a\u4f1a\u52a0\u5165\u4e0a\u91c7\u6837\u3002</p>"},{"location":"book/chapter3_GAN/3.3stylegan/paper/#3-convblock","title":"3. ConvBlock","text":"<pre><code>    def forward(self, x, w, randomize_noise=False):\n        if self.position != 'const_init':\n            x = self.upsample(x)\n            weight = self.weight * self.wscale\n            x = F.conv2d(x,\n                             weight=weight,\n                             bias=None,\n                             stride=self.stride,\n                             padding=self.padding)\n            x = self.blur(x)\n        else:\n            x = self.const.repeat(w.shape[0], 1, 1, 1)  # \u521d\u59cb\u8f93\u5165\n\n        bias = self.bias * self.bscale if self.bias is not None else None\n\n        if self.position == 'last':\n            if bias is not None:\n                x = x + bias.view(1, -1, 1, 1)\n            return x\n\n        x = self.apply_noise(x, randomize_noise)\n        if bias is not None:\n            x = x + bias.view(1, -1, 1, 1)\n        x = self.activate(x)\n        x = self.normalize(x)\n        x, style = self.style(x, w)\n        return x, style\n</code></pre> <p>\u5177\u4f53\u6b65\u9aa4</p> <ol> <li> <p>\u4e0a\u91c7\u6837\u548c\u5377\u79ef\u3002\u53ea\u5728\u6bcf\u4e00\u4e2a\u5206\u8fa8\u7387\u7684\u7b2c\u4e00\u5c42\u8fdb\u884c\u4e0a\u91c7\u6837\u3002\u4f7f\u7528\u7684\u4e0a\u91c7\u6837\u662f</p> <p><code>python return F.interpolate(x, scale_factor=self.scale_factor, mode='nearest')</code></p> <p>\u5377\u79ef\u7684\u8fc7\u7a0b\u4e2d\u4f1a\u5229\u7528\u53ef\u5b66\u4e60\u7684\u4e00\u4e2a\\(w_{scale}\\)\u5bf9\u5377\u79ef\u7684\u6743\u91cd\u8fdb\u884c\u7f29\u653e\u3002\u8fd9\u4e2a\u60f3\u6cd5\u6765\u81ea\u4e8e progressive growing of gan\u4e2d\u3002</p> <p>\u5176\u4e2d\uff0c\u5982\u679c\u662finit \u8f93\u5165 \u53ea\u4f1a\u5c06\u53ef\u4ee5\u5b66\u4e60\u7684init\u5728batch \u7ef4\u5ea6\u8fdb\u884c\u590d\u5236 2. \u589e\u52a0bias \u53ef\u5b66\u4e60\u7684\u4e00\u4e2abias\u3002\u8fd9\u4e2a\u5e94\u8be5\u4e0d\u91cd\u8981\u3002 3. \u589e\u52a0\u566a\u58f0</p> </li> </ol> <pre><code>if randomize_noise:\n    if self.noise_type == 'spatial':\n        noise = torch.randn(x.shape[0], 1, self.res, self.res).to(x)\n    elif self.noise_type == 'channel':\n        noise = torch.randn(x.shape[0], self.channels, 1, 1).to(x)\nelse:\n    noise = self.noise\nif self.noise_type == 'spatial':\n    x = x + noise * self.weight.view(1, self.channels, 1, 1)\nelif self.noise_type == 'channel':\n    x = x + noise * self.weight.view(1, 1, self.res, self.res)\n</code></pre> <p>\u566a\u97f3\u7684\u589e\u52a0\u6709\u4e24\u79cd\u6cd5\u6848\u3002\u4e00\u4e2a\u662f\u589e\u52a0\u5728\u7a7a\u95f4\u4e0a\uff0c\u4e00\u4e2a\u662f\u589e\u52a0\u5728channel \u4e0a\u3002 4. \u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570</p> <pre><code>self.activate = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n</code></pre> <p> LeakyReLU \u7684\u597d\u5904\u4e3b\u8981\u4f53\u73b0\u5728\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\uff1a</p> <ul> <li>\u9632\u6b62\u795e\u7ecf\u5143\u6b7b\u4ea1\uff1a\u8d1f\u503c\u533a\u57df\u7684\u659c\u7387\u4f7f\u5f97\u795e\u7ecf\u5143\u4e0d\u4f1a\u5b8c\u5168\u5931\u6d3b\u3002</li> <li>\u6539\u5584\u68af\u5ea6\u6d41\u52a8\uff1a\u51cf\u5c11\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u652f\u6301\u66f4\u6df1\u5c42\u7f51\u7edc\u7684\u8bad\u7ec3\u3002</li> <li>\u9002\u5e94\u6570\u636e\u5206\u5e03\uff1a\u6355\u6349\u8d1f\u503c\u533a\u57df\u7684\u4fe1\u606f\uff0c\u66f4\u597d\u5730\u5904\u7406\u5177\u6709\u8d1f\u503c\u7279\u5f81\u7684\u6570\u636e\u3002</li> <li>\u8bad\u7ec3\u7a33\u5b9a\u6027\uff1a\u63d0\u5347\u8bad\u7ec3\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6027\u3002</li> <li> <p>\u901a\u7528\u6027\uff1a\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u548c\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u4e3a\u8bb8\u591a\u6a21\u578b\u7684\u9ed8\u8ba4\u9009\u62e9\u3002</p> </li> <li> <p>instance normalization  \u6807\u51c6instance normalization\u3002 \u4f7f\u5f97\u6bcf\u4e2a\u6837\u672c\u7684\u6bcf\u4e2a\u901a\u9053\u7684featuremap\u7684\u5747\u503c\u4e3a0\uff0c \u65b9\u5dee\u4e3a1.</p> </li> <li>\u98ce\u683c\u5316</li> </ul> <pre><code>def forward(self, x, w):\n    style = self.forward_style(w)\n    style_split = style.view(-1, 2, self.out_channels, 1, 1)\n    x = x * (style_split[:, 0] + 1) + style_split[:, 1]\n    return x, style\n</code></pre> <p>\u5bf9featuremap\u7684\u6bcf\u4e00\u5c42\u5229\u7528\u5b66\u4e60\u5230\u7684\u98ce\u683c\u5411\u91cf\u8fdb\u884c\u98ce\u683c\u8c03\u5236\u3002\u5177\u4f53\u6765\u8bf4style_split \u6bcf\u4e2achannel \u5305\u542b\u4e00\u4e2a\u5b66\u4e60\u5230\u7684\u6807\u51c6\u5dee\u548c\u5747\u503c</p> <pre><code>x = x * (style_split[:, 0] + 1) + style_split[:, 1]\n</code></pre> <p>\u610f\u5473\u6700\u6bcf\u4e00\u4e2achannel \u7684feature map\u663e\u5f0f\u5f97\u8c03\u8282\u65b9\u5dee\u548c\u6807\u51c6\u5dee\u3002\u4e00\u822c\u5f97\uff0cfeaturemap\u7684\u65b9\u5dee\u548c\u5747\u503c\u8868\u793a\u56fe\u7247\u7684\u98ce\u683c\u3002</p> <p>```</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/","title":"StyleGAN3 Overview","text":""},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#terms","title":"Terms","text":"<p>Aliasing\uff08\u6df7\u53e0\uff09 \u662f\u4e00\u4e2a\u4fe1\u53f7\u5904\u7406\u9886\u57df\u7684\u57fa\u672c\u95ee\u9898\uff0c\u6307\u7684\u662f\u5f53\u4fe1\u53f7\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u91c7\u6837\u7387\u4e0d\u8db3\u65f6\uff0c\u9ad8\u9891\u4fe1\u53f7\u7684\u7279\u6027\u4f1a\u88ab\u9519\u8bef\u5730\u6620\u5c04\u5230\u4f4e\u9891\u90e8\u5206\uff0c\u4ece\u800c\u5bfc\u81f4\u5931\u771f\u6216\u4e0d\u6b63\u786e\u7684\u7ed3\u679c\u3002\u8fd9\u79cd\u73b0\u8c61\u540c\u6837\u9002\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u548c\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u751f\u6210\u7f51\u7edc\u3002</p> <p>\u5728\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u89e3\u91ca \u5b9a\u4e49\uff1a</p> <p>\u5728\u79bb\u6563\u5316\u8fde\u7eed\u4fe1\u53f7\u65f6\uff0c\u5982\u679c\u91c7\u6837\u7387\u4f4e\u4e8e\u4fe1\u53f7\u7684\u5948\u594e\u65af\u7279\u9891\u7387\uff08Nyquist frequency\uff0c\u5373\u4fe1\u53f7\u6700\u9ad8\u9891\u7387\u7684\u4e24\u500d\uff09\uff0c\u5c31\u4f1a\u53d1\u751f\u6df7\u53e0\u3002 \u6df7\u53e0\u4f1a\u5bfc\u81f4\u9ad8\u9891\u4fe1\u53f7\u88ab\u201c\u6298\u53e0\u201d\u5230\u4f4e\u9891\u57df\uff0c\u4ea7\u751f\u9519\u8bef\u7684\u9891\u7387\u4fe1\u606f\u3002 \u793a\u4f8b\uff1a</p> <p>\u5728\u97f3\u9891\u4fe1\u53f7\u4e2d\uff0c\u91c7\u6837\u4e0d\u8db3\u4f1a\u5bfc\u81f4\u539f\u672c\u7684\u9ad8\u97f3\u53d8\u6210\u4e0d\u76f8\u5173\u7684\u4f4e\u97f3\u3002 \u5728\u56fe\u50cf\u5904\u7406\u4e2d\uff0c\u7f29\u5c0f\u56fe\u50cf\u800c\u6ca1\u6709\u6b63\u786e\u4f4e\u901a\u6ee4\u6ce2\u65f6\u4f1a\u5bfc\u81f4\u8fb9\u7f18\u6216\u7eb9\u7406\u51fa\u73b0\u6ce2\u7eb9\u6216\u952f\u9f7f\u72b6\u4f2a\u5f71\u3002</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#abstract","title":"Abstract","text":"<p>\u95ee\u9898\u63cf\u8ff0\uff1a \u5f53\u524d\u5178\u578b\u7684 GAN \u5c3d\u7ba1\u5177\u6709\u5c42\u6b21\u5316\u7684\u5377\u79ef\u7ed3\u6784\uff0c\u4f46\u751f\u6210\u8fc7\u7a0b\u5374\u4f9d\u8d56\u4e8e\u7edd\u5bf9\u50cf\u7d20\u5750\u6807\uff0c\u5bfc\u81f4\u56fe\u50cf\u7ec6\u8282\u201c\u7c98\u5728\u201d\u50cf\u7d20\u7f51\u683c\u4e0a\uff0c\u800c\u4e0d\u662f\u81ea\u7136\u5730\u9644\u7740\u5728\u7269\u4f53\u8868\u9762\u3002</p> <p>\u95ee\u9898\u6839\u6e90\uff1a \u8fd9\u79cd\u73b0\u8c61\u7684\u6839\u672c\u539f\u56e0\u662f\u751f\u6210\u5668\u7f51\u7edc\u4e2d\u7684\u4e0d\u5f53\u4fe1\u53f7\u5904\u7406\uff0c\u7279\u522b\u662f\u6df7\u53e0\u95ee\u9898\uff08aliasing\uff09\u3002</p> <p>\u89e3\u51b3\u65b9\u6cd5\uff1a \u901a\u8fc7\u5c06\u7f51\u7edc\u4e2d\u7684\u6240\u6709\u4fe1\u53f7\u89c6\u4e3a\u8fde\u7eed\u4fe1\u53f7\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e9b\u666e\u9002\u7684\u3001\u8f83\u5c0f\u7684\u67b6\u6784\u8c03\u6574\uff0c\u4ee5\u786e\u4fdd\u4e0d\u5fc5\u8981\u7684\u4fe1\u606f\u4e0d\u4f1a\u6cc4\u6f0f\u5230\u5c42\u6b21\u5316\u7684\u751f\u6210\u8fc7\u7a0b\u4e2d\u3002</p> <p>\u6539\u8fdb\u6548\u679c\uff1a \u65b0\u67b6\u6784\u5728\u56fe\u50cf\u8d28\u91cf\u6307\u6807\uff08\u5982 FID\uff09\u4e0a\u4e0e StyleGAN2 \u76f8\u5f53\uff0c\u4f46\u5176\u5185\u90e8\u8868\u793a\u53d1\u751f\u4e86\u663e\u8457\u53d8\u5316\u3002 \u65b0\u6a21\u578b\u5728\u4e9a\u50cf\u7d20\u7ea7\u522b\u5b9e\u73b0\u4e86\u5bf9\u5e73\u79fb\u548c\u65cb\u8f6c\u7684\u7b49\u53d8\u6027\u3002</p> <p>\u5e94\u7528\u524d\u666f\uff1a \u8fd9\u4e9b\u6539\u8fdb\u4e3a\u751f\u6210\u6a21\u578b\u5728\u89c6\u9891\u548c\u52a8\u753b\u7b49\u52a8\u6001\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4f7f\u751f\u6210\u6548\u679c\u66f4\u52a0\u81ea\u7136\u548c\u8fde\u8d2f\u3002</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#introduction","title":"Introduction","text":"<p>\u5f15\u8a00\u90e8\u5206\u660e\u786e\u4e86\u73b0\u6709 GAN \u4e2d\u6df7\u53e0\u95ee\u9898\u7684\u5371\u5bb3\uff0c\u5206\u6790\u4e86\u5176\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u901a\u8fc7\u6539\u8fdb\u4fe1\u53f7\u5904\u7406\u548c\u751f\u6210\u5668\u67b6\u6784\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u7814\u7a76\u76ee\u6807\u548c\u8d21\u732e</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#_1","title":"\u95ee\u9898\u80cc\u666f","text":"<p>\u8fd1\u5e74\u6765\uff0cGAN \u7684\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u6709\u4e86\u663e\u8457\u63d0\u5347\uff08\u5982 StyleGAN2 \u7b49\uff09\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u56fe\u50cf\u7f16\u8f91\u3001\u57df\u8fc1\u79fb\u548c\u89c6\u9891\u751f\u6210\u7b49\u9886\u57df\u3002 \u5c3d\u7ba1\u751f\u6210\u5668\u5177\u6709\u5c42\u6b21\u5316\u7684\u5377\u79ef\u7ed3\u6784\uff0c\u4f46\u751f\u6210\u8fc7\u7a0b\u5bf9\u7edd\u5bf9\u50cf\u7d20\u5750\u6807\u7684\u4f9d\u8d56\u4f1a\u5bfc\u81f4 \u201c\u7eb9\u7406\u7c98\u8fde\u201d\uff08texture sticking\uff09 \u95ee\u9898\uff0c\u5373\u7ec6\u8282\u56fa\u5b9a\u5728\u50cf\u7d20\u5750\u6807\u4e0a\u800c\u975e\u81ea\u7136\u9644\u7740\u5728\u7269\u4f53\u8868\u9762\u3002</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#_2","title":"\u95ee\u9898\u5206\u6790","text":"<p>\u7eb9\u7406\u7c98\u8fde\u73b0\u8c61\uff1a \u751f\u6210\u7ec6\u8282\uff08\u5982\u7eb9\u7406\u3001\u6bdb\u53d1\uff09\u5728\u5bf9\u8c61\u79fb\u52a8\u65f6\u672a\u80fd\u6b63\u786e\u968f\u7269\u4f53\u8868\u9762\u79fb\u52a8\uff0c\u7834\u574f\u4e86\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u7279\u522b\u662f\u5728\u89c6\u9891\u548c\u52a8\u753b\u751f\u6210\u4e2d\u3002 \u95ee\u9898\u6839\u6e90\uff1a \u6df7\u53e0\u662f\u5bfc\u81f4\u8fd9\u4e00\u73b0\u8c61\u7684\u6838\u5fc3\u539f\u56e0\u3002\u6df7\u53e0\u6e90\u4e8e\u4e0d\u5f53\u7684\u4fe1\u53f7\u5904\u7406\uff0c\u5982\uff1a    - \u4e0d\u7406\u60f3\u7684\u4e0a\u91c7\u6837\u6ee4\u6ce2\u5668\uff08\u5982\u53cc\u7ebf\u6027\u63d2\u503c\uff09\u3002    - \u6fc0\u6d3b\u51fd\u6570\uff08\u5982 ReLU\uff09\u5e26\u6765\u7684\u9ad8\u9891\u5206\u91cf\u3002    - \u7f51\u7edc\u8fb9\u754c\u586b\u5145\uff08padding\uff09\u7b49\u64cd\u4f5c\u5bfc\u81f4\u7edd\u5bf9\u50cf\u7d20\u5750\u6807\u6cc4\u9732\u3002    - \u6df7\u53e0\u7684\u653e\u5927\u673a\u5236\uff1a \u7f51\u7edc\u4f1a\u4e3b\u52a8\u5229\u7528\u8fd9\u4e9b\u6df7\u53e0\u4fe1\u606f\uff0c\u901a\u8fc7\u591a\u4e2a\u5c3a\u5ea6\u7684\u7ed3\u5408\u653e\u5927\u5b83\u4eec\uff0c\u4ece\u800c\u5efa\u7acb\u4f9d\u8d56\u7edd\u5bf9\u5c4f\u5e55\u5750\u6807\u7684\u7eb9\u7406\u6a21\u5f0f\u3002</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#_3","title":"\u8bba\u6587\u76ee\u6807","text":"<p>\u8bbe\u8ba1\u4e00\u4e2a\u751f\u6210\u5668\u67b6\u6784\uff0c\u80fd\u591f\uff1a \u5b8c\u5168\u6d88\u9664\u6df7\u53e0\u95ee\u9898\uff0c\u786e\u4fdd\u751f\u6210\u8fc7\u7a0b\u4e0d\u4f9d\u8d56\u7edd\u5bf9\u50cf\u7d20\u5750\u6807\u3002 \u5b9e\u73b0 \u5e73\u79fb\u548c\u65cb\u8f6c\u7b49\u53d8\u6027\uff08translation and rotation equivariance\uff09\uff0c\u4f7f\u751f\u6210\u7684\u56fe\u50cf\u7ec6\u8282\u80fd\u591f\u81ea\u7136\u968f\u7269\u4f53\u53d8\u5316\u3002 \u4fdd\u6301\u4e0e\u73b0\u6709 GAN\uff08\u5982 StyleGAN2\uff09\u76f8\u5f53\u7684\u56fe\u50cf\u8d28\u91cf\u3002</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#_4","title":"\u4e3b\u8981\u8d21\u732e","text":"<p>\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e \u8fde\u7eed\u4fe1\u53f7\u89e3\u91ca\uff08continuous signal interpretation\uff09 \u7684\u65b9\u6cd5\uff0c\u91cd\u65b0\u8bbe\u8ba1 StyleGAN2 \u7684\u751f\u6210\u5668\u67b6\u6784\u3002 \u53d1\u73b0\u5f53\u524d\u4e0a\u91c7\u6837\u6ee4\u6ce2\u5668\u5728\u6291\u5236\u6df7\u53e0\u65b9\u9762\u4e0d\u591f\u6709\u6548\uff0c\u9700\u8981\u66f4\u9ad8\u8d28\u91cf\u7684\u6ee4\u6ce2\u5668\uff08\u5982\u8d85\u8fc7 100dB \u7684\u8870\u51cf\u80fd\u529b\uff09\u3002 \u63d0\u51fa\u4e86\u4e00\u4e9b\u666e\u9002\u7684\u5c0f\u578b\u67b6\u6784\u6539\u52a8\uff0c\u65e2\u80fd\u6709\u6548\u6291\u5236\u6df7\u53e0\uff0c\u53c8\u80fd\u4fdd\u8bc1\u751f\u6210\u5668\u7684\u7b49\u53d8\u6027\u3002 \u65b0\u7684\u751f\u6210\u5668\u5728\u56fe\u50cf\u8d28\u91cf\u6307\u6807\uff08\u5982 FID\uff09\u4e0a\u4e0e StyleGAN2 \u6301\u5e73\uff0c\u4f46\u89e3\u51b3\u4e86\u7eb9\u7406\u7c98\u8fde\u95ee\u9898\uff0c\u66f4\u9002\u5408\u7528\u4e8e\u89c6\u9891\u548c\u52a8\u753b\u751f\u6210\u3002</p> <ul> <li>Texture sticking \u95ee\u9898\u4e3e\u4f8b\u8bf4\u660e \u5de6\u4fa7\uff08Central \u548c Averaged\uff09\uff1a StyleGAN2\uff1a \u73b0\u8c61\uff1a\u5373\u4f7f\u751f\u6210\u4e86\u4e00\u4e2a\u56fe\u50cf\u7684\u591a\u4e2a\u7248\u672c\uff08\u901a\u8fc7\u5bf9\u6f5c\u5728\u7a7a\u95f4\u7684\u7ec6\u5fae\u6270\u52a8\uff09\uff0c\u56fe\u50cf\u7684\u7ec6\u8282\uff08\u5982\u732b\u7684\u6bdb\u53d1\u6216\u7eb9\u7406\uff09\u4f9d\u7136\u56fa\u5b9a\u5728\u7edd\u5bf9\u7684\u50cf\u7d20\u5750\u6807\u4e0a\u3002 \u7ed3\u679c\uff1a\u5f53\u5bf9\u4e0d\u540c\u7248\u672c\u8fdb\u884c\u5e73\u5747\u65f6\uff0c\u7ec6\u8282\u4f1a\u5448\u73b0\u51fa\u975e\u81ea\u7136\u7684\u9510\u5316\uff08sharpness\uff09\uff0c\u56e0\u4e3a\u7eb9\u7406\u6ca1\u6709\u968f\u7269\u4f53\u79fb\u52a8\u3002 Ours\uff08\u6539\u8fdb\u540e\u7684\u6a21\u578b\uff09\uff1a \u73b0\u8c61\uff1a\u56fe\u50cf\u7684\u7ec6\u8282\u4e0d\u518d\u56fa\u5b9a\u4e8e\u50cf\u7d20\u5750\u6807\uff0c\u800c\u662f\u81ea\u7136\u5730\u8ddf\u968f\u7269\u4f53\u8868\u9762\u7684\u53d8\u5316\u3002 \u7ed3\u679c\uff1a\u5e73\u5747\u540e\u7684\u56fe\u50cf\u66f4\u5e73\u6ed1\uff0c\u7b26\u5408\u9884\u671f\u3002 \u53f3\u4fa7\uff08Latent Interpolation\uff09\uff1a StyleGAN2\uff1a \u73b0\u8c61\uff1a\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u63d2\u503c\u751f\u6210\u65f6\uff0c(\u5f53\u751f\u6210\u5185\u5bb9\u8fdb\u884c\u6c34\u5e73\u7684traslation\u65f6) \u5934\u53d1\u7b49\u7ec6\u8282\u6ca1\u6709\u8ddf\u968f\u4eba\u8138\u7684\u4f4d\u7f6e\u53d8\u5316\uff0c\u800c\u662f\u50cf\u201c\u7c98\u5728\u201d\u56fa\u5b9a\u7684\u50cf\u7d20\u4e0a\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u52a8\u753b\u4e2d\u5934\u53d1\u5448\u73b0\u6c34\u5e73\u6761\u7eb9\uff08streaks\uff09\u3002 Ours\uff08\u6539\u8fdb\u540e\u7684\u6a21\u578b\uff09\uff1a \u73b0\u8c61\uff1a\u5934\u53d1\u7684\u7ec6\u8282\u4f1a\u968f\u4eba\u7269\u7684\u4f4d\u7f6e\u81ea\u7136\u79fb\u52a8\uff0c\u751f\u6210\u66f4\u8fde\u8d2f\u7684\u52a8\u6001\u6548\u679c\u3002 \u7ed3\u679c\uff1a\u89e3\u51b3\u4e86\u7eb9\u7406\u7c98\u8fde\u95ee\u9898\uff0c\u5934\u53d1\u770b\u8d77\u6765\u66f4\u50cf\u771f\u5b9e\u7269\u4f53\u7684\u4e00\u90e8\u5206\u3002 \u89c6\u5c4f\u4e2d\u66f4\u52a0\u660e\u663e\u3002 </li> </ul>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#continuous-and-discrete-signals","title":"Continuous and Discrete Signals","text":"<p>\u5c06\u7f51\u7edc\u4e2d\u7684\u7279\u5f81\u56fe\u89c6\u4e3a\u8fde\u7eed\u4fe1\u53f7\uff0c\u800c\u4e0d\u662f\u79bb\u6563\u50cf\u7d20\uff0c\u4ece\u800c\u4e25\u683c\u9075\u5faa\u4fe1\u53f7\u5904\u7406\u7406\u8bba\uff08\u5982 Nyquist \u5b9a\u7406\uff09</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#relationship-between-continuous-and-discrete-signals","title":"Relationship Between Continuous and Discrete Signals","text":"<p>\u5047\u8bbe\u8fde\u7eed\u4fe1\u53f7\u4e3a\\(z(x)\\),\u79bb\u6563\u4fe1\u53f7\u4e3a\\(Z(x)\\), \u91c7\u6837\u9891\u7387\u4e3a\\(s\\), \u91c7\u6837\u95f4\u9694\u5219\u4e3a\\(T=\\frac{1}{s}\\).\\(\\cdot\\)\u8868\u793a\u4e58\u79ef, \u5377\u79ef\u8868\u793a\\(\\ast\\). \u4e8c\u7ef4\u72c4\u62c9\u514b\u68b3\u5986\u51fd\u6570\u4e3a\\(III(x,y)=\\sum_{m=-\\infty}^{\\infty}\\sum_{n=-\\infty}^{\\infty}\\delta(x-mT)\\delta(y-nT)\\) \u6839\u636e\u63d2\u503c\u516c\u5f0f</p> \\[ Z(x,y)=z(x,y)\\cdot\\left(\\text{III}(x,y)\\right)\\newline = z(x,y)\\cdot\\left(\\sum_{m=-\\infty}^{\\infty}\\sum_{n=-\\infty}^{\\infty}\\delta(x-mT)\\delta(y-nT)\\right)\\newline = \\sum_{m=-\\infty}^{\\infty}\\sum_{n=-\\infty}^{\\infty}z(mT,nT)\\delta(x-mT)\\delta(y-nT)\\newline \\] <p>\u8bba\u6587\u4e2d\u5bf9\u91c7\u6837\u70b9\u5411\u53f3\u8fdb\u884c\u4e86\\(\\frac{T}{2}\\)\u7684\u504f\u79fb\uff0c\u8fd9\u6837\u7684\u8bdd</p> \\[Z(x,y) = \\left(\\sum_{m=-\\infty}^{\\infty}\\sum_{n=-\\infty}^{\\infty} z\\left((m+\\frac{1}{2})T,(n+\\frac{1}{2})T\\right)\\delta(x-(m+\\frac{1}{2})T)\\delta(y-(n+\\frac{1}{2})T)\\right)\\newline \\] <p>\u4ece\u79bb\u6563\u4fe1\u53f7\u6062\u590d\u4e3a\u8fde\u7eed\u4fe1\u53f7\uff0c \u6839\u636e\u63d2\u503c\u516c\u5f0f</p> \\[ z(x,y) = \\left(\\sum_{m=-\\infty}^{\\infty}\\sum_{n=-\\infty}^{\\infty}Z(mT,nT)\\text{sinc}(\\frac{x - mT}{T})\\text{sinc}(\\frac{y - nT}{T})\\right)\\newline  = \\left(\\sum_{m=-\\infty}^{\\infty}\\sum_{n=-\\infty}^{\\infty}Z(mT,nT)\\alpha(x - mT,y - nT)\\right)\\newline  = (Z\\ast \\alpha)(x,y) \\] <p>suppose\\(\\alpha(x\uff0cy) = \\text{sinc}(\\frac{x}{T})\\text{sinc}(\\frac{y}{T})\\)</p> <p>\u5047\u8bbe\u9650\u5b9a\u7a7a\u95f4\u753b\u5e03\u4e3a[0,1]x[0,1]\uff0c \u4ee5\\(s=\\frac{1}{T}\\)\u9891\u7387\u8fdb\u884c\u91c7\u6837\u65f6\uff0c\u504f\u79fb\\(\\frac{1}{2}T\\)\u91c7\u6837\u4f1a\u751f\u6210\u603b\u5171\\(s^2\\)\u4e2a\u91c7\u6837\u70b9. \u6839\u636e\u63d2\u503c\u516c\u5f0f\uff0c\u5982\u679c\u8981\u5b8c\u5168\u6062\u590d\u539f\u59cb\u4fe1\u53f7\uff0c\u5219\u9700\u8981[0,1]x[0,1] \u4e4b\u5916\u7684\u4fe1\u53f7\u3002 \u4e3a\u4ec0\u4e48\u5462\uff1f  \u4e0a\u9762\u662fsinc \u51fd\u6570\u7684\u56fe\u5f62\u3002\u5bf9\u4e8ex,y \u8fd9\u4e2a\u4f4d\u7f6e, \u6062\u590d\u8fd9\u4e00\u70b9\u7684\u4fe1\u606f\u9700\u8981\u9644\u8fd1\u533a\u57df\u7684\u91c7\u6837\u503c\uff0c\u4e14\u8ddd\u79bbx,y \u8d8a\u8fdc\uff0c\u5bf9\u4e8e\u6062\u590d\u4fe1\u606f\u7684\u8d21\u732e\u8d8a\u5c0f\u3002 \u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48\u9700\u8981\u6269\u5145\u91c7\u6837\u7a7a\u95f4\u7684\u539f\u56e0\u3002\u5728\u5b9e\u9645\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u7ed9\u5b9a\u533a\u57df\u4e0d\u9700\u8981\u6269\u5145\u592a\u5927\uff0c\u56e0\u4e3asinc \u5448\u73b0\u4e86\u4e00\u4e2a\u968f\u8ddd\u79bb\u8870\u51cf\u7684\u8d8b\u52bf\u3002\u8bba\u6587\u5728\u5b9e\u73b0\u7684\u65f6\u5019\u4f1a\u5148\u5bf9\u7279\u5f81\u56fe\u505a\u4e00\u4e2a\u6269\u5145\uff0c\u4e5f\u5c31\u662f\u8fd9\u4e2a\u539f\u7406\u3002</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#equivariant-network-layers","title":"Equivariant Network Layers","text":"<p>translation equivariance transformation</p> \\[f\\cdot t = t\\cdot f\\] <p>\u5047\u8bbe\u8f93\u51fa\u7684\u91c7\u6837\u9891\u7387\u4e3a\\(s'\\)\uff0c \u90a3\u4e48\u6839\u636e\u91c7\u6837\u5b9a\u7406\uff0c\u9700\u8981\u6ee1\u8db3\\(f\\)\u4f5c\u7528\u4e4b\u540e\u7684\u4fe1\u53f7\u9891\u7387\u4e0d\u64cd\u4f5c\\(s'/2\\)\uff0c\u4e5f\u5c31\\(f\\)\u4e0d\u4ea7\u751f\u8d85\u8fc7\\(s'/2\\)\u7684\u9ad8\u9891\u4fe1\u53f7\u3002</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#convolution-layer","title":"Convolution Layer","text":""},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#convolution","title":"Convolution","text":"<p>Consider a standard convolution with a discrete kernel\\(K\\). We can interpret\\(K\\)as living in the same grid as the input feature map, with sampling rate\\(s\\). The discrete-domain operation is simply:</p> \\[ \\text{F}_{\\text{conv}}(Z) = K \\ast Z, \\] <p>and we obtain the corresponding continuous operation:</p> \\[ \\text{f}_{\\text{conv}}(z) = \\varphi_s \\ast \\left( K \\ast \\left( \\Pi_s \\odot z \\right) \\right) = K \\ast \\left( \\varphi_s \\ast \\left( \\Pi_s \\odot z \\right) \\right) = K \\ast z \\] <p>\u8fd9\u4e00\u6b65\u7528\u5230\u4e86\u5377\u79ef\u662f\u53ef\u4ee5\u4ea4\u6362\u7684\u3002</p> <p>\u540c\u65f6 \u6839\u636e\u5377\u79ef\u7684\u9891\u57df\u7b49\u4e8e\u9891\u57df\u7684\u4e58\u79ef</p> \\[ \\text{F}_{\\text{conv}}(K\\ast Z)(e^{jw}) =\\text{F}_{\\text{conv}}(Z)(e^{jw}) \\ast\\text{F}_{\\text{conv}}(K)(e^{jw}) \\] <p>\u4e5f\u5c31\u662f\u5bf9\u5e94\u9891\u7387\u7684\u7cfb\u6570\u7b49\u4e8e\u539f\u59cb\u4fe1\u53f7\u7684\u9891\u7387\u4e58\u4ee5\u5377\u79ef\u7684\u9891\u7387\u7cfb\u6570\u3002 \u56e0\u6b64\u5e26\u5bbd\u53ea\u4f1a\u53d8\u5c0f\uff0c\u4e0d\u4f1a\u53d8\u5927\u3002 \u8fd9\u5c31\u4f7f\u5f97\u5377\u79ef\u64cd\u4f5c\u5929\u7136\u6ee1\u8db3\u91c7\u6837\u5b9a\u7406\u7684\u8981\u6c42\u3002</p> <p>\u5377\u79ef\u5bf9\u5e94\u7684\u9891\u57df\u793a\u610f\u56fe </p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#upsample-and-downsample","title":"Upsample and Downsample","text":"<p>\u4e0a\u91c7\u6837 \u4e0a\u91c7\u6837\u589e\u52a0\u4e86\u4fe1\u53f7\u7684\u91c7\u6837\u7387\u3002\u8fd9\u91cc\u5047\u8bbe\u662f\u7406\u60f3\u4e0a\u91c7\u6837\uff0c\u4e5f\u5c31\u662f\u201c\u63d20\u201d + \u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2 \u7406\u60f3\u4e0a\u91c7\u6837\u540e\u7684\u4fe1\u53f7\u9891\u7387\u4f1a\u88ab\u538b\u7f29\\(n\\)\u500d.\\(n\\)\u662f\u4e0a\u91c7\u6837\u7684\u500d\u6570\u3002 \u7ecf\u8fc7\u7406\u60f3\u4e0a\u91c7\u6837\u540e\uff0c\u8f93\u51fa\\(Y(e^{j\\omega})\\)\u5b8c\u5168\u548c\u539f\u6765\u7684 \\(X(e^{j\\omega})\\)\u5728\u9891\u57df\u4e2d\u201c\u7f29\u653e\u201d\u540e\u7684\u4e3b\u74e3\u90e8\u5206\u76f8\u5bf9\u5e94\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u5b83\u5728\u65b0\u7684\u91c7\u6837\u7387\u4e0b\u5904\u4e8e\u5e26\u5bbd\\(\\pi/n\\)\u7684\u8303\u56f4\u4e4b\u5185\uff0c\u4e14\u643a\u5e26\u7740\u539f\u4fe1\u53f7\u4e0d\u5931\u771f\u7684\u9891\u8c31\u4fe1\u606f\u3002\u8fd9\u5c31\u662f\u201c\u7406\u60f3\u5347\u91c7\u6837\u201d\u8981\u8fbe\u5230\u7684\u76ee\u7684\u3002</p> <p>\u4e0b\u91c7\u6837</p> <p>\u4e0b\u91c7\u6837\u9891\u57df\u516c\u5f0f\u79bb\u6563\u5f62\u5f0f</p> \\[y[m] = x[m\\,M], \\quad Y(e^{j\\Omega}) = \\frac{1}{M}\\sum_{k=0}^{M-1} X\\!\\Bigl(e^{\\,j\\,\\tfrac{\\Omega + 2\\pi k}{M}}\\Bigr).\\] <p>\u8fde\u7eed\u5f62\u5f0f</p> \\[ Y_{\\mathrm{sample}}(\\omega) \\;=\\; \\frac{1}{M\\,T} \\sum_{k=-\\infty}^{\\infty} X\\!\\Bigl(\\omega - 2\\pi\\,\\tfrac{k}{M\\,T}\\Bigr). \\] <p>\u4e3a\u4e86\u6d88\u9664\u6df7\u53e0\u73b0\u8c61\uff0c\u9700\u8981\u8fc7\u6ee4\u6389\u9ad8\u9891\u4fe1\u53f7(f/2\u4ee5\u4e0a\u7684, f\u662f\u539f\u59cb\u4fe1\u53f7\u9891\u7387)\u518d\u8fdb\u884c\u4e0b\u91c7\u6837\u3002</p> <p>\u5047\u8bbe\u4e0b\u91c7\u6837\u500d\u6570\u4e3a\\(s' = s/n\\),\\(s\\)\u4e3a\u539f\u59cb\u91c7\u6837\u9891\u7387\uff0c\u90a3\u4e48</p> \\[ \\begin{aligned} F_{down} (Z)&amp; = \\mathrm{III}_{s'} \\odot \\left[  \\psi_{s'} \\ast \\left( \\phi_{s} \\ast Z \\right) \\right]\\\\ &amp; = \\frac{1}{s^2} \\mathrm{III}_{s'} \\odot \\left[  \\psi_{s'} \\ast \\psi_{s} \\ast Z \\right]\\\\ &amp; = \\frac{s'^2}{s^2} \\mathrm{III}_{s'} \\odot \\left[  \\phi_{s'} \\ast Z \\right] \\end{aligned} \\] <p>\u4e24\u4e2a\u4f4e\u901a\u6ee4\u6ce2\u7684\u5377\u79ef\u4efb\u7136\u662f\u4e00\u4e2a\u4f4e\u901a\u6ee4\u6ce2.\u5e26\u5bbd\u4e3a\u6700\u5c0f\u7684\u5e26\u5bbd\u3002 \\(\\left( \\phi_{s} \\ast Z \\right)\\)\u662f\u539f\u59cb\u91c7\u6837\u4fe1\u53f7\u7ecf\u8fc7\u4f4e\u901a\u6ee4\u6ce2\uff0c\u53bb\u9664\u79bb\u6563\u91c7\u7528\u5bfc\u81f4\u7684\u9ad8\u9891\u4fe1\u53f7\u3002 \u56e0\u4e3a\u9700\u8981\u4e0b\u91c7\u6837,\u9700\u8981\u989d\u5916\u79fb\u9664\u9ad8\u9891\u4fe1\u53f7\uff0c\u6240\u4ee5\u4f1a\u4f5c\u7528\u53e6\u5916\u4e00\u4e2a\\(\\psi_{s'}\\)\u4f4e\u901a\u6ee4\u6ce2\u3002\u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\u5668\u7684\u7cfb\u6570\u53ea\u6539\u53d8\u4fe1\u53f7\u6574\u4f53\u7684\u5f3a\u5ea6\uff0c\u76f8\u5f53\u4e8e\u5f52\u4e00\u4e0b\u3002\u5148\u5ffd\u7565\u6389\u7cfb\u6570\u7684\u5f71\u54cd\u3002 \u56e0\u4e3a\u5377\u79ef\u5177\u6709\u4ea4\u6362\u5f8b\uff0c\u53ef\u4ee5\u5148\u7ed3\u5408\u4e24\u4e2a\u7406\u60f3\u9ad8\u901a\u6ee4\u6ce2\u5668\uff0c\u4ece\u800c\u5f97\u5230\u6700\u7ec8\u7684\u7ed3\u679c\u3002 \u8fd9\u4e2a\u7ed3\u679c\u76f4\u89c2\u8868\u793a\u4e3a \u4e0b\u91c7\u6837\u4e4b\u540e\u7684\u4fe1\u53f7\u548c\u539f\u59cb\u4fe1\u53f7\u6309\u7167\u6700\u65b0\u7684\u4e0b\u91c7\u6837\u9891\u7387\u8fdb\u884c\u91c7\u6837\u65f6\u4e00\u81f4\u7684\uff0c\u90fd\u6ee1\u8db3\u91c7\u6837\u5b9a\u7406\uff0c\u524d\u63d0\u662f\u539f\u59cb\u4fe1\u53f7\u9700\u8981\u6ee1\u8db3\u5e26\u9650\u6027\uff0c\u5e76\u4e14\u5e26\u5bbd\\(f&lt;\\frac{1}{2}s'\\)\u3002</p> <p>\u4e0b\u91c7\u6837\u5177\u6709translation \u4e0d\u53d8\u6027\uff0c\u5373\u5e73\u79fb\u4e4b\u540e\u518d\u4e0b\u91c7\u6837\u548c\u4e0b\u91c7\u6837\u4e4b\u540e\u518d\u5e73\u79fb\u5f97\u5230\u7684\u7ed3\u679c\u662f\u4e00\u6837\u7684\u3002 \u4f46\u662f\u5bf9\u4e8e\u65cb\u8f6c\u5e76\u4e0d\u4fdd\u6301\u4e0d\u53d8\u6027\u3002\u5982\u679c\u8981\u4fdd\u6301\u9009\u62e9\u4e0d\u53d8\u6027\uff0c\u9700\u8981\u5f15\u5165\u7406\u60f3\u5706\u76d8\u4f4e\u7b52\u6ee4\u6ce2\u5668\uff0c i.e., 2D Jinc filter</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#2d-jinc-filter-frequency-spectrum","title":"2D \"Jinc\" Filter Frequency Spectrum","text":""},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#spatial-domain-definition","title":"Spatial Domain Definition","text":"<p>In the spatial domain, the 2D \"Jinc\" filter is:</p> \\[ h(x, y) = 2 \\frac{J_1(2\\pi r)}{r}, \\quad r = \\sqrt{x^2 + y^2}. \\] <p>at \\(r = 0\\),\\(h(0, 0) = 1\\).</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#fourier-transform","title":"Fourier Transform","text":"<p>The frequency response is:</p> \\[ H(\\rho) = 2\\pi \\int_0^\\infty h(r) r J_0(2\\pi \\rho r) \\, dr, \\] <p>where: -\\(\\rho = \\sqrt{u^2 + v^2}\\), -\\(J_0\\)is the zero-order Bessel function. Substitute\\(h(r)\\):</p> \\[ H(\\rho) = 4\\pi \\int_0^\\infty J_1(2\\pi r) J_0(2\\pi \\rho r) \\, dr. \\] <p>Using Bessel orthogonality:</p> \\[ H(\\rho) = \\begin{cases} 1, &amp; \\rho \\leq 1, \\\\ 0, &amp; \\rho &gt; 1. \\end{cases} \\] <p>\u6839\u636e\u5085\u91cc\u53f6\u53d8\u6362\u7406\u8bba\uff0c\u53ef\u77e5\u7a7a\u95f4\u7684\u65cb\u8f6c\u5bf9\u5e94\u7740\u9891\u57df\u7684\u65cb\u8f6c\uff0c\u9891\u57df\u7684\u65cb\u8f6c\u5bf9\u5e94\u7a7a\u95f4\u7684\u65cb\u8f6c\u3002\u5373\u65cb\u8f6c\u7b49\u53d8\u6027\u3002 \u5982\u679c\u4e00\u4e2a\u64cd\u4f5c\u5728\u7a7a\u95f4\u4e0a\u6ee1\u8db3\u65cb\u8f6c\u4e0d\u53d8\u6027\uff0c\u5c31\u9700\u8981\u5728\u9891\u57df\u4e0a\u6ee1\u8db3\u65cb\u8f6c\u4e0d\u53d8\u6027\u3002\u9891\u57df\u4e0a\u66f4\u5bb9\u6613\u7406\u89e3\u3002\u56e0\u6b64\u6211\u4eec\u53ea\u8981\u8003\u8651\u9891\u57df\u5c31\u884c\u3002\u9891\u57df\u6ee1\u8db3\u65cb\u8f6c\u4e0d\u53d8\u6027\u9996\u5148\u5c31\u9700\u8981\u9891\u57df\u662f\u5f84\u5411\u5bf9\u79f0\u7684\uff0c\u8fd9\u4e5f\u662f\u4e3a\u4ec0\u4e48\u9700\u8981\u4e00\u4e2a\u5f84\u5411\u5bf9\u79f0\u7406\u60f3\u6ee4\u6ce2\u5668(2D Jinc filter)\u7684\u539f\u56e0\u3002\u5f53\u7136\u8fdb\u4e00\u6b65\u4e25\u683c\u5b9e\u73b0\u9009\u62e9\u4e0d\u53d8\u6027\u9700\u8981\u4e0b\u91c7\u6837\u662f\u4f5c\u7528\u5728\u6781\u5750\u6807\u7a7a\u95f4\u4e0b\u3002</p> <p>\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570 pointwise\u64cd\u4f5c\u5929\u7136\u6ee1\u8db3 \u51e0\u4f55\u7b49\u53d8\u6027\u3002\u4e0b\u9762\u8003\u8651\u5e26\u5bbd\u7684\u6761\u4ef6\u3002 </p> <p>\u4e3e\u4e00\u4e2a\u5177\u4f53\u7684\u4f8b\u5b50\uff0c\u4f8b\u5982\\(x^2\\), - \u539f\u59cb\u4fe1\u53f7     \u8bbe\u4e00\u4e2a\u4fe1\u53f7\u7684\u5085\u91cc\u53f6\u5c55\u5f00\u4e3a\uff1a</p> <p>$$     x(t) = \\sum_{k} a_k e^{j 2\\pi f_k t},    $$</p> <pre><code>\u5176\u4e2d\uff1a\n-$a_k$\uff1a\u4fe1\u53f7\u7684\u5085\u91cc\u53f6\u7cfb\u6570\u3002\n-$f_k$\uff1a\u4fe1\u53f7\u7684\u9891\u7387\u5206\u91cf\u3002\n</code></pre> <ul> <li>\u975e\u7ebf\u6027\u64cd\u4f5c\u540e\u7684\u9891\u8c31**     \u5047\u8bbe\u5bf9\u4fe1\u53f7\u8fdb\u884c\u975e\u7ebf\u6027\u64cd\u4f5c\\(f(x(t))\\)\uff0c\u5982\u5e73\u65b9\u64cd\u4f5c\\(x^2(t)\\)\uff0c\u5219\u7ed3\u679c\u4e3a\uff1a</li> </ul> <p>$$     f(x(t)) \\sim \\sum_{k,l} g(a_k, a_l) e^{j 2\\pi (f_k + f_l)t},    $$</p> <pre><code>\u5176\u4e2d\uff1a\n-$g(a_k, a_l)$\uff1a\u975e\u7ebf\u6027\u64cd\u4f5c\u4ea7\u751f\u7684\u7cfb\u6570\u3002\n-$f_k + f_l$\uff1a\u65b0\u4ea7\u751f\u7684\u9891\u7387\u5206\u91cf\u3002\n</code></pre> <p>\u90a3\u8981\u600e\u4e48\u5b9e\u73b0\u975e\u7ebf\u6027\u64cd\u4f5c\u7684\u5e26\u5bbd\u9650\u5236</p> <p>\u975e\u7ebf\u6027\u64cd\u4f5c\u540e\uff0c\u7ed3\u5408\u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\u5668\\(\\psi_s\\)\uff0c\u8fde\u7eed\u8868\u793a\u4e3a\uff1a</p> \\[ f_\\sigma(z) = \\psi_s * \\sigma(z), \\] <p>\u5176\u4e2d\uff1a -\\(\\psi_s\\)\u662f\u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\u5668\u3002 -\\(\\sigma(z)\\)\u662f\u975e\u7ebf\u6027\u64cd\u4f5c\u540e\u7684\u4fe1\u53f7\u3002 -\\(*\\)\u8868\u793a\u5377\u79ef\u3002</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#_5","title":"\u79bb\u6563\u5f62\u5f0f","text":"<p>\u5bf9\u5e94\u7684\u79bb\u6563\u7248\u672c\u8868\u793a\u4e3a\uff1a</p> \\[ F_\\sigma(Z) = s^2 \\cdot \\Pi_s \\odot (\\phi_s _\\sigma(\\phi_s_ Z)), \\] <p>\u5176\u4e2d\uff1a -\\(s^2\\)\u662f\u6bd4\u4f8b\u56e0\u5b50\u3002 -\\(\\Pi_s\\)\u8868\u793a\u964d\u91c7\u6837\u64cd\u4f5c\u3002 -\\(\\odot\\)\u8868\u793a\u9010\u70b9\u4e58\u79ef\uff08Hadamard\u4e58\u79ef\uff09\u3002 -\\(\\phi_s\\)\u662f\u63d2\u503c\u6ee4\u6ce2\u5668\u3002 -\\(\\sigma(\\phi_s * Z)\\)\u8868\u793a\u5728\u4e0a\u91c7\u6837\u540e\u5e94\u7528\u975e\u7ebf\u6027\u64cd\u4f5c\u3002</p> <p>\u5176\u4e2d\\(\\sigma(\\phi_s * Z)\\)\u9700\u8981\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u8fdb\u884c\u64cd\u4f5c\u3002\u4e3a\u4ec0\u4e48\u4e0d\u80fd\u5728\u79bb\u6563\u7a7a\u95f4\u4e2d\u5b9e\u73b0\uff1f</p> <p>\u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\u5668\u7684\u65e0\u9650\u652f\u6301\uff1a\u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\u5668\u7684\u9891\u57df\u662f\u4e00\u4e2a\u77e9\u5f62\u51fd\u6570\uff0c\u5176\u65f6\u57df\u54cd\u5e94\u662f\u4e00\u4e2a\u65e0\u9650\u957f\u7684 sinc \u51fd\u6570\u3002\u5728\u79bb\u6563\u57df\u4e2d\uff0c\u8fd9\u79cd\u65e0\u9650\u652f\u6301\u65e0\u6cd5\u76f4\u63a5\u5b9e\u73b0\uff0c\u53ea\u80fd\u901a\u8fc7\u622a\u65ad\u8fd1\u4f3c\uff0c\u4f46\u8fd9\u4f1a\u5f15\u5165\u5176\u4ed6\u8bef\u5dee\uff08\u5982\u632f\u94c3\u6548\u5e94\uff09\u3002 \u975e\u7ebf\u6027\u64cd\u4f5c\u4f1a\u5f15\u5165\u9ad8\u9891\u5206\u91cf\uff0c\u79bb\u6563\u57df\u4e2d\u5bb9\u6613\u4ea7\u751f\u9891\u8c31\u6df7\u53e0\u3002</p> <p>\u56e0\u6b64\u4f5c\u8005\u7684\u60f3\u6cd5\u662f\u7528\u4e0a\u91c7\u6837\u6a21\u62df\u8fde\u7eed\u7a7a\u95f4\uff0c\u5148\u4e0a\u91c7\u6837\uff0c\u518d\u505a\u975e\u7ebf\u6027\u64cd\u4f5c\uff0c\u518d\u4e0b\u91c7\u6837\u3002\u5f53\u7136\uff0c\u8981\u6ee1\u8db3\u65cb\u8f6c\u4e0d\u53d8\u6027\uff0c\u5219\u9700\u8981\u4e0b\u91c7\u6837\u5e94\u7528 2D Jinc filter\u3002</p> <p>\u800c\u4e14\u975e\u7ebf\u6027\u64cd\u4f5c\u4e5f\u662f\u7f51\u7edc\u4e2d\u552f\u4e00\u751f\u6210\u989d\u5916\u9ad8\u9891\u4fe1\u606f\u7684\u64cd\u4f5c\u3002\u53ef\u4ee5\u63a7\u5236\u5979\u4ece\u800c\u63a7\u5236\u6bcf\u4e00\u5c42\u9700\u8981\u7684\u989d\u5916\u9ad8\u9891\u4fe1\u606f\u3002\u9ad8\u9891\u4fe1\u606f\u5373\u7eb9\u7406\u4fe1\u606f\u3002</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#practical-application","title":"Practical Application","text":"<p>descriminator \u4e0d\u53d1\u751f\u53d8\u5316</p> <p>generator \u76ee\u6807\uff1a \u8bbe\u8ba1\u7b49\u53d8\u6027\u795e\u7ecf\u7f51\u7edc \u7528PSNR\u503c\u8861\u91cf\u3002\u503c\u8d8a\u9ad8\u8868\u793a\u7b49\u53d8\u5f62\u8d8a\u5f3a\u3002</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#eq-t","title":"EQ-T \u516c\u5f0f","text":"\\[ \\text{EQ-T} = 10 \\cdot \\log_{10} \\left( \\frac{I_{\\text{max}}^2}{\\mathbb{E}_{\\text{w} \\sim \\mathcal{W}, x \\sim \\mathcal{X}^2, p \\sim \\mathcal{V}, c \\sim \\mathcal{C}} \\left[ \\left( g(t_x[z_0]; \\text{w})_c(p) - t_x[g(z_0; \\text{w})]_c(p) \\right)^2 \\right]} \\right) \\]"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#_6","title":"\u8bf4\u660e","text":"<p>-\\(I_{\\text{max}}\\): \u56fe\u50cf\u50cf\u7d20\u7684\u6700\u5927\u53ef\u80fd\u503c\uff08\u5982 255\uff09\u3002 -\\(\\mathbb{E}\\): \u8868\u793a\u671f\u671b\u503c\u3002 -\\(\\text{w} \\sim \\mathcal{W}\\): \u6743\u91cd\u4ece\u5206\u5e03\\(\\mathcal{W}\\)\u4e2d\u91c7\u6837\u3002 -\\(x \\sim \\mathcal{X}^2\\): \u8f93\u5165\u4ece\u5206\u5e03\\(\\mathcal{X}^2\\)\u4e2d\u91c7\u6837\u3002 -\\(p \\sim \\mathcal{V}\\): \u50cf\u7d20\u5750\u6807\u4ece\u7a7a\u95f4\u5206\u5e03\\(\\mathcal{V}\\)\u4e2d\u91c7\u6837\u3002 -\\(c \\sim \\mathcal{C}\\): \u901a\u9053\\(c\\)\u4ece\u901a\u9053\u5206\u5e03\\(\\mathcal{C}\\)\u4e2d\u91c7\u6837\u3002 -\\(g(\\cdot; \\text{w})\\): \u8868\u793a\u56fe\u50cf\u64cd\u4f5c\u6216\u6a21\u578b\u751f\u6210\u51fd\u6570\u3002 -\\(t_x\\): \u51e0\u4f55\u53d8\u6362\uff08\u5982\u5e73\u79fb\u3001\u65cb\u8f6c\u3001\u7f29\u653e\u7b49\uff09\u3002 -\\(z_0\\): \u8f93\u5165\u566a\u58f0\u6216\u6f5c\u5728\u53d8\u91cf\u3002 \u4ece\u8fd9\u4e2a\u516c\u5f0f\u53ef\u4ee5\u770b\u5230, \u5728\u65cb\u8f6c\u548c\u5e73\u79fb\u662f\u4f5c\u7528\u5728 \u521d\u59cb\u8f93\u5165\\(z_0\\)\u4e0a\u7684\uff0c\u800c\u4e0d\u662flatent code\\(w\\).</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#fourier","title":"Fourier \u7279\u5f81","text":"<p>\u7528 Fourier \u7279\u5f81\u66ff\u6362 StyleGAN2 \u4e2d\u7684\u5b66\u4e60\u8f93\u5165\u5e38\u6570\uff0c\u4fbf\u4e8e\u7cbe\u786e\u8fde\u7eed\u53d8\u6362\u8f93\u5165\uff0c\u6539\u5584\u4e86 FID \u5e76\u80fd\u8ba1\u7b97\u7b49\u53d8\u6027\u6307\u6807\uff0c\u4f46\u521d\u59cb\u67b6\u6784\u79bb\u7b49\u53d8\u6027\u4ecd\u8fdc\u3002 SynthesisInput Forward \u8ba1\u7b97\u6d41\u7a0b\u6570\u5b66\u516c\u5f0f \u8f93\u5165\u5b9a\u4e49\u548c\u7b26\u53f7\u8bf4\u660e \u5047\u8bbe\uff1a -\\(\\text{w} \\in \\mathbb{R}^{B \\times d}\\)\uff1a\u8f93\u5165\u6837\u5f0f\u5411\u91cf\uff0c\u6765\u81ea\u6f5c\u5728\u7a7a\u95f4\u7684\u6620\u5c04\u7f51\u7edc\u3002 -\\(\\text{f} \\in \\mathbb{R}^{C \\times 2}\\)\uff1a\u53ef\u8bad\u7ec3\u7684\u9891\u7387\u5411\u91cf\u3002 -\\(\\text{p} \\in \\mathbb{R}^C\\)\uff1a\u53ef\u8bad\u7ec3\u7684\u76f8\u4f4d\u5411\u91cf\u3002 -\\(\\text{A} \\in \\mathbb{R}^{C \\times C}\\)\uff1a\u53ef\u8bad\u7ec3\u7684\u7ebf\u6027\u6620\u5c04\u6743\u91cd\u3002 -\\(\\text{T} \\in \\mathbb{R}^{3 \\times 3}\\)\uff1a\u7528\u6237\u5b9a\u4e49\u7684\u53d8\u6362\u77e9\u9635\u3002 -\\(H, W\\)\uff1a\u8f93\u51fa\u7279\u5f81\u56fe\u7684\u7a7a\u95f4\u5c3a\u5bf8\u3002 -\\(\\text{bandwidth}\\)\u548c\\(\\text{sampling\\_rate}\\)\uff1a\u9891\u7387\u8303\u56f4\u548c\u91c7\u6837\u7387\u3002 \u521d\u59cb\u5316\u9891\u7387\u548c\u76f8\u4f4d \u9891\u7387\u5f52\u4e00\u5316 \u5bf9\u4e8e\u6bcf\u4e2a\u901a\u9053\u7684\u9891\u7387\u5411\u91cf\\(\\text{f}_i \\in \\mathbb{R}^2\\)\uff0c\u8fdb\u884c\u5f52\u4e00\u5316\uff1a</p> \\[ \\text{f}_i' = \\frac{\\text{f}_i}{\\|\\text{f}_i\\|^2} \\cdot \\|\\text{f}_i\\|^{0.25} \\cdot \\text{bandwidth}, \\quad \\forall i \\in [1, C] \\] <p>\u5176\u4e2d\uff1a -\\(\\|\\text{f}_i\\|^2 = \\text{f}_i \\cdot \\text{f}_i\\)\u662f\u9891\u7387\u5411\u91cf\u7684\u5e73\u65b9\u8303\u6570\u3002 \u968f\u673a\u521d\u59cb\u5316\u76f8\u4f4d \u76f8\u4f4d\\(\\text{p}_i\\)\u521d\u59cb\u5316\u4e3a\u5747\u5300\u5206\u5e03\uff1a</p> \\[ \\text{p}_i \\sim \\text{Uniform}(-0.5, 0.5), \\quad \\forall i \\in [1, C] \\] <p>\u6837\u5f0f\u5411\u91cf\u5230\u53d8\u6362\u53c2\u6570 \u4ece\u6837\u5f0f\u5411\u91cf\\(\\text{w}\\)\u6620\u5c04\u5230\u53d8\u6362\u53c2\u6570\\(\\text{t}\\)\uff1a</p> \\[ \\text{t} = \\text{Affine}(\\text{w}), \\quad \\text{t} = [r_c, r_s, t_x, t_y] \\in \\mathbb{R}^{B \\times 4} \\] <p>\u5f52\u4e00\u5316\u65cb\u8f6c\u53c2\u6570 \u5bf9\u65cb\u8f6c\u53c2\u6570\u8fdb\u884c\u5f52\u4e00\u5316\uff1a</p> \\[ r_c' = \\frac{r_c}{\\sqrt{r_c^2 + r_s^2}}, \\quad r_s' = \\frac{r_s}{\\sqrt{r_c^2 + r_s^2}} \\] <p>\u6784\u9020\u53d8\u6362\u77e9\u9635 \u6784\u9020\u65cb\u8f6c\u77e9\u9635\u548c\u5e73\u79fb\u77e9\u9635\uff1a</p> \\[ \\text{M}_r=\\begin{bmatrix} r_c'&amp;-r_s'&amp;0;\\\\ r_s'&amp;r_c'&amp;0;\\\\ 0&amp;0&amp;1; \\end{bmatrix}, \\text{M}_t=\\begin{bmatrix} 1&amp;0&amp;-t_x;\\\\ 0&amp;1&amp;-t_y;\\\\ 0&amp;0&amp;1;\\end{bmatrix}\\] <p>\u7528\u6237\u5b9a\u4e49\u7684\u53d8\u6362\u77e9\u9635\\(T_\\text{user}\\)\u548c\u4e0a\u8ff0\u77e9\u9635\u7ec4\u5408\u5f97\u5230\u6700\u7ec8\u7684\u53d8\u6362\u77e9\u9635\uff1a</p> \\[ T = M_r M_t T_\\text{user} \\] <p>\u9891\u7387\u548c\u76f8\u4f4d\u7684\u53d8\u6362 \u5c06\u53d8\u6362\u77e9\u9635\u5e94\u7528\u4e8e\u9891\u7387\u548c\u76f8\u4f4d\uff1a</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#31","title":"3.1 \u9891\u7387\u53d8\u6362","text":"\\[ \\text{f}_i'' = \\text{f}_i' \\cdot \\text{T}_{1:2, 1:2}, \\quad \\forall i \\in [1, C] \\] <p>\u76f8\u4f4d\u53d8\u6362</p> \\[ \\text{p}_i'' = \\text{p}_i + \\text{f}_i' \\cdot \\text{T}_{1:2, 2:3}, \\quad \\forall i \\in [1, C] \\] <p>\u9891\u7387\u5e45\u503c\u8c03\u6574 \u4e3a\u4e86\u6291\u5236\u8d85\u51fa\u9891\u7387\u8303\u56f4\u7684\u5206\u91cf\uff0c\u8c03\u6574\u5e45\u503c\uff1a</p> \\[ \\text{a}_i = \\max\\left(0, 1 - \\frac{\\|\\text{f}_i''\\| - \\text{bandwidth}}{\\frac{\\text{sampling\\_rate}}{2} - \\text{bandwidth}}\\right), \\quad \\forall i \\in [1, C] \\] <p>\u6784\u9020\u5085\u91cc\u53f6\u7279\u5f81 \u91c7\u6837\u7f51\u683c \u5b9a\u4e49\u91c7\u6837\u7f51\u683c\\(\\text{G} \\in \\mathbb{R}^{H \\times W \\times 2}\\)\uff1a</p> \\[ \\text{G}[h, w] = \\left(\\frac{w}{W} - 0.5, \\frac{h}{H} - 0.5\\right), \\quad \\forall h \\in [0, H], w \\in [0, W] \\] <p>\u5085\u91cc\u53f6\u7279\u5f81\u8ba1\u7b97 \u901a\u8fc7\u9891\u7387\u548c\u76f8\u4f4d\u751f\u6210\u5085\u91cc\u53f6\u7279\u5f81\uff1a</p> \\[ \\text{x}_\\text{fourier}[b, h, w, c] = \\sin\\left(2 \\pi (\\text{G}[h, w] \\cdot \\text{f}_c'' + \\text{p}_c'')\\right) \\cdot \\text{a}_c \\] <p>\u5e94\u7528\u7ebf\u6027\u6620\u5c04 \u4f7f\u7528\u53ef\u8bad\u7ec3\u7684\u7ebf\u6027\u6743\u91cd\\(\\text{A}\\)\u5bf9\u5085\u91cc\u53f6\u7279\u5f81\u8fdb\u884c\u901a\u9053\u6620\u5c04\uff1a</p> \\[ \\text{x}_\\text{out}[b, c, h, w] = \\sum_{c'=1}^C \\text{x}_\\text{fourier}[b, h, w, c'] \\cdot \\frac{\\text{A}_{c, c'}}{\\sqrt{C}} \\] <p>\u6700\u7ec8\u516c\u5f0f\u603b\u7ed3 \u5b8c\u6574\u7684 <code>SynthesisInput</code> forward \u8ba1\u7b97\u6d41\u7a0b\uff1a</p> \\[ \\text{x}_\\text{out}[b, c, h, w] = \\sum_{c'=1}^C \\left[ \\sin\\left(2 \\pi (\\text{G}[h, w] \\cdot \\text{f}_{c'}'' + \\text{p}_{c'}'')\\right) \\cdot \\text{a}_{c'} \\right] \\cdot \\frac{\\text{A}_{c, c'}}{\\sqrt{C}} \\] <p>\u5176\u4e2d\uff1a -\\(\\text{f}_c''\\)\u548c\\(\\text{p}_c''\\)\u662f\u7ecf\u8fc7\u6837\u5f0f\u53d8\u6362\u540e\u7684\u9891\u7387\u548c\u76f8\u4f4d\u3002 -\\(\\text{a}_c\\)\u662f\u5e45\u503c\u8c03\u6574\u56e0\u5b50\u3002 -\\(\\text{A}\\)\u662f\u901a\u9053\u6620\u5c04\u7684\u6743\u91cd\u3002 \u7269\u7406\u610f\u4e49</p> <ol> <li>\u901a\u8fc7\u9891\u7387\\(\\text{f}\\)\u548c\u76f8\u4f4d\\(\\text{p}\\)\u751f\u6210\u5085\u91cc\u53f6\u7279\u5f81\uff0c\u6784\u5efa\u7a7a\u95f4\u4f4d\u7f6e\u548c\u6837\u5f0f\u76f8\u5173\u7684\u521d\u59cb\u7279\u5f81\u3002</li> <li>\u7ebf\u6027\u6620\u5c04\\(\\text{A}\\)\u5b66\u4e60\u4e0d\u540c\u9891\u7387\u548c\u76f8\u4f4d\u7684\u7ec4\u5408\u5173\u7cfb\uff0c\u4e3a\u751f\u6210\u5668\u63d0\u4f9b\u4efb\u52a1\u76f8\u5173\u7684\u7279\u5f81\u3002 \u5e73\u79fb\u4e0d\u4f1a\u6539\u53d8\u4fe1\u53f7\u7684\u9891\u7387\u5206\u91cf\uff08\u632f\u5e45\u4e0d\u53d8\uff09\uff0c\u4f46\u4f1a\u5f15\u5165\u4e0e\u5e73\u79fb\u91cf\u6210\u6bd4\u4f8b\u7684\u76f8\u4f4d\u504f\u79fb\u3002 \u65cb\u8f6c\u4f1a\u540c\u65f6\u6539\u53d8\u7a7a\u95f4\u57df\u548c\u9891\u57df\u4fe1\u53f7\u7684\u65b9\u5411\uff0c\u4f46\u9891\u7387\u7684\u5e45\u5ea6\u4e0d\u53d8</li> </ol>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#_7","title":"\u4ee3\u7801\u89e3\u8bfb","text":""},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#modulated_conv2d","title":"modulated_conv2d","text":"<pre><code>def modulated_conv2d(\n    x,                  # Input tensor: [batch_size, in_channels, in_height, in_width]\n    w,                  # Weight tensor: [out_channels, in_channels, kernel_height, kernel_width]\n    s,                  # Style tensor: [batch_size, in_channels]\n    demodulate  = True, # Apply weight demodulation?\n    padding     = 0,    # Padding: int or [padH, padW]\n    input_gain  = None, # Optional scale factors for the input channels: [], [in_channels], or [batch_size, in_channels]\n):\n    with misc.suppress_tracer_warnings(): # this value will be treated as a constant\n        batch_size = int(x.shape[0])\n    out_channels, in_channels, kh, kw = w.shape\n    misc.assert_shape(w, [out_channels, in_channels, kh, kw]) # [OIkk]\n    misc.assert_shape(x, [batch_size, in_channels, None, None]) # [NIHW]\n    misc.assert_shape(s, [batch_size, in_channels]) # [NI]\n\n    # Pre-normalize inputs.\n    if demodulate:\n        w = w * w.square().mean([1,2,3], keepdim=True).rsqrt()\n        s = s * s.square().mean().rsqrt()\n\n    # Modulate weights.\n    w = w.unsqueeze(0) # [NOIkk] # \u589e\u52a0batch\u7ef4\u5ea6\n    w = w * s.unsqueeze(1).unsqueeze(3).unsqueeze(4) # [NOIkk]\n\n    # Demodulate weights.\n    if demodulate:\n        dcoefs = (w.square().sum(dim=[2,3,4]) + 1e-8).rsqrt() # [NO]\n        w = w * dcoefs.unsqueeze(2).unsqueeze(3).unsqueeze(4) # [NOIkk]\n\n    # Apply input scaling.\n    if input_gain is not None:\n        input_gain = input_gain.expand(batch_size, in_channels) # [NI]\n        w = w * input_gain.unsqueeze(1).unsqueeze(3).unsqueeze(4) # [NOIkk]\n\n    # Execute as one fused op using grouped convolution.\n    x = x.reshape(1, -1, *x.shape[2:])\n    w = w.reshape(-1, in_channels, kh, kw)\n    x = conv2d_gradfix.conv2d(input=x, weight=w.to(x.dtype), padding=padding, groups=batch_size)\n    x = x.reshape(batch_size, -1, *x.shape[2:])\n    return x\n</code></pre>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#_8","title":"\u6700\u7ec8\u6570\u5b66\u8868\u8fbe\u5f0f","text":"<p>\u7ecf\u8fc7\u4ee5\u4e0a\u6b65\u9aa4\uff0c\u6700\u7ec8\u8f93\u51fa \\( y \\) \u7684\u6570\u5b66\u8868\u8fbe\u4e3a\uff1a</p> \\[ y_{b,o,h',w'} = \\sum_{i,k_h,k_w} \\left[ x_{b,i,h+k_h,w+k_w} \\cdot \\left( w_{o,i,k_h,k_w} \\cdot s_{b,i} \\cdot d_{\\text{coef},o} \\cdot \\text{input\\_gain}_{b,i} \\right) \\right] \\] <p>\u5176\u4e2d\uff1a</p> <ul> <li>$ b $: \u6279\u91cf\u7d22\u5f15\u3002</li> <li>$ o, i $: \u8f93\u51fa\u901a\u9053\u548c\u8f93\u5165\u901a\u9053\u7d22\u5f15\u3002</li> <li>$ h', w'$: \u8f93\u51fa\u7279\u5f81\u56fe\u7684\u4f4d\u7f6e\u3002</li> <li>$ k_h, k_w $: \u5377\u79ef\u6838\u7684\u7a7a\u95f4\u7ef4\u5ea6\u7d22\u5f15\u3002</li> </ul>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#_9","title":"\u76f4\u89c2\u610f\u4e49","text":"<ol> <li>\u6837\u5f0f $ s $ \u52a8\u6001\u5730\u8c03\u8282\u6743\u91cd\uff0c\u8c03\u6574\u6bcf\u4e2a\u5377\u79ef\u6838\u7684\u6743\u91cd\u3002\u540c\u4e00\u4e2abatch\u91cc\u4e0d\u540c\u7684\u4e0d\u540c\u7684\u6837\u672cstyle\u662f\u72ec\u7acb\u7684\u3002\u4f53\u73b0\u4e86\u751f\u6210\u5f0f\u6a21\u578b\u4e2d\u7279\u5f81\u7684\u4e2a\u6027\u5316\u8c03\u6574\u3002</li> <li>\u5f52\u4e00\u5316\u548c\u53bb\u5f52\u4e00\u5316\u8fc7\u7a0b\u907f\u514d\u4e86\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u3002\u6743\u91cd\u5f52\u4e00\u5316\u4f5c\u7528\u5728i,k,h \u7ef4\u5ea6\u4e0a\uff0c\u4e5f\u5c31\u662f\u4e0d\u540c\u7684\u5377\u79ef\u6838\u4e0a\u505anormalize\uff0c\u5177\u6709\u76f8\u540c\u7684\u8303\u6570\u3002\u5bf9style \u7684\u5f52\u4e00\u5316\u4f5c\u7528\u5728\u6240\u6709\u7684\u7ef4\u5ea6\u4e0a\u3002\u4e5f\u5c31\u662f\u6574\u4e2as \u5f52\u4e00\u5316\u4e4b\u540e\u7684\u8303\u6570\u4e3a1.</li> <li>\u5206\u7ec4\u5377\u79ef\u9ad8\u6548\u5730\u5b9e\u73b0\u4e86\u6bcf\u4e2a\u6837\u672c\u72ec\u7acb\u7684\u6743\u91cd\u8c03\u5236\u8ba1\u7b97\u3002</li> </ol>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#synthesislayer","title":"SynthesisLayer","text":"<pre><code># Track input magnitude.\nif update_emas:\n    with torch.autograd.profiler.record_function('update_magnitude_ema'):\n        magnitude_cur = x.detach().to(torch.float32).square().mean()\n        self.magnitude_ema.copy_(magnitude_cur.lerp(self.magnitude_ema, self.magnitude_ema_beta))\ninput_gain = self.magnitude_ema.rsqrt()\n\n# Execute affine layer.\nstyles = self.affine(w)\nif self.is_torgb:\n    weight_gain = 1 / np.sqrt(self.in_channels * (self.conv_kernel ** 2))\n    styles = styles * weight_gain\n\n# Execute modulated conv2d.\ndtype = torch.float16 if (self.use_fp16 and not force_fp32 and x.device.type == 'cuda') else torch.float32\nx = modulated_conv2d(x=x.to(dtype), w=self.weight, s=styles,\n    padding=self.conv_kernel-1, demodulate=(not self.is_torgb), input_gain=input_gain)\n</code></pre>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#_10","title":"\u6570\u5b66\u516c\u5f0f","text":"<ol> <li>\u5f53\u524d\u8f93\u5165\u5e45\u503c\u7684\u5e73\u65b9\u5747\u503c\uff1a</li> </ol> <p>$$    \\text{magnitude_cur} = \\frac{1}{N} \\sum_i x_i^2    $$</p> <p>\u5176\u4e2d\uff1a    - \\( x \\): \u8f93\u5165\u7279\u5f81\u3002    - \\( N \\): \u8f93\u5165\u7279\u5f81\u4e2d\u5143\u7d20\u7684\u603b\u6570\u3002</p> <ol> <li>EMA \u66f4\u65b0\uff1a</li> </ol> <p>$$    \\text{magnitude_ema} = \\beta \\cdot \\text{magnitude_ema} + (1 - \\beta) \\cdot \\text{magnitude_cur}    $$</p> <p>\u5176\u4e2d\uff1a    - $ \\beta\\(: EMA \u7684\u5e73\u6ed1\u7cfb\u6570 (\\) 0 &lt; \\beta &lt; 1 $)\u3002    - $ \\text{magnitude_cur} $: \u5f53\u524d\u8f93\u5165\u5e45\u503c\u7684\u5e73\u65b9\u5747\u503c\u3002    - $ \\text{magnitude_ema} $: \u5386\u53f2\u5e73\u6ed1\u7684\u8f93\u5165\u5e45\u503c\u3002</p> <ol> <li>\u8f93\u5165\u589e\u76ca\uff1a</li> </ol> <p>$$    \\text{input_gain} = \\frac{1}{\\sqrt{\\text{magnitude_ema}}}    $$</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#_11","title":"\u516c\u5f0f\u7684\u76f4\u89c2\u610f\u4e49","text":"<ul> <li>\u52a8\u6001\u5f52\u4e00\u5316\uff1a\u901a\u8fc7 \\(\\text{input\\_gain}\\) \u8c03\u6574\u8f93\u5165\u7279\u5f81\u7684\u5e45\u503c\uff0c\u786e\u4fdd\u5176\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u9002\u5f53\u8303\u56f4\u3002</li> <li>\u5e73\u6ed1\u66f4\u65b0\uff1aEMA \u63d0\u4f9b\u4e86\u4e00\u79cd\u5e73\u6ed1\u7684\u5386\u53f2\u5e45\u503c\u4f30\u8ba1\uff0c\u907f\u514d\u8bad\u7ec3\u4e2d\u7684\u5267\u70c8\u6ce2\u52a8\u3002</li> <li>\u6570\u503c\u7a33\u5b9a\u6027\uff1a\u9650\u5236\u8f93\u5165\u7279\u5f81\u5e45\u503c\u7684\u8fc7\u5927\u6216\u8fc7\u5c0f\uff0c\u786e\u4fdd\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u7a33\u5b9a\u3002     input_gain \u4f1a\u4f5c\u7528\u5728\u4e4b\u540e\u7684\u5377\u79ef\u4e0a\uff0c\u4f5c\u7528\u76f8\u5f53\u4e8e\u5bf9X\u505a\u4e86\u4e00\u4e2a\u5f52\u4e00\u5316\u3002\u53ef\u4ee5\u79f0\u4e4b\u4e3a\u6ed1\u52a8\u5e73\u5747\u5f52\u4e00\u5316\u3002\u76ee\u524d\u6240\u6709\u7684\u5f52\u4e00\u5316\u90fd\u4e0d\u6539\u53d8mean\u3002</li> </ul>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#filtered_lrelu","title":"filtered_lrelu","text":"<pre><code>x = filtered_lrelu.filtered_lrelu(x=x, fu=self.up_filter, fd=self.down_filter, b=self.bias.to(x.dtype),\n            up=self.up_factor, down=self.down_factor, padding=self.padding, gain=gain, slope=slope, clamp=self.conv_clamp)\n</code></pre> <p>https://github.com/NVlabs/stylegan3/blob/main/torch_utils/ops/filtered_lrelu.py \u4f5c\u8005\u628a\u4e0a\u91c7\u6837\u4e0b\u91c7\u6837\u548c\u6fc0\u6d3b\u51fd\u6570\u91cd\u65b0\u5199\u4e86\u4e00\u4e2acuda kernel\u3002\u592a\u8fc7\u4e8e\u5de5\u7a0b\u5316\uff0c\u5148\u8df3\u8fc7\u8fd9\u90e8\u5206\u3002</p>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#synthesislayerdesign_lowpass_filter","title":"SynthesisLayer.design_lowpass_filter","text":"<pre><code>    @staticmethod\n    def design_lowpass_filter(numtaps, cutoff, width, fs, radial=False):\n        assert numtaps &gt;= 1\n\n        # Identity filter.\n        if numtaps == 1:\n            return None\n\n        # Separable Kaiser low-pass filter.\n        if not radial:\n            f = scipy.signal.firwin(numtaps=numtaps, cutoff=cutoff, width=width, fs=fs)\n            return torch.as_tensor(f, dtype=torch.float32)\n\n        # Radially symmetric jinc-based filter.\n        x = (np.arange(numtaps) - (numtaps - 1) / 2) / fs\n        r = np.hypot(*np.meshgrid(x, x))\n        f = scipy.special.j1(2 * cutoff * (np.pi * r)) / (np.pi * r)\n        beta = scipy.signal.kaiser_beta(scipy.signal.kaiser_atten(numtaps, width / (fs / 2)))\n        w = np.kaiser(numtaps, beta)\n        f *= np.outer(w, w)\n        f /= np.sum(f)\n        return torch.as_tensor(f, dtype=torch.float32)\n</code></pre>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#_12","title":"\u6ee4\u6ce2\u5668\u7684\u4f5c\u7528\u57df\u4e0e\u8bbe\u8ba1\u8fc7\u7a0b","text":""},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#_13","title":"\u4f5c\u7528\u57df","text":"<p>\u8fd9\u4e9b\u6ee4\u6ce2\u5668 \u4f5c\u7528\u5728\u65f6\u95f4\u57df\uff0c\u5b83\u4eec\u901a\u8fc7\u65f6\u95f4\u57df\u5377\u79ef\u64cd\u4f5c\u5904\u7406\u8f93\u5165\u4fe1\u53f7\u6216\u56fe\u50cf\u3002\u5377\u79ef\u516c\u5f0f\u4e3a\uff1a</p> \\[ y[n] = \\sum_{k} x[k] \\cdot h[n-k] \\] <p>\u5176\u4e2d\uff1a</p> <ul> <li>\\(x[k]\\): \u8f93\u5165\u4fe1\u53f7\u3002</li> <li>\\(h[k]\\): \u6ee4\u6ce2\u5668\u7684\u65f6\u95f4\u57df\u6743\u91cd\uff08\u8109\u51b2\u54cd\u5e94\uff09\u3002</li> <li>\\(y[n]\\): \u6ee4\u6ce2\u540e\u7684\u8f93\u51fa\u4fe1\u53f7\u3002</li> </ul>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#_14","title":"\u8bbe\u8ba1\u8fc7\u7a0b","text":"<p>\u5c3d\u7ba1\u6ee4\u6ce2\u5668\u4f5c\u7528\u5728\u65f6\u95f4\u57df\uff0c\u5176\u8bbe\u8ba1\u8fc7\u7a0b\u901a\u5e38\u57fa\u4e8e\u9891\u57df\u7684\u8981\u6c42\uff0c\u6bd4\u5982\uff1a</p> <ul> <li>\u622a\u6b62\u9891\u7387 (\\(\\text{cutoff}\\))\uff1a\u6307\u5b9a\u6ee4\u6ce2\u5668\u5141\u8bb8\u901a\u8fc7\u7684\u9891\u7387\u8303\u56f4\u3002</li> <li>\u8fc7\u6e21\u5e26\u5bbd (\\(\\text{width}\\))\uff1a\u63a7\u5236\u6ee4\u6ce2\u5668\u4ece\u901a\u5e26\u5230\u963b\u5e26\u7684\u8fc7\u6e21\u533a\u57df\u7684\u5bbd\u5ea6\u3002</li> <li>\u9891\u57df\u5f62\u72b6\uff1a\u6ee4\u6ce2\u5668\u7684\u8bbe\u8ba1\u76ee\u6807\u662f\u5b9e\u73b0\u67d0\u79cd\u7406\u60f3\u9891\u7387\u54cd\u5e94\uff08\u5982\u4f4e\u901a\u3001\u9ad8\u901a\u6216\u5e26\u901a\uff09\u3002</li> </ul>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#_15","title":"\u9891\u57df\u4e0e\u65f6\u95f4\u57df\u7684\u5173\u7cfb","text":"<p>\u6839\u636e\u5085\u91cc\u53f6\u53d8\u6362\u7406\u8bba\uff0c\u65f6\u95f4\u57df\u4e0e\u9891\u57df\u662f\u4e00\u5bf9\u4e92\u8865\u7684\u8868\u793a\uff1a</p> <ul> <li>\u6ee4\u6ce2\u5668\u7684 \u65f6\u95f4\u57df\u6743\u91cd\uff08\u8109\u51b2\u54cd\u5e94\uff09 \u51b3\u5b9a\u5176\u9891\u57df\u54cd\u5e94\u3002</li> <li>\u6ee4\u6ce2\u5668\u7684 \u9891\u57df\u8bbe\u8ba1\u76ee\u6807\uff08\u5982\u622a\u6b62\u9891\u7387\u3001\u8fc7\u6e21\u5e26\u5bbd\uff09\u901a\u8fc7\u8bbe\u8ba1\u8fc7\u7a0b\u6620\u5c04\u4e3a\u65f6\u95f4\u57df\u6743\u91cd\u3002</li> </ul>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#_16","title":"\u4ee3\u7801\u4e2d\u7684\u6ee4\u6ce2\u5668\u8bbe\u8ba1","text":"<ol> <li>Kaiser \u6ee4\u6ce2\u5668\uff1a</li> <li>\u4f7f\u7528 <code>scipy.signal.firwin</code> \u57fa\u4e8e\u9891\u57df\u8981\u6c42\u751f\u6210\u65f6\u95f4\u57df\u6ee4\u6ce2\u5668\u6743\u91cd\u3002</li> <li>\u7a97\u51fd\u6570\u6cd5\u901a\u8fc7\u8c03\u6574 Kaiser \u7a97\u53e3\u7684\u5f62\u72b6\u5b9e\u73b0\u9891\u7387\u54cd\u5e94\u7684\u4f18\u5316\u3002</li> <li>Jinc \u6ee4\u6ce2\u5668\uff1a</li> <li>\u57fa\u4e8e\u5f84\u5411\u5bf9\u79f0\u7684 Jinc \u51fd\u6570\u8bbe\u8ba1\uff0c\u5176\u6838\u5fc3\u601d\u60f3\u6e90\u81ea\u9891\u57df\u7279\u6027\u3002</li> <li>\u901a\u8fc7\u53e0\u52a0 Kaiser \u7a97\u53e3\u8fdb\u4e00\u6b65\u8c03\u6574\u9891\u57df\u7279\u6027\u5e76\u751f\u6210\u65f6\u95f4\u57df\u6ee4\u6ce2\u5668\u6743\u91cd\u3002</li> </ol>"},{"location":"book/chapter3_GAN/3.5stylegan3/paper/#_17","title":"\u603b\u7ed3","text":"<ul> <li>\u4f5c\u7528\u57df\uff1a\u6ee4\u6ce2\u5668\u4f5c\u7528\u5728\u65f6\u95f4\u57df\uff0c\u4f7f\u7528\u5377\u79ef\u64cd\u4f5c\u5904\u7406\u8f93\u5165\u4fe1\u53f7\u6216\u56fe\u50cf\u3002</li> <li>\u8bbe\u8ba1\u8fc7\u7a0b\uff1a\u57fa\u4e8e\u9891\u57df\u8981\u6c42\uff08\u5982\u622a\u6b62\u9891\u7387\u3001\u5e26\u5bbd\uff09\u8bbe\u8ba1\u65f6\u95f4\u57df\u6ee4\u6ce2\u5668\u6743\u91cd\u3002</li> <li>\u5b9e\u9645\u5e94\u7528\uff1a\u5728\u65f6\u95f4\u57df\u4e2d\u901a\u8fc7\u5377\u79ef\u5b9e\u73b0\u5bf9\u4fe1\u53f7\u6216\u56fe\u50cf\u9891\u7387\u6210\u5206\u7684\u63a7\u5236\u3002</li> </ul>"},{"location":"book/chapter3_GAN/3.6styleganT/paper/","title":"StyleGAN T","text":"<p>paper year: 2023</p>"},{"location":"book/chapter3_GAN/3.7R3Gan/paper/","title":"R3GAN","text":"<p>R3gan year: 20025</p>"},{"location":"book/chapter4_diffusion/4.1introduction/","title":"Diffusion Models","text":""},{"location":"book/chapter4_diffusion/4.1introduction/#_1","title":"\u9700\u8981\u7684\u524d\u7f6e\u77e5\u8bc6","text":"<p>\u6570\u5b66\uff1a</p> <ul> <li>\u5fae\u79ef\u5206\uff1a</li> <li>\u5bfc\u6570\u548c\u68af\u5ea6\u7684\u8ba1\u7b97\u3002</li> <li>\u504f\u5fae\u5206\u548c\u94fe\u5f0f\u6cd5\u5219\u3002</li> <li>\u591a\u53d8\u91cf\u51fd\u6570\u4f18\u5316\uff08\u5982\u68af\u5ea6\u4e0b\u964d\uff09\u3002</li> <li>\u5fae\u5206\u65b9\u7a0b\uff1a</li> <li>\u5e38\u5fae\u5206\u65b9\u7a0b\u7684\u57fa\u7840\u77e5\u8bc6\uff08\u6269\u6563\u6a21\u578b\u6d89\u53ca\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff09\u3002</li> <li>\u4f0a\u85e4\u5f15\u7406\uff08It\u00f4 Lemma\uff09\u3002</li> </ul> <p>\u7edf\u8ba1\u5b66\uff1a</p> <ul> <li>\u5fc5\u5907\u77e5\u8bc6\u70b9\uff1a</li> <li>\u5e03\u6717\u8fd0\u52a8\uff08Brownian Motion\uff09\uff1a<ul> <li>\u968f\u673a\u6e38\u8d70\u53ca\u5176\u8fde\u7eed\u6781\u9650\u3002</li> <li>\u9ad8\u65af\u8fc7\u7a0b\u3002</li> </ul> </li> <li>\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\uff1a<ul> <li>It\u00f4\u516c\u5f0f\u53ca\u5176\u5728\u6269\u6563\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002</li> </ul> </li> <li>\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\uff1a<ul> <li>\u65f6\u95f4\u8fde\u7eed\u548c\u79bb\u6563\u7684\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u3002</li> <li>\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u5b9a\u4e49\u53ca\u6027\u8d28\u3002</li> <li>\u8f6c\u79fb\u6982\u7387\u77e9\u9635\u7684\u7406\u89e3\u3002</li> </ul> </li> </ul> <p>\u7cfb\u7edf\u5730\u5b66\u4e60\u6269\u6563\u6a21\u578b\uff08Diffusion Models\uff09\u9700\u8981\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u5b66\u4e60\u8def\u5f84\uff0c\u4ece\u57fa\u7840\u7406\u8bba\u5230\u9ad8\u7ea7\u5e94\u7528\u9010\u6b65\u6df1\u5165\u3002\u4ee5\u4e0b\u662f\u5efa\u8bae\u7684\u5b66\u4e60\u6b65\u9aa4\uff1a</p>"},{"location":"book/chapter4_diffusion/4.1introduction/#reading-list","title":"Reading list","text":"<ol> <li> <p>Denoising Diffusion Probabilistic Models (DDPM)  - \u6838\u5fc3\u7406\u8bba\u3002</p> </li> <li> <p>Improved Denoising Diffusion Probabilistic Models  - \u6a21\u578b\u4f18\u5316\u65b9\u6cd5\u3002</p> </li> <li> <p>Diffusion Models Beat GANs on Image Synthesis  - \u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u3002</p> </li> <li> <p>Score-Based Generative Modeling through Stochastic Differential Equations  - \u4ecb\u7ecd\u57fa\u4e8e\u5206\u6570\u7684\u751f\u6210\u5efa\u6a21\u65b9\u6cd5\u3002</p> </li> <li> <p>Latent Diffusion Models  - \u6269\u6563\u6a21\u578b\u4e0e\u6f5c\u5728\u7a7a\u95f4\u7684\u7ed3\u5408\u3002</p> </li> </ol> <p>TO READ * [ ] https://www.cvmart.net/community/detail/6827</p>"},{"location":"book/chapter4_diffusion/4.2DDPM/","title":"DDPM","text":""},{"location":"book/chapter4_diffusion/4.2DDPM/#author-jonathna-ho","title":"AUTHOR Jonathna Ho","text":""},{"location":"book/chapter4_diffusion/4.2DDPM/#uc-berkeley","title":"UC Berkeley","text":""},{"location":"book/chapter5_generation_theory/5.1MLE/","title":"\u6700\u5927\u4f3c\u7136\u4f30\u8ba1","text":""},{"location":"book/chapter5_generation_theory/5.1MLE/#_2","title":"\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u6839\u636e\u5176\u76ee\u6807\u548c\u65b9\u6cd5\u5206\u7c7b","text":"\u6a21\u578b\u7c7b\u522b \u76ee\u6807 \u5178\u578b\u6a21\u578b \u5e94\u7528\u9886\u57df GAN \u6700\u5c0f\u5206\u5e03\u8ddd\u79bb GAN DCGAN\u3001StyleGAN\u3001BigGAN \u56fe\u50cf\u751f\u6210 \u9690\u53d8\u91cf\u6a21\u578b \u6700\u5927\u5316\u5bf9\u6570\u4f3c\u7136 VAE, PixelVAE \u56fe\u50cf\u751f\u6210\u3001\u5e8f\u5217\u751f\u6210 \u6982\u7387\u5bc6\u5ea6\u4f30\u8ba1 \u6700\u5927\u5316\u5bf9\u6570\u4f3c\u7136 Normalizing Flow, Energy-Based Models \u5bc6\u5ea6\u4f30\u8ba1\u3001\u56fe\u50cf\u751f\u6210 \u9010\u6b65\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u6a21\u578b \u9010\u6b65\u6700\u5927\u5316\u5bf9\u6570\u4f3c\u7136 DDPM\u3001Latent Diffusion Models (LDM) \u56fe\u50cf\u751f\u6210\u3001\u8865\u5168\u3001\u8d85\u5206\u8fa8\u7387 \u81ea\u56de\u5f52\u6a21\u578b \u6700\u5927\u5316\u6761\u4ef6\u5bf9\u6570\u4f3c\u7136 \u81ea\u56de\u5f52\u6a21\u578b \u56fe\u50cf\u751f\u6210\u3001\u8865\u5168\u3001\u8d85\u5206\u8fa8\u7387 \u51e0\u4f55/\u7269\u7406\u7ea6\u675f\u6a21\u578b \u6700\u5c0f\u5316\u91cd\u5efa\u8bef\u5dee NeRF, DeepSDF \u4e09\u7ef4\u5efa\u6a21\u3001\u89c6\u70b9\u5408\u6210 \u89c4\u5219/\u7edf\u8ba1\u751f\u6210\u6a21\u578b \u57fa\u4e8e\u89c4\u5219\u6216\u7ecf\u9a8c Procedural Generation, SMOTE \u6570\u636e\u589e\u5f3a\u3001\u751f\u6210\u7eb9\u7406 \u79bb\u6563\u751f\u6210\u6a21\u578b \u79bb\u6563\u6700\u5927\u4f3c\u7136\u4f30\u8ba1 GPT, Transformer \u6587\u672c\u751f\u6210\u3001\u4ee3\u7801\u751f\u6210 \u7a00\u758f/\u538b\u7f29\u751f\u6210\u6a21\u578b \u7a00\u758f\u8868\u793a\u6216\u538b\u7f29\u540e\u91cd\u5efa Sparse Coding, Autoencoders \u7279\u5f81\u63d0\u53d6\u3001\u6570\u636e\u538b\u7f29 \u6df7\u5408\u751f\u6210\u6a21\u578b \u7ed3\u5408\u591a\u4e2a\u751f\u6210\u76ee\u6807 VAE-GAN, Diffusion-GAN \u56fe\u50cf\u751f\u6210\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u751f\u6210 <p>\u4e3b\u8981\u751f\u6210\u4efb\u52a1\u4f7f\u7528\u7684\u65b9\u6cd5</p> \u6570\u636e\u7c7b\u578b \u5e38\u7528\u65b9\u6cd5 \u7279\u70b9 \u5178\u578b\u6a21\u578b \u56fe\u50cf\u751f\u6210 GAN\u3001\u6269\u6563\u6a21\u578b\u3001VAE\u3001\u81ea\u56de\u5f52\u6a21\u578b \u751f\u6210\u8d28\u91cf\u9ad8\uff0c\u9002\u5408\u5355\u5e27\u56fe\u50cf\u751f\u6210\uff0c\u591a\u6837\u6027\u548c\u63a7\u5236\u6027\u89c6\u6a21\u578b\u800c\u5b9a StyleGAN\u3001DDPM\u3001PixelCNN \u8bed\u97f3\u751f\u6210 \u81ea\u56de\u5f52\u6a21\u578b\u3001\u8c31\u56fe\u751f\u6210\u3001GAN\u3001\u6269\u6563\u6a21\u578b \u9ad8\u4fdd\u771f\u8bed\u97f3\u751f\u6210\uff0c\u5e38\u7ed3\u5408\u58f0\u7801\u5668\u5b8c\u6210\u7aef\u5230\u7aef\u751f\u6210 WaveNet\u3001Tacotron\u3001HiFi-GAN\u3001DiffWave \u89c6\u9891\u751f\u6210 GAN\u3001\u81ea\u56de\u5f52\u6a21\u578b\u3001\u6269\u6563\u6a21\u578b\u3001\u6df7\u5408\u6a21\u578b \u89c6\u9891\u751f\u6210\u9700\u8981\u8003\u8651\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u6a21\u578b\u66f4\u590d\u6742\uff0c\u751f\u6210\u8d28\u91cf\u4f9d\u8d56\u4e8e\u65f6\u95f4\u548c\u7a7a\u95f4\u7684\u5efa\u6a21\u80fd\u529b MoCoGAN\u3001VideoGPT\u3001Video Diffusion Models <p>\u63a5\u4e0b\u6765\u6211\u4eec\u4e3b\u8981\u4ecb\u7ecd\u5728\u6700\u5c0f\u8ddd\u79bb\u5206\u5e03\u548c\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u6846\u67b6\u4e0b\uff0c\u600e\u4e48\u7edf\u4e00\u89e3\u91ca\u4e0d\u540c\u7684\u751f\u6210\u6a21\u578b\u3002\u540c\u65f6\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u662f\u6700\u5c0f\u5206\u5e03\u8ddd\u79bb\u7684\u4e00\u79cd\u7279\u4f8b\uff0c\u6211\u4eec\u5176\u5b9e\u53ef\u4ee5\u5728\u201d\u6700\u5c0f\u8ddd\u79bb\u5206\u5e03\u201c\u8fd9\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u4e0b\u6765\u89e3\u91ca\u751f\u6210\u65b9\u6cd5\u7684\u539f\u7406\u3002</p> <p>\u8bb0\u4f4f\u8fd9\u4e2a\u516c\u5f0f\uff0c\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u8868\u8fbe\u5f0f</p> \\[  \\int_{x\\sim p_{\\text {data }}} p_{\\text {data }}(x) \\log p_{\\theta}(x) d x \\] <p>\u4ece\u79bb\u6563\u7684\u89d2\u5ea6\uff0c\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u8868\u793a\u4e3a</p> \\[  \\sum_{x\\sim p_{\\text {data }}}  \\log p_{\\theta}(x) \\]"},{"location":"book/chapter5_generation_theory/5.1MLE/#1-mlp","title":"\u5b9a\u74061. MLP \u662f\u6700\u5c0f\u5316\u5206\u5e03\u5dee\u5f02\u7684\u7279\u5b9a\u5f62\u5f0f","text":"<p>\u8bc1\u660e: \u5148\u8bf4\u660e\u7ed3\u8bba\uff1a\u4e24\u4e2a\u5206\u5e03\u95f4\u7684\u201c\u8ddd\u79bb\u201d\u53ef\u4ee5\u7528\u4e0d\u540c\u7684\u6307\u6807\u6765\u8861\u91cf\uff08\u5982 KL \u6563\u5ea6\u3001Jensen-Shannon \u6563\u5ea6\u3001Wasserstein \u8ddd\u79bb\u7b49\uff09\u3002MLE \u7684\u76ee\u6807\u662f\u6700\u5c0f\u5316 KL \u6563\u5ea6\uff1a</p> \\[ D_{\\text{KL}}(p_{\\text{data}} || p_\\theta) \\] <p>\u56e0\u6b64\uff0cMLE \u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u4ee5 KL \u6563\u5ea6  \u4f5c\u4e3a\u8ddd\u79bb\u8861\u91cf\u6807\u51c6\u7684\u7279\u4f8b\u3002 \u4e0b\u9762\u6211\u4eec\u53ea\u8981\u8bc1\u660eMLP\u76ee\u6807\u7b49\u4ef7\u4e8e\u4f18\u5316KL\u6563\u5ea6\u5c31\u884c\u3002</p> <p>\u5176\u4e2d</p> \\[D_{\\mathrm{KL}}\\left(p_{\\text {data }} \\| p_\\theta\\right)=\\int p_{\\text {data }}(x) \\log \\frac{p_{\\text {data }}(x)}{p_\\theta(x)} d x\\] <p>\u5c55\u5f00\u4e3a</p> \\[D_{\\mathrm{KL}}\\left(p_{\\text {data }} \\| p_\\theta\\right)=\\int p_{\\text {data }}(x) \\log p_{\\text {data }}(x) d x-\\int p_{\\text {data }}(x) \\log p_\\theta(x) d x\\] <p>\u7b2c\u4e00\u9879\u548c\\(\\theta\\) \u4e5f\u5c31\u662f\u6a21\u578b\u65e0\u5173\uff0c\u56e0\u6b64\u53ef\u4ee5\u5ffd\u7565\u3002\u7b2c\u4e8c\u9879\u548c\\(\\theta\\)\u6709\u5173\uff0c\u56e0\u6b64\u53ef\u4ee5\u770b\u6210KL\u7684\u76ee\u6807\u3002</p> <p>\u53e6\u5916MLE\u7684\u539f\u59cb\u5b9a\u4e49\u4e3a</p> \\[ \\theta^*=\\arg \\max_{\\theta} L_{\\theta}  \\\\ == \\arg\\max_{\\theta} E_{x \\sim p_{\\text {data }}}[\\log p_{\\theta}(x)]\\\\ = \\arg\\min_{\\theta} - \\int p_{\\text {data }}(x) \\log p_{\\theta}(x) d x \\] <p>\u4ece\u8fd9\u4e2a\u89d2\u5ea6\u4ece\u65b0\u4e0d\u540c\u7684\u751f\u6210\u6a21\u578b\uff0c\u5305\u62ecVAE, GAN, Diffusion\u7b49\u7b49\uff0c\u5b83\u4eec\u7684\u76ee\u6807\u90fd\u662f\u6700\u5c0f\u5316\u751f\u6210\u5206\u5e03\u548c\u539f\u59cb\u6570\u636e\u5206\u5e03\u7684\u5dee\u5f02(\u8ddd\u79bb)\u7684\u6700\u5c0f\u5316\u3002</p>"},{"location":"book/chapter5_generation_theory/5.1MLE/#2gan","title":"\u5b9a\u74062\uff1agan\u7684\u4f18\u5316\u76ee\u6807\u7b49\u4ef7\u6700\u5c0f\u5316\u5206\u5e03\u8ddd\u79bb","text":"<p>GAN\u7684\u76ee\u7684\u662f\u6700\u5c0f\u5316\u5206\u5e03\u5dee\u5f02\uff0c\u5176\u4e2dvanila GAN\u7684\u76ee\u7684\u662f\u6700\u5c0f\u5316\u4e24\u4e2a\u5206\u5e03\u4e4b\u95f4\u7684JSD\u6563\u5ea6, WGAN\u7684\u76ee\u7684\u662f\u6700\u5c0f\u5316\u8fde\u4e2a\u5206\u5e03\u4e4b\u524d\u7684Wasserstein\u8ddd\u79bb</p> <p>\u8bc1\u660e\uff1a</p> <p>1. Vanilla GAN \u7684\u4f18\u5316\u76ee\u6807\u4e0e Jensen-Shannon \u6563\u5ea6</p> <p>1.1 GAN \u7684\u4f18\u5316\u76ee\u6807</p> <p>GAN \u7684\u76ee\u6807\u51fd\u6570\u7531\u751f\u6210\u5668 \\(G\\) \u548c\u5224\u522b\u5668 \\(D\\) \u7684\u5bf9\u6297\u535a\u5f08\u7ec4\u6210\uff1a</p> \\[  \\min_G \\max_D \\mathbb{E}_{x \\sim p_{\\text{data}}} [\\log D(x)] + \\mathbb{E}_{z \\sim p(z)} [\\log (1 - D(G(z)))] \\] <p>\u5176\u4e2d\uff1a</p> <ul> <li> <p>\\(p_{\\text{data}}(x)\\)\uff1a\u771f\u5b9e\u6570\u636e\u5206\u5e03\u3002</p> </li> <li> <p>\\(p_\\theta(x) = G(z)\\)\uff1a\u751f\u6210\u5206\u5e03\u3002</p> </li> </ul> <p>1.2 \u5224\u522b\u5668\u7684\u4f18\u5316</p> <p>\u5bf9\u4e8e\u56fa\u5b9a\u7684\u751f\u6210\u5668 \\(G\\)\uff0c\u5224\u522b\u5668 \\(D\\) \u7684\u76ee\u6807\u662f\u6700\u5927\u5316\uff1a</p> \\[  \\mathcal{L}(D) = \\mathbb{E}_{x \\sim p_{\\text{data}}} [\\log D(x)] + \\mathbb{E}_{x \\sim p_\\theta} [\\log (1 - D(x))] \\] <p>\u4f18\u5316 \\(D(x)\\)\uff1a</p> <p>\u5047\u8bbe \\(D(x)\\) \u8f93\u51fa\u7684\u503c\u662f \\(D(x) \\in [0, 1]\\)\uff0c\u5bf9\u5176\u6c42\u5bfc\u5e76\u627e\u5230\u6700\u4f18\u89e3</p> \\[ D^*(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_\\theta(x)} \\] <p>\u6b64\u65f6\u6700\u4f18\u5224\u522b\u5668 \\(D^*(x)\\) \u8868\u793a\u8f93\u5165\u6837\u672c\u6765\u81ea\u771f\u5b9e\u5206\u5e03\u7684\u6982\u7387\u3002</p> <p>1.3 \u5c06\u6700\u4f18\u5224\u522b\u5668\u4ee3\u5165\u635f\u5931</p> <p>\u5c06 \\(D^*(x)\\) \u4ee3\u5165 GAN \u7684\u76ee\u6807\u51fd\u6570\uff0c\u5f97\u5230\u751f\u6210\u5668\u7684\u4f18\u5316\u76ee\u6807\uff1a</p> \\[  \\min_G \\max_D \\mathcal{L}(D) = \\mathbb{E}*{x \\sim p*{\\text{data}}} \\left[\\log \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_\\theta(x)} \\right] + \\mathbb{E}*{x \\sim p*\\theta} \\left[\\log \\frac{p_\\theta(x)}{p_{\\text{data}}(x) + p_\\theta(x)} \\right] \\] <p>\u5316\u7b80\uff1a</p> \\[  \\mathcal{L}(G) = -\\log(4) + 2 \\cdot D_{\\text{JS}}(p_{\\text{data}} || p_\\theta) \\] <p>\u5176\u4e2d \\(D_{\\text{JS}}\\) \u662f Jensen-Shannon \u6563\u5ea6 \uff0c\u5b9a\u4e49\u4e3a\uff1a</p> \\[  D_{\\text{JS}}(p_{\\text{data}} || p_\\theta) = \\frac{1}{2} D_{\\text{KL}}(p_{\\text{data}} || m) + \\frac{1}{2} D_{\\text{KL}}(p_\\theta || m) \\] <p>\\(m = \\frac{1}{2}(p_{\\text{data}} + p_\\theta)\\)\u3002\u7ed3\u8bba\uff1a Vanilla GAN \u7684\u4f18\u5316\u76ee\u6807\u662f\u6700\u5c0f\u5316\u751f\u6210\u5206\u5e03\u548c\u6570\u636e\u5206\u5e03\u4e4b\u95f4\u7684 Jensen-Shannon \u6563\u5ea6\u3002</p> <p>2. WGAN \u7684\u4f18\u5316\u76ee\u6807\u4e0e Wasserstein \u8ddd\u79bb 2.1 WGAN \u7684\u76ee\u6807\u51fd\u6570</p> <p>WGAN \u7684\u76ee\u6807\u51fd\u6570\u662f\uff1a</p> \\[  \\mathcal{L}(G, D) = \\min_G \\max_{D \\in \\text{Lip-1}} \\mathbb{E}_{x \\sim p_{\\text{data}}} [D(x)] - \\mathbb{E}_{x \\sim p_\\theta} [D(x)] \\] <p>2.1 \u7ea6\u675f\u6761\u4ef6\uff1a</p> <ul> <li> <p>\u5224\u522b\u5668 \\(D(x)\\) \u4e0d\u518d\u8f93\u51fa\u6982\u7387\uff0c\u800c\u662f\u6807\u91cf\u503c\u3002</p> </li> <li> <p>\\(D(x)\\) \u662f 1-Lipschitz \u8fde\u7eed\u51fd\u6570\uff0c\u5373\u6ee1\u8db3 \\(|D(x_1) - D(x_2)| \\leq \\|x_1 - x_2\\|\\)\u3002</p> </li> </ul> <p>2.2 Wasserstein \u8ddd\u79bb\u5b9a\u4e49 Wasserstein \u8ddd\u79bb\uff08\\(W_1\\) \u8ddd\u79bb\uff09\u5b9a\u4e49\u4e3a\uff1a</p> \\[  W_1(p_{\\text{data}}, p_\\theta) = \\inf_{\\gamma \\in \\Pi(p_{\\text{data}}, p_\\theta)} \\mathbb{E}_{(x, y) \\sim \\gamma} [\\|x - y\\|] \\] <p>\u5176\u4e2d \\(\\Pi(p_{\\text{data}}, p_\\theta)\\) \u662f\u6240\u6709\u4f7f\u8fb9\u7f18\u5206\u5e03\u4e3a \\(p_{\\text{data}}\\) \u548c \\(p_\\theta\\) \u7684\u8054\u5408\u5206\u5e03\u3002 \u6839\u636e Kantorovich-Rubinstein \u5bf9\u5076\u6027\uff0cWasserstein \u8ddd\u79bb\u53ef\u4ee5\u91cd\u5199\u4e3a\uff1a</p> \\[  W_1(p_{\\text{data}}, p_\\theta) = \\sup_{\\|D\\|_L \\leq 1} \\mathbb{E}_{x \\sim p_{\\text{data}}} [D(x)] - \\mathbb{E}_{x \\sim p_\\theta} [D(x)] \\] <p>3. \u603b\u7ed3</p> <ul> <li> <p>Vanilla GAN\uff1a  \u5224\u522b\u5668 \\(D\\) \u8f93\u51fa\u7684\u662f\u6982\u7387\uff0c\u4f18\u5316\u76ee\u6807\u662f\u6700\u5c0f\u5316\u751f\u6210\u5206\u5e03\u548c\u771f\u5b9e\u5206\u5e03\u7684 Jensen-Shannon \u6563\u5ea6\uff08JSD\uff09\u3002</p> </li> <li> <p>WGAN\uff1a  \u5224\u522b\u5668 \\(D\\) \u8f93\u51fa\u7684\u662f\u6807\u91cf\u503c\uff0c\u4f18\u5316\u76ee\u6807\u662f\u6700\u5c0f\u5316\u751f\u6210\u5206\u5e03\u548c\u771f\u5b9e\u5206\u5e03\u7684 Wasserstein \u8ddd\u79bb\uff08\\(W_1\\)\uff09\u3002</p> </li> <li> <p>\u4e24\u8005\u7684\u672c\u8d28\uff1a  \u90fd\u5728\u901a\u8fc7\u4e0d\u540c\u7684\u5206\u5e03\u5dee\u5f02\u5ea6\u91cf\u6307\u6807\u4f18\u5316\u751f\u6210\u5206\u5e03 \\(p_\\theta(x)\\) \u903c\u8fd1\u771f\u5b9e\u6570\u636e\u5206\u5e03 \\(p_{\\text{data}}(x)\\)\u3002</p> </li> </ul> <p>4. \u5907\u6ce8</p> <p>\u5728wgan \u4e2d\u4e3a\u4ec0\u4e48\u51fa\u73b0\u4e86Lipschitz \u6761\u4ef6\u3002\u8fd9\u662f\u56e0\u4e3a Kantorovich-Rubinstein \u5bf9\u5076\u6027\u8981\u6c42\u76ee\u6807\u51fd\u6570 \\(\ud835\udc53(\ud835\udc65)\\) \u662f 1-Lipschitz \u51fd\u6570\u3002\u5982\u679c\u6ca1\u6709\u8fd9\u4e2a\u6761\u4ef6\uff0cWasserstein \u8ddd\u79bb\u65e0\u6cd5\u901a\u8fc7\u5bf9\u5076\u5f62\u5f0f\u8ba1\u7b97\u3002 \u5728 WGAN \u4e2d\uff0c\u5224\u522b\u5668 D(x) \u5b9e\u9645\u4e0a\u662f f(x) \u7684\u5b9e\u73b0\uff0c\u56e0\u6b64\u9700\u8981\u6ee1\u8db3 Lipschitz \u8fde\u7eed\u6027\uff0c\u4fdd\u8bc1\u4f18\u5316\u76ee\u6807\u4e0e Wasserstein \u8ddd\u79bb\u7684\u6570\u5b66\u5b9a\u4e49\u4e00\u81f4\u3002</p> <p>\u4ece\u53e6\u5916\u4e00\u4e2a\u89d2\u5ea6\u8bf4\u660e\uff1a\u5224\u522b\u5668\\(D\\)\u7684\u4f5c\u7528\u53ef\u80fd\u4e0d\u4e00\u81f4\uff0c\u4f46\u662fD\u7684loss \u90fd\u8868\u793a\u4e86\u4e24\u4e2a\u5206\u5e03\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u5206\u522b\u662fJSD \u6563\u5ea6\u548cWasserstein \u8ddd\u79bb\u3002\u4f18\u5316\\(D\\) \u7684\u4f5c\u7528\u53ca\u65f6\u8ba9\u8fd9\u4e2aLoss \u5c3d\u91cf\u51c6\u786e\u6a21\u62df\u51fa\u4e24\u4e2a\u5206\u5e03\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002\u5982\u679c\u628a\u8fd9\u4e2aloss \\(L_\\theta(x,y)\\) \u4f5c\u4e3a\u4e00\u4e2a\u51fd\u6570\u770b\u5f85,\u5b83\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b66\u4e60\u7684\u5c31\u662f\u4e24\u4e2a\u5206\u5e03\u4e4b\u95f4\u7684\u8ddd\u79bb\u7684\u8fd1\u4f3c\u3002</p> <p>\u5f53\u7136\u5728\u4e0a\u9762\u7684\u5206\u6790\u4e2d\uff0c\u6211\u4eec\u662f\u77e5\u9053\u4e86GAN\u7684\u5b9e\u73b0\uff0c\u7136\u540e\u8bc1\u660e\u4e86\u5b83\u7684\u4f5c\u7528\u3002</p> <p>\u7406\u8bba\u4e0a\u6211\u4eec\u8861\u91cf\u4e24\u4e2a\u5206\u5e03\u4e4b\u95f4\u7684\u8ddd\u79bb\u6709\u4e0d\u540c\u7684\u9009\u62e9\uff0c\u90a3\u5728\"GAN\"\u7684\u8bbe\u8ba1\u4e2d\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u6839\u636e\u4e0d\u540c\u7684\u8ddd\u79bb\u9009\u62e9\u53ef\u4ee5\u8ba9\u6211\u4eec\u53bb\u8bbe\u8ba1\u4e0d\u540c\u7684D\u548cD\u7684loss\u3002</p> <p>\u90a3\u4e48\u5047\u8bbe\u6211\u4eec\u60f3\u8981\u7528KL \u6563\u5ea6\u53bb\u8861\u91cf\u4e24\u4e2a\u5206\u5e03\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u90a3\u662f\u4e0d\u662f\u53ef\u4ee5\u8bbe\u8ba1\u51fa\u76f8\u5e94\u7684loss\u3002 \u7b54\u6848\u662f\u80af\u5b9a\u7684\uff0c</p> <p>\u6211\u4eec\u53ef\u4ee5\u63a8\u5bfc\u51fa\u5bf9\u7279\u5b9a\u6563\u5ea6\u7684\u4f18\u5316\u8fd1\u4f3c\u4e8e \\(\\min _\\theta \\max _\\omega F(\\theta, \\omega)=\\mathbb{E}_{x \\sim P}\\left[T_\\omega(x)\\right]-\\mathbb{E}_{x \\sim Q_\\theta}\\left[f^*\\left(T_\\omega(x)\\right)\\right]\\).</p> <p>\u4ece\u800cKL\u6563\u5ea6\u5bf9\u5e94\u7684loss \u5219\u4e3a</p> \\[E_{x\\sim P}\\left[\\log D(x)\\right]-E_{x\\sim Q_\\theta}\\left[\\log D(x)\\right]\\] <p>\u53c2\u8003\u8fd9\u4e2a\u8bba\u6587 https://arxiv.org/pdf/1606.00709 \u4e86\u89e3\u66f4\u591a\u6563\u5ea6\u5bf9\u5e94\u7684loss</p>"},{"location":"book/chapter5_generation_theory/5.1MLE/#_3","title":"\u5e7f\u6cdb\u542b\u4e49\u4e0a\u7684\u5206\u5e03\u4e4b\u95f4\u7684\u8861\u91cf\u8bbe\u8ba1","text":"\u8ddd\u79bb\u8861\u91cf\u65b9\u6cd5 GAN \u7c7b\u578b \u4f18\u52bf \u52a3\u52bf \u8bba\u6587\u94fe\u63a5 Jensen-Shannon \u6563\u5ea6 Vanilla GAN \u7406\u8bba\u57fa\u7840\u6e05\u6670\uff0c\u76ee\u6807\u660e\u786e \u68af\u5ea6\u6d88\u5931\uff0c\u6a21\u5f0f\u5d29\u6e83 Generative Adversarial Nets Wasserstein \u8ddd\u79bb WGAN \u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u6709\u610f\u4e49\u7684\u68af\u5ea6 \u8ba1\u7b97\u4ee3\u4ef7\u9ad8\uff0c\u9700\u5f3a\u5236 Lipschitz \u6761\u4ef6 Wasserstein GAN f-\u6563\u5ea6 f-GAN \u7075\u6d3b\u7684\u6563\u5ea6\u9009\u62e9\uff0c\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u9700\u6c42 \u9700\u9009\u62e9\u5408\u9002\u7684 f-\u6563\u5ea6 f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization MMD\uff08\u6700\u5927\u5747\u503c\u5dee\u5f02\uff09 MMD-GAN \u6838\u51fd\u6570\u7075\u6d3b\uff0c\u9ad8\u7ef4\u6570\u636e\u8868\u73b0\u4f18\u8d8a \u6838\u51fd\u6570\u9009\u62e9\u5f71\u54cd\u6027\u80fd MMD GAN: Towards Deeper Understanding of Moment Matching Network Sliced Wasserstein \u8ddd\u79bb Sliced-WGAN \u6539\u5584\u9ad8\u7ef4\u6570\u636e\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027 \u9700\u8981\u9009\u62e9\u9002\u5f53\u7684\u6295\u5f71\u65b9\u5411 Max-Sliced Wasserstein Distance and Its Use for GANs Sobolev \u8ddd\u79bb Sobolev GAN \u653e\u5bbd Lipschitz \u6761\u4ef6\uff0c\u63d0\u9ad8\u8bad\u7ec3\u7075\u6d3b\u6027 \u7406\u8bba\u590d\u6742\u6027\u589e\u52a0 Towards Generalized Implementation of Wasserstein Distance in GANs"},{"location":"book/chapter5_generation_theory/5.1MLE/#3-vae","title":"\u5b9a\u74063: VAE \u662f\u5bf9\u6700\u5927\u4f3c\u7136\u7684\u4f18\u5316","text":"<p>\u8bc1\u660e</p> <ol> <li>\u8fde\u7eed\u5206\u5e03\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u76ee\u6807</li> </ol> <p>\u5bf9\u4e8e\u89c2\u6d4b\u6570\u636e\u7684\u6982\u7387\u5206\u5e03 \\(p_{\\text{data}}(x)\\)\uff0c\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u76ee\u6807\u662f\u6700\u5927\u5316\u6570\u636e\u5206\u5e03\u4e0b\u6a21\u578b \\(p_\\theta(x)\\) \u7684\u5bf9\u6570\u4f3c\u7136\uff1a</p> \\[  \\int_{x \\sim p_{\\text{data}}} p_{\\text{data}}(x) \\log p_\\theta(x) \\, dx \\] <p>\u8fd9\u91cc\u6211\u4eec\u9700\u8981\u627e\u5230\u4e00\u79cd\u529e\u6cd5\u53bb\u8868\u8fbe\u6216\u8005\u8fd1\u4f3c \\(p_\\theta(x)\\)\u3002 \u8fd9\u662f\u5173\u952e\u7684\u4e00\u90e8\u5206\u3002 \u5bf9\u4e8e\u9690\u53d8\u91cf\u751f\u6210\u6a21\u578b\u800c\u8a00\uff0c\u4f1a\u6709\u4e00\u4e2a\\(z\\) \u548c\\(X\\)\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c</p> <p>\u6211\u4eec\u53ef\u4ee5\u5199\u6210 \\(p_\\theta(x) = \\int p_\\theta(x, z) \\, dz\\) \u5305\u542b\u5bf9\u9690\u53d8\u91cf \\(z\\) \u7684\u79ef\u5206\u3002\u518d\u5bf9\\(z\\) \u505a\u4e00\u4e9b\u5047\u8bbe\uff0c\u53ef\u80fd\u5c31\u4f1a\u7b80\u5316\u6c42\u89e3\u7684\u8fc7\u7a0b\u3002</p> <ol> <li>\u91cd\u5199\u8fb9\u7f18\u4f3c\u7136</li> </ol> <p>\u5bf9\u4e8e\u5355\u4e2a\u6570\u636e\u70b9 \\(x\\)\uff0c\u89c2\u6d4b\u6570\u636e\u7684\u8fb9\u7f18\u5bf9\u6570\u4f3c\u7136\u53ef\u4ee5\u5199\u4e3a\uff1a</p> \\[  \\log p_\\theta(x) = \\log \\int p_\\theta(x, z) \\, dz \\] <p>\u5229\u7528\u8054\u5408\u5206\u5e03\u7684\u5206\u89e3 \\(p_\\theta(x, z) = p_\\theta(x \\mid z) p(z)\\)\uff0c\u6211\u4eec\u6709\uff1a</p> \\[  \\log p_\\theta(x) = \\log \\int p_\\theta(x \\mid z) p(z) \\, dz \\] <p>\u76f4\u63a5\u4f18\u5316\u8fd9\u4e2a\u76ee\u6807\u901a\u5e38\u5f88\u56f0\u96be\uff0c\u56e0\u4e3a\u79ef\u5206 \\(\\int p_\\theta(x \\mid z)p(z) dz\\) \u5bf9\u4e8e\u9ad8\u7ef4 \\(z\\) \u4e0d\u53ef\u89e3\u6790\u3002</p> <ol> <li>\u5f15\u5165\u53d8\u5206\u5206\u5e03 \\(q_\\phi(z \\mid x)\\)\u4e3a\u4e86\u89e3\u51b3\u79ef\u5206\u4e0d\u53ef\u89e3\u6790\u7684\u95ee\u9898</li> </ol> <p>\u5f15\u5165\u4e00\u4e2a\u8fd1\u4f3c\u540e\u9a8c\u5206\u5e03 \\(q_\\phi(z \\mid x)\\)\uff0c\u7528\u4e8e\u8fd1\u4f3c\u771f\u5b9e\u540e\u9a8c \\(p_\\theta(z \\mid x)\\)\u3002\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u5206\u89e3\u91cd\u65b0\u8868\u793a \\(\\log p_\\theta(x)\\)\uff1a</p> \\[  \\log p_\\theta(x) = \\mathbb{E}_{q_\\phi(z \\mid x)} \\left[ \\log \\frac{p_\\theta(x, z)}{q_\\phi(z \\mid x)} \\right] + \\mathrm{KL}(q_\\phi(z \\mid x) \\| p_\\theta(z \\mid x)) \\] <p>\u8bc1\u660e\u7ec6\u8282\u8bf7\u770b ELBP\u8bc1\u660e\u3002ELBO\u4e5f\u53ef\u4ee5\u7531\u51f8\u51fd\u6570\u7684\u6027\u8d28\u5229\u7528 Jensen's inequality\u76f4\u63a5\u63a8\u5bfc\u51fa\u4e0d\u7b49\u5f0f(https://en.wikipedia.org/wiki/Evidence_lower_bound).</p> <p>\u5176\u4e2d\uff1a</p> <ul> <li> <p>\u7b2c\u4e00\u9879\u662f\u53d8\u5206\u4e0b\u754c\uff08Evidence Lower Bound, ELBO)),\u6211\u4eec\u53ef\u4ee5\u4f18\u5316\u5b83\u6765\u95f4\u63a5\u4f18\u5316 \\(\\log p_\\theta(x)\\)\u3002</p> </li> <li> <p>\u7b2c\u4e8c\u9879\u662f KL \u6563\u5ea6\uff0c\u8868\u793a\u8fd1\u4f3c\u540e\u9a8c \\(q_\\phi(z \\mid x)\\) \u4e0e\u771f\u5b9e\u540e\u9a8c \\(p_\\theta(z \\mid x)\\) \u7684\u5dee\u8ddd\u3002</p> </li> </ul> <p>\u7531\u4e8e KL \u6563\u5ea6\u603b\u662f\u975e\u8d1f\uff1a\\(\\mathrm{KL}(q_\\phi(z \\mid x) \\| p_\\theta(z \\mid x)) \\geq 0\\)\uff0c\u6240\u4ee5\uff1a\\(\\log p_\\theta(x) \\geq \\mathcal{L}(\\theta, \\phi; x)\\), \u5176\u4e2d\\(L\\) \u8868\u793a\u53d8\u5206\u4e0b\u754c,\u4e5f\u5c31\u662f ELBO\uff0c\u4e3a\u4e0a\u5f0f\u7684\u7b2c\u4e00\u9879\u3002</p> <ol> <li>. \u53d8\u5206\u4e0b\u754c\uff08ELBO, \u8bbe\u4e3a $ \\mathcal{L}$\uff09\u7684\u5206\u89e3</li> </ol> <p>\u53d8\u5206\u4e0b\u754c\u7684\u5177\u4f53\u5f62\u5f0f\u4e3a\uff1a</p> \\[  \\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}_{q_\\phi(z \\mid x)} \\left[ \\log p_\\theta(x, z) - \\log q_\\phi(z \\mid x) \\right] \\] <p>\u8fdb\u4e00\u6b65\u5206\u89e3\u8054\u5408\u6982\u7387 \\(p_\\theta(x, z) = p_\\theta(x \\mid z)p(z)\\)\uff0c\u5f97\u5230\uff1a</p> \\[  \\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}_{q_\\phi(z \\mid x)} \\left[ \\log p_\\theta(x \\mid z) \\right] - \\mathrm{KL}(q_\\phi(z \\mid x) \\| p(z)) \\] <p>\u7b2c\u4e00\u9879 \\(\\mathbb{E}_{q_\\phi(z \\mid x)} [\\log p_\\theta(x \\mid z)]\\)\uff1a\u91cd\u6784\u8bef\u5dee\uff08Reconstruction Error\uff09\uff0c\u9f13\u52b1\u751f\u6210\u5668\u80fd\u591f\u751f\u6210\u63a5\u8fd1\u771f\u5b9e\u6570\u636e \\(x\\) \u7684\u5206\u5e03\u3002 \u8fd9\u4e2a\u6ca1\u6cd5\u76f4\u63a5\u8ba1\u7b97\u3002\u5982\u679c\u7528\u8499\u7279\u5361\u6d1b\u91c7\u6837\u7684\u8bdd\uff0c\u4e5f\u9700\u8981\u6709 \\(p_\\theta(z|x)\\)\u7684\u503c\uff0c\u4f46\u662f\u8fd9\u4e2a\u503c\u4e5f\u662f\u4e0d\u53ef\u89e3\u6790\u7684\u3002 \u56e0\u6b64\u5728\u539f\u59cb\u7684\u8bba\u6587\u91cc\uff0c\u5047\u8bbe \\(p_\\theta(z|x)\\)\u662f\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u8ba1\u7b97\u3002</p> <p>\u5982\u679c\u6211\u4eec\u5047\u8bbe \\(p_\\theta(x|z)\\) \u662f\u9ad8\u65af\u5206\u5e03\uff1a</p> \\[ p_\\theta(x|z) = \\mathcal{N}(\\hat{x}, \\sigma^2 I) \\] <p>\u5176\u4e2d \\(\\hat{x}\\) \u662f\u89e3\u7801\u5668\u751f\u6210\u7684\u5747\u503c\uff0c\\(\\sigma^2\\) \u662f\u56fa\u5b9a\u65b9\u5dee\u3002 \u5bf9\u6570\u4f3c\u7136\u5c55\u5f00\uff1a</p> \\[ \\begin{aligned} \\log p_\\theta(x|z) &amp;= \\log \\mathcal{N}(x; \\hat{x}, \\sigma^2 I)\\\\   &amp;= -\\frac{1}{2} \\left( \\frac{\\|x - \\hat{x}\\|^2}{\\sigma^2} + d \\log(2\\pi\\sigma^2) \\right), \\end{aligned} \\] <p>\u5176\u4e2d \\(d\\) \u662f \\(x\\) \u7684\u7ef4\u5ea6\u3002</p> <p>\u91cd\u6784\u8bef\u5dee\u7b49\u4ef7\u4e8e\u8d1f\u5bf9\u6570\u4f3c\u7136\uff0c\u5ffd\u7565\u5e38\u6570\u9879\u540e\u4e3a\uff1a</p> \\[ \\mathbb{E}_{q_\\phi(z|x)}[-\\log p_\\theta(x|z)] \\propto \\mathbb{E}_{q_\\phi(z|x)}[\\|x - \\hat{x}\\|^2]. \\] <ol> <li>\u91cd\u5efa\u8bef\u5dee\u7b80\u5316\u4e3a \\(|x - \\hat{x}|\\) \u5728\u5177\u4f53\u5b9e\u73b0\u4e2d\uff0c\u901a\u5e38\u5047\u8bbe\u65b9\u5dee \\(\\sigma^2 = 1\\) \u4e14\u91cd\u5efa\u8bef\u5dee\u4ec5\u8003\u8651\u5747\u503c\u4f30\u8ba1\uff0c\u6b64\u65f6\uff1a</li> </ol> \\[ \\mathbb{E}_{q_\\phi(z|x)}[\\|x - \\hat{x}\\|^2] \\approx \\|x - \\hat{x}\\|^2, \\] <p>\u5bf9\u5e94\u7684\u5dee\u9879\u4e3a \\(|x - \\hat{x}|\\) \u6216\u5176\u5e73\u65b9\u5f62\u5f0f\u3002</p> <p>\u5f53\u7136\u5982\u679c\u6211\u4eec\u5047\u8bbe \\(p_\\theta(x|z)\\) \u662f\u5c5e\u4e8e\u5176\u4ed6\u5206\u5e03\uff0c\u90a3\u5c31\u4f1a\u5bfc\u51fa\u4e0d\u540c\u7684\u91cd\u6784\u8bef\u5dee\u3002\u66f4\u8fdb\u4e00\u6b65\uff0c\u5bf9\u4e8e\u4efb\u610f\u4e00\u4e2a\u91cd\u5efa\u5dee\u7684\u5ea6\u91cf\uff0c\u5176\u5b9e\u90fd\u5bf9\u5e94\u7740\u4e00\u4e2a\\(p_\\theta(x|z)\\), \u8fd9\u4e2a\u53ef\u4ee5\u7531\u5ea6\u91cf\u51fd\u6570\u53bb\u6784\u5efa\u4e00\u4e2a\u5bc6\u5ea6\u51fd\u6570\u3002 \u5177\u4f53\uff0c\u67e5\u770b VAE: introduction</p> <ul> <li> <p>\u7b2c\u4e8c\u9879 \\(\\mathrm{KL}(q_\\phi(z \\mid x) \\| p(z))\\)\uff1a\u6b63\u5219\u5316\u9879\uff0c\u7ea6\u675f \\(q_\\phi(z \\mid x)\\) \u7684\u5206\u5e03\u63a5\u8fd1\u5148\u9a8c \\(p(z)\\)\u3002</p> </li> <li> <p>\u6700\u7ec8\u5f62\u5f0f\uff08VAE \u635f\u5931\u51fd\u6570\uff09 \u4e3a\u4e86\u4f18\u5316\u4e0a\u8ff0\u76ee\u6807\uff0c\u6211\u4eec\u9700\u8981\u8fdb\u884c\u91c7\u6837 \\(z \\sim q_\\phi(z \\mid x)\\)\u3002\u4e3a\u4e86\u89e3\u51b3\u91c7\u6837\u8fc7\u7a0b\u4e2d\u4e0d\u53ef\u5fae\u7684\u95ee\u9898\uff0c\u4f7f\u7528 \u91cd\u53c2\u6570\u5316\u6280\u5de7\uff08Reparameterization Trick\uff09 \uff1a\u5c06 \\(q_\\phi(z \\mid x)\\) \u5b9a\u4e49\u4e3a\u9ad8\u65af\u5206\u5e03 \\(\\mathcal{N}(\\mu_\\phi(x), \\sigma_\\phi(x)^2)\\)\uff0c\u901a\u8fc7\u5982\u4e0b\u65b9\u5f0f\u91c7\u6837\uff1a</p> </li> </ul> \\[ z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\] <p>\u6700\u7ec8\uff0cVAE \u7684\u635f\u5931\u51fd\u6570\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a</p> \\[  \\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}_{z \\sim q_\\phi(z \\mid x)} [\\log p_\\theta(x \\mid z)] - \\mathrm{KL}(q_\\phi(z \\mid x) \\| p(z)) \\] <p>\u8fd9\u5bf9\u5e94\u4e8e\uff1a</p> <ol> <li> <p>\u91cd\u6784\u8bef\u5dee \uff1a\u901a\u8fc7\u751f\u6210\u5206\u5e03 \\(p_\\theta(x \\mid z)\\) \u5b66\u4e60\u5982\u4f55\u751f\u6210\u6570\u636e\u3002</p> </li> <li> <p>KL \u6563\u5ea6\u6b63\u5219\u5316 \uff1a\u7ea6\u675f\u6f5c\u53d8\u91cf\u5206\u5e03\u3002</p> </li> </ol> <p>\u4f18\u5316\u8be5\u76ee\u6807\uff0c\u5373\u5b9e\u73b0\u4ece\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5230 VAE \u7684\u8f6c\u5316\u3002</p> <p>\u4ece\u8fd9\u91cc\u6211\u4eec\u4e5f\u80fd\u770b\u5230\uff0cVAE\u548c\u901a\u5e38\u7684\u795e\u7ecf\u7f51\u7edc\u6784\u9020\u4e0d\u4e00\u6837\uff0c\u5b83\u9884\u6d4b\u7684\u65f6\u4e00\u4e2a\u5206\u5e03\uff0c\u4e0d\u7ba1\u7f16\u7801\u5668\u8fd8\u662f\u89e3\u7801\u5668\uff0c\u9884\u6d4b\u7684\u90fd\u662f\u4e00\u4e2a\u5206\u5e03\u3002\u66f4\u7cbe\u786e\u5f97\u6765\u8bf4\uff0c\u6700\u521d\u7684VAE \u7f16\u7801\u5668\u9884\u6d4b\u7684\u65f6\u9ad8\u65af\u5206\u5e03\u7684mean \u548cvariance\uff0c \u89e3\u7801\u5668\u9884\u6d4b\u7684\u65f6\u9ad8\u65af\u5206\u5e03\u7684mean\u3002\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u65f6\uff0c\u6211\u4eec\u4f7f\u7528VAE \u8fdb\u884c\u91cd\u5efa\u65f6\uff0c\u76f4\u63a5\u4f7f\u7528\u7684\u5c31\u662f\u8fd9\u4e2amean\uff0c\u56e0\u6b64\u5ffd\u7565\u6389\u4e86\u5176\u5b9edecoder \u4e5f\u662f\u672c\u8d28\u4e0a\u9884\u6d4b\u7684\u662fmean.</p> <p>\u5b9e\u9645\u7684VAE\u8fc7\u7a0b </p> <p>\u5bf9\u4e8eVAE\u800c\u8a00\uff0c\u7b2c\u4e00\u9879\u7684\u8ba1\u7b97\u662f\u7b49\u4ef7\u4e8e\u91cd\u5efaloss \u7684\uff0c\u5173\u952e\u662f\u7b2c\u4e8c\u9879\u9700\u8981\u600e\u4e48\u8bbe\u8ba1\u3002\u4e0d\u540c\u7684\u8bbe\u8ba1\u53ef\u80fd\u5c31\u4ee5\u4e3a\u7740\u4e0d\u540c\u7684\u65b9\u6cd5\u3002</p> <p>\u8fd9\u91cc\u6211\u4eec\u5217\u4e3e\u4e86\u4e00\u7cfb\u5217\u5bf9\u7b2c\u4e8c\u9879\u8ba1\u7b97\u7684\u6539\u8fdb\u65b9\u6cd5</p> \u65b9\u6cd5 \u63cf\u8ff0 \u4f18\u70b9 \u7f3a\u70b9 \u9002\u7528\u573a\u666f \u6807\u51c6\u9ad8\u65af VAE \u5047\u8bbe \\(q_\\phi(z \\mid x)\\)\u4e3a\u9ad8\u65af\u5206\u5e03\uff0c\u7f16\u7801\u5668\u8f93\u51fa\u5747\u503c\u548c\u65b9\u5dee\u3002 \u7b80\u5355\u9ad8\u6548\uff0c\u53ef\u89e3\u6790\u8ba1\u7b97 KL \u6563\u5ea6\u3002 \u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u5206\u5e03\u3002 \u5e38\u89c4\u4efb\u52a1\uff0c\u6570\u636e\u5206\u5e03\u7b80\u5355\u3002 \u9ad8\u65af\u6df7\u5408 VAE \u5047\u8bbe \\(q_\\phi(z \\mid x)\\) \u662f\u6df7\u5408\u9ad8\u65af\u5206\u5e03\uff08\u591a\u4e2a\u9ad8\u65af\u6210\u5206\uff09\u3002 \u80fd\u591f\u6355\u6349\u591a\u6a21\u6001\u5206\u5e03\uff0c\u66f4\u9002\u5408\u590d\u6742\u6570\u636e\u5206\u5e03\u3002 \u589e\u52a0\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9700\u8981\u4f30\u8ba1\u66f4\u591a\u53c2\u6570\u3002 \u591a\u6a21\u6001\u6570\u636e\u5efa\u6a21\uff0c\u4f8b\u5982\u805a\u7c7b\u6216\u5206\u7c7b\u4efb\u52a1\u3002 \u6b63\u6001\u5316\u6d41 VAE \u7528\u4e00\u7cfb\u5217\u53ef\u9006\u53d8\u6362\u6784\u9020 \\(q_\\phi(z \\mid x)\\)\uff0c\u63d0\u9ad8\u540e\u9a8c\u5206\u5e03\u7684\u7075\u6d3b\u6027\u3002 \u540e\u9a8c\u5206\u5e03\u66f4\u7075\u6d3b\uff0c\u9002\u5408\u590d\u6742\u5206\u5e03\u5efa\u6a21\u3002 \u589e\u52a0\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9700\u8981\u8bbe\u8ba1\u5408\u7406\u7684\u6d41\u6a21\u578b\u3002 \u9ad8\u7ef4\u590d\u6742\u5206\u5e03\u7684\u8868\u793a\u548c\u5efa\u6a21\u3002 \u79bb\u6563 VAE \u5047\u8bbe\u6f5c\u53d8\u91cf\u662f\u79bb\u6563\u53d8\u91cf\uff0c\u7528 Gumbel-Softmax \u6280\u5de7\u5b9e\u73b0\u53ef\u5fae\u4f18\u5316\u3002 \u9002\u5408\u79bb\u6563\u6f5c\u53d8\u91cf\u7a7a\u95f4\uff08\u5982\u5206\u7c7b\u6216\u6587\u672c\u751f\u6210\uff09\u3002 \u65e0\u6cd5\u6355\u6349\u8fde\u7eed\u5206\u5e03\u7684\u7ec6\u8282\uff0c\u5bf9\u6e29\u5ea6\u53c2\u6570\\(\\tau\\)\u654f\u611f\u3002 \u79bb\u6563\u6570\u636e\u751f\u6210\u6216\u5206\u7c7b\u4efb\u52a1\u3002 \u80fd\u91cf\u6a21\u578b VAE \u7528\u80fd\u91cf\u51fd\u6570\u5b9a\u4e49 \\(q_\\phi(z \\mid x)\\),\u5982\\(q_\\phi(z \\mid x) \\propto \\exp(-E_\\phi(x, z))\\)\uff0c\u5e76\u901a\u8fc7 MCMC \u91c7\u6837\u3002 \u975e\u53c2\u6570\u5316\uff0c\u80fd\u591f\u7075\u6d3b\u5efa\u6a21\u590d\u6742\u5206\u5e03\u3002 \u91c7\u6837\u6548\u7387\u8f83\u4f4e\uff0c\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\u3002 \u9ad8\u5ea6\u590d\u6742\u6216\u672a\u77e5\u5206\u5e03\u5efa\u6a21\u3002 \u5bf9\u6297\u5f0f VAE (AAE) \u7528 GAN \u7684\u5bf9\u6297\u5b66\u4e60\u66ff\u4ee3 KL \u6563\u5ea6\u6b63\u5219\u5316\uff0c\u5224\u522b\u5668\u7528\u4e8e\u5339\u914d \\(q_\\phi(z)\\) \u548c \\(p(z)\\)\u3002 \u4e0d\u9700\u8981\u663e\u5f0f\u8ba1\u7b97 KL \u6563\u5ea6\uff0c\u9002\u5408\u590d\u6742\u5206\u5e03\u3002 \u5bf9\u6297\u8bad\u7ec3\u53ef\u80fd\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u7cbe\u5fc3\u8c03\u8bd5\u3002 \u590d\u6742\u6570\u636e\u5206\u5e03\u751f\u6210\u4efb\u52a1\u3002 \u03b2-VAE \u5728 VAE \u635f\u5931\u4e2d\u589e\u52a0 KL \u6563\u5ea6\u7684\u6743\u91cd \\(1\\beta &gt; 1\\)\uff0c\u5f3a\u8c03\u6f5c\u53d8\u91cf\u7684\u538b\u7f29\u6027\u548c\u89e3\u8026\u6027\u3002 \u63d0\u9ad8\u6f5c\u53d8\u91cf\u7684\u8868\u793a\u8d28\u91cf\uff0c\u66f4\u9002\u5408\u8868\u5f81\u5b66\u4e60\u4efb\u52a1\u3002 \u53ef\u80fd\u5bfc\u81f4\u91cd\u6784\u6027\u80fd\u4e0b\u964d\uff0c\u5e73\u8861\u91cd\u6784\u548c\u6b63\u5219\u5316\u8f83\u96be\u3002 \u8868\u5f81\u5b66\u4e60\uff0c\u7279\u5f81\u89e3\u8026\uff08\u5982\u751f\u6210 disentangled \u8868\u793a\uff09\u3002 \u5c42\u6b21\u5316 VAE \u5f15\u5165\u591a\u4e2a\u5c42\u6b21\u7684\u6f5c\u53d8\u91cf\uff0c\u4f8b\u5982 $z_1 \\sim q_\\phi(z_1 \\mid x), z_2 \\sim q_\\phi(z_2 \\mid z_1) $\uff0c\u6355\u6349\u5206\u5e03\u7684\u5c42\u6b21\u7279\u6027\u3002 \u80fd\u591f\u66f4\u597d\u5730\u8868\u793a\u590d\u6742\u6570\u636e\u5206\u5e03\u7684\u5c42\u6b21\u5173\u7cfb\u3002 \u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u8bad\u7ec3\u66f4\u56f0\u96be\u3002 \u9ad8\u7ef4\u590d\u6742\u6570\u636e\uff0c\u4f8b\u5982\u56fe\u50cf\u6216\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u3002 <p>\u603b\u7ed3</p> <p>\u5bf9\u4e8eVAE\uff0c\u76f4\u89c2\u4e0a\u770b\uff0c\u56e0\u4e3a\u76f4\u63a5\u4f30\u8ba1 \\(p_\\theta(x)\\) \u662f\u6bd4\u8f83\u56f0\u96be\u7684\uff0c\u56e0\u6b64\u5f15\u5165\u5206\u5e03 \\(q_\\phi(z)\\)\uff0c\u6839\u636e \\(p_\\theta(x) = \\int_z p_\\theta(x, z) \\, dz= \\int_z p_\\theta(x|z) p(z) \\, dz\\) \u6765\u8ba1\u7b97 \\(p_\\theta(x)\\)\u3002</p> <p>\u6211\u4eec\u53ef\u4ee5\u9009\u4e00\u4e2a\\(p(z)\\) \u6bd4\u8f83\u597d\u8ba1\u7b97\u7684\u5206\u5e03\u6765\u7b80\u5316\u95ee\u9898\u3002\u5230\u8fd9\u4e00\u6b65\uff0c\u4ecd\u7136\u6ca1\u6709\u529e\u6cd5\u8ba1\u7b97\u3002\u56e0\u6b64\u8bba\u6587\u91cc\u6709\u4e24\u4e2a\u6bd4\u8f83\u91cd\u8981\u7684\u5047\u8bbe\uff0c</p> <ol> <li>\\(q_\\phi(z|x)\\) \u670d\u4ece\u6b63\u6001\u5206\u5e03</li> <li>\\(p_\\theta(x|z)\\) \u670d\u4ece\u6b63\u6001\u5206\u5e03</li> </ol> <p>\u6839\u636e\u8fd9\u4e24\u4e2a\u5047\u8bbe\u4ece\u800c\u4f7f\u5f97loss \u53ef\u4ee5\u8ba1\u7b97\u3002\u7b2c2\u4e2a\u5047\u8bbe\u53ef\u4ee5\u7406\u89e3\u6210\uff0c\u6211\u4eec\u6ca1\u6709\u529e\u6cd5\u5047\u8bbe \\(p_\\theta(x)\\) \u670d\u4ece\u6b63\u6001\u5206\u5e03\uff0c\u56e0\u6b64\u5f31\u5316\u4e00\u4e0b\u6761\u4ef6, \u90a3 \\(p_\\theta(x)\\) \u76f8\u5f53\u4e8e\u6df7\u5408\u9ad8\u65af\u5206\u5e03\uff08\u79bb\u6563\u548c\u7684\u6781\u9650\u5f62\u5f0f),\u4ece\u8fd9\u4e2a\u5f62\u5f0f\u4e0a\u8fd9\u4e2a\u5047\u8bbe\u662f\u5b58\u5728\u5408\u7406\u6027\u7684\uff0c\u6700\u7ec8\u7684\u5206\u5e03\u4e5f\u662f\u80fd\u6ee1\u8db3\u591a\u6837\u6027\u7684\u8981\u6c42\u3002</p> <p>\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91ccVAE \u90fd\u662f\u5728\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u63a8\u5bfc\u7684\u3002\u4f46\u662f\u8861\u91cf\u4e24\u4e2a\u5206\u5e03\u7684\u8ddd\u79bb\u4e5f\u53ef\u4ee5\u7528\u5176\u4ed6\u7684\u5ea6\u91cf\u3002 \u53ef\u4ee5\u53c2\u8003</p> <ul> <li> <p>(Wasserstein AE) https://arxiv.org/pdf/1711.01558</p> </li> <li> <p>(GW-AE) https://arxiv.org/pdf/2209.07007</p> </li> </ul> <p>\u5bf9\u4e8e\u4e0d\u540c\u7684\u5047\u8bbe\u6761\u4ef6\uff0c\u53ef\u4ee5\u5f97\u51fa\u4e0d\u540c\u7684\u5b9e\u73b0\u65b9\u6848\u3002</p>"},{"location":"book/chapter5_generation_theory/5.1MLE/#normalize-flow-nf","title":"normalize flow \uff08NF\uff09","text":"<p>\u5982\u679c\u6620\u5c04\u662f\u53ef\u9006\u7684\uff0c\u90a3\u4e48\u6211\u4eec\u5c31\u53ef\u4ee5\u76f4\u63a5\u8ba1\u7b97\u51fa\u751f\u6210\u6837\u672c\u7684\u5bc6\u5ea6\u51fd\u6570(\u5206\u5e03), \u8fd9\u4e2a\u65f6\u5019\u76f4\u63a5\u4f18\u5316\u6700\u5927\u4f3c\u7136\u5c31\u884c\u4e86\u3002 </p> <p>\u5f52\u4e00\u5316\u6d41\uff08Normalizing Flow\uff09\u662f\u4e00\u79cd\u6982\u7387\u5bc6\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u4e00\u7cfb\u5217\u53ef\u9006\u7684\u53d8\u6362\uff0c\u5c06\u4e00\u4e2a\u590d\u6742\u5206\u5e03\u6620\u5c04\u5230\u4e00\u4e2a\u7b80\u5355\u5206\u5e03\uff08\u901a\u5e38\u662f\u6807\u51c6\u6b63\u6001\u5206\u5e03\uff09\uff0c\u8fd9\u4e9b\u53d8\u6362\u662f\u7531\u5177\u6709\u53ef\u5fae\u53c2\u6570\u7684\u51fd\u6570\u5b9a\u4e49\u7684\uff0c\u56e0\u6b64\u53ef\u4ee5\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5bf9\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u3002\u4ee5\u4e0b\u662f\u4ece\u6700\u5927\u4f3c\u7136\u7684\u89d2\u5ea6\u8be6\u7ec6\u89e3\u91ca\u5f52\u4e00\u5316\u6d41\u7684\u539f\u7406\uff1a</p>"},{"location":"book/chapter5_generation_theory/5.1MLE/#_4","title":"\u6700\u5927\u4f3c\u7136\u4f30\u8ba1","text":"<ol> <li>\u76ee\u6807\uff1a\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u590d\u6742\u5206\u5e03\u7684\u6982\u7387\u5bc6\u5ea6 \u7ed9\u5b9a\u4e00\u4e2a\u6570\u636e\u96c6 \\(\\{x_1, x_2, \\dots, x_N\\}\\)\uff0c\u6211\u4eec\u5e0c\u671b\u62df\u5408\u4e00\u4e2a\u6982\u7387\u5206\u5e03 \\(p_X(x)\\) \u6765\u63cf\u8ff0\u6570\u636e\u7684\u751f\u6210\u8fc7\u7a0b\u3002\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\uff08MLE)),\u76ee\u6807\u662f\u6700\u5927\u5316\u6a21\u578b\u5206\u5e03\u5bf9\u6570\u636e\u7684\u5bf9\u6570\u4f3c\u7136\uff1a\\(\\mathcal{L} = \\sum_{i=1}^N \\log p_X(x_i)\\).</li> </ol> <p>\u7531\u4e8e\u76f4\u63a5\u5efa\u6a21 \\(p_X(x)\\) \u53ef\u80fd\u975e\u5e38\u590d\u6742\uff0c\u5f52\u4e00\u5316\u6d41\u901a\u8fc7\u53d8\u6362\u5c06\u590d\u6742\u5206\u5e03 \\(p_X(x)\\) \u6620\u5c04\u5230\u4e00\u4e2a\u7b80\u5355\u7684\u5206\u5e03\uff08\u5982\u6b63\u6001\u5206\u5e03\uff09\u3002</p> <ol> <li>\u53ef\u9006\u53d8\u6362\u4e0e\u53d8\u5316\u516c\u5f0f \u5f52\u4e00\u5316\u6d41\u5047\u8bbe\u6570\u636e \\(x\\) \u53ef\u4ee5\u901a\u8fc7\u4e00\u7cfb\u5217\u53ef\u9006\u53d8\u6362 \\(f\\) \u4ece\u4e00\u4e2a\u7b80\u5355\u7684\u5206\u5e03 \\(p_Z(z)\\) \u4e2d\u751f\u6210\uff1a</li> </ol> \\[  x = f(z), \\quad z = f^{-1}(x), \\] <p>\u5176\u4e2d \\(z\\) \u662f\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8868\u793a\uff0c\u5176\u6982\u7387\u5bc6\u5ea6\u4e3a \\(p_Z(z)\\)\u3002\u6839\u636e\u6982\u7387\u53d8\u5316\u516c\u5f0f\uff0c\\(x\\) \u7684\u6982\u7387\u5bc6\u5ea6\u53ef\u4ee5\u901a\u8fc7\u53d8\u6362\u7684\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\u8ba1\u7b97\u4e3a\uff1a</p> \\[  p_X(x) = p_Z(z) \\left| \\det \\frac{\\partial f^{-1}}{\\partial x} \\right|, \\] <p>\u6216\u8005\u7b49\u4ef7\u5730\uff1a</p> \\[  \\log p_X(x) = \\log p_Z(z) - \\log \\left| \\det \\frac{\\partial f(x)}{\\partial x} \\right|. \\] <p>\u8fd9\u91cc \\(\\det \\frac{\\partial f(x)}{\\partial x}\\) \u662f\u53d8\u6362 \\(f\\) \u7684\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u884c\u5217\u5f0f\u3002</p> <ol> <li>\u6700\u5927\u4f3c\u7136\u4f30\u8ba1 \u4e3a\u4e86\u6700\u5927\u5316\u5bf9\u6570\u4f3c\u7136 \\(\\mathcal{L}\\)\uff0c\u9700\u8981\u8ba1\u7b97\u6bcf\u4e2a\u6570\u636e\u70b9 \\(x\\) \u7684\u5bf9\u6570\u5bc6\u5ea6\uff1a</li> <li> <p>\u8ba1\u7b97\u6f5c\u5728\u53d8\u91cf \\(z\\)\uff1a \u5229\u7528 \\(z = f^{-1}(x)\\)\uff0c\u5c06\u8f93\u5165\u6570\u636e \\(x\\) \u6620\u5c04\u5230\u6f5c\u5728\u7a7a\u95f4\u3002</p> </li> <li> <p>\u8ba1\u7b97\u7b80\u5355\u5206\u5e03 \\(p_Z(z)\\)\uff1a \u7b80\u5355\u5206\u5e03\u901a\u5e38\u9009\u62e9\u4e3a\u6807\u51c6\u6b63\u6001\u5206\u5e03 \\(\\mathcal{N}(0, I)\\)\uff0c\u56e0\u6b64 \\(\\log p_Z(z)\\) \u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7 \\(z\\) \u7684\u503c\u8ba1\u7b97\u3002</p> </li> <li> <p>\u8ba1\u7b97\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\uff1a \u53d8\u6362 \\(f(x)\\) \u5fc5\u987b\u8bbe\u8ba1\u6210\u6613\u4e8e\u8ba1\u7b97\u5176\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f \\(\\det \\frac{\\partial f(x)}{\\partial x}\\)\u3002</p> </li> </ol> <p>\u6700\u7ec8\uff0c\u901a\u8fc7\u4f18\u5316\u53c2\u6570\uff0c\u6700\u5927\u5316\u4ee5\u4e0b\u5bf9\u6570\u4f3c\u7136\uff1a</p> \\[  \\log p_X(x) = \\log p_Z(f^{-1}(x)) - \\log \\left| \\det \\frac{\\partial f(x)}{\\partial x} \\right|. \\] <ol> <li> <p>\u8bbe\u8ba1\u5f52\u4e00\u5316\u6d41\u7684\u53d8\u6362 \u4e3a\u4e86\u6709\u6548\u8bad\u7ec3\u5f52\u4e00\u5316\u6d41\uff0c\u53d8\u6362 \\(f\\) \u901a\u5e38\u9700\u8981\u6ee1\u8db3\u4ee5\u4e0b\u8981\u6c42\uff1a</p> </li> <li> <p>\u53ef\u9006\u6027 \uff1a\u786e\u4fdd \\(f\\) \u548c \\(f^{-1}\\) \u6613\u4e8e\u8ba1\u7b97\u3002</p> </li> <li> <p>\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\u9ad8\u6548\u8ba1\u7b97 \uff1a\u4f7f\u5f97 \\(\\det \\frac{\\partial f(x)}{\\partial x}\\) \u7684\u8ba1\u7b97\u6210\u672c\u8f83\u4f4e\u3002</p> </li> </ol> <p>\u5e38\u7528\u7684\u53d8\u6362\u5305\u62ec\uff1a</p> <ol> <li> <p>Affine Coupling Layer \uff1a\u53ea\u5bf9\u90e8\u5206\u53d8\u91cf\u8fdb\u884c\u53d8\u6362\uff0c\u7b80\u5316\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\u7684\u8ba1\u7b97\u3002</p> </li> <li> <p>Spline Flows \uff1a\u57fa\u4e8e\u5206\u6bb5\u51fd\u6570\u7684\u6d41\uff0c\u80fd\u591f\u6355\u83b7\u66f4\u591a\u590d\u6742\u6027\u3002</p> </li> <li> <p>RealNVP  \u548c Glow \uff1a\u5229\u7528\u7279\u5b9a\u7684\u7ed3\u6784\u8bbe\u8ba1\u9ad8\u6548\u7684\u53d8\u6362\u3002</p> </li> </ol>"},{"location":"book/chapter5_generation_theory/5.1MLE/#loss","title":"Loss \u7684\u516c\u5f0f\u76f4\u89c2\u62c6\u89e3","text":"<p>\u5728\u5f52\u4e00\u5316\u6d41\u4e2d\uff0c\u5bf9\u6570\u4f3c\u7136\u53ef\u4ee5\u5199\u4e3a\uff1a</p> \\[  \\log p_X(x) = \\log p_Z(f^{-1}(x)) - \\log \\left| \\det \\frac{\\partial f(x)}{\\partial x} \\right|. \\] <p>\u8bad\u7ec3\u7684\u76ee\u6807\u662f\u6700\u5927\u5316\u8fd9\u4e2a\u5bf9\u6570\u4f3c\u7136 \uff0c\u5373\u6700\u5c0f\u5316\u8d1f\u5bf9\u6570\u4f3c\u7136\uff08Negative Log-Likelihood, NLL\uff09\uff1a</p> \\[  \\text{Loss} = -\\mathbb{E}_{x \\sim p_X} \\left[ \\log p_Z(f^{-1}(x)) - \\log \\left| \\det \\frac{\\partial f(x)}{\\partial x} \\right| \\right]. \\] <p>\u4ece\u76f4\u89c2\u89d2\u5ea6\uff0cLoss \u7684\u4e24\u4e2a\u90e8\u5206\u53ef\u4ee5\u7406\u89e3\u4e3a\uff1a</p> <ol> <li> <p>\u6f5c\u5728\u7a7a\u95f4\u7684\u8d1f\u5bf9\u6570\u5bc6\u5ea6 \uff08\\(-\\log p_Z(f^{-1}(x))\\)\uff09\uff1a</p> </li> <li> <p>\u8fd9\u4e2a\u9879\u8868\u793a\u6570\u636e\u70b9 \\(x\\) \u6620\u5c04\u5230\u6f5c\u5728\u7a7a\u95f4\u7684\u70b9 \\(z = f^{-1}(x)\\) \u5728\u7b80\u5355\u5206\u5e03 \\(p_Z(z)\\) \u4e0a\u7684\u6982\u7387\u5bc6\u5ea6\u3002</p> </li> <li> <p>\u76f4\u89c2\u4e0a\uff0c\u8d8a\u9ad8\u7684\u5bc6\u5ea6\u8868\u793a \\(x\\) \u88ab\u6a21\u578b\u89e3\u91ca\u5f97\u8d8a\u597d\uff0cLoss \u8d8a\u5c0f\u3002</p> </li> <li> <p>\u76ee\u6807\u662f\u8ba9\u6570\u636e\u70b9 \\(z\\) \u66f4\u63a5\u8fd1\u7b80\u5355\u5206\u5e03\u7684\u9ad8\u5bc6\u5ea6\u533a\u57df\uff08\u5982\u6807\u51c6\u6b63\u6001\u5206\u5e03\u7684\u4e2d\u5fc3\uff09\u3002</p> </li> <li> <p>\u53d8\u6362\u590d\u6742\u6027\u7684\u4ee3\u4ef7 \uff08\\(-\\log \\left| \\det \\frac{\\partial f(x)}{\\partial x} \\right|\\)\uff09\uff1a</p> </li> <li> <p>\u8fd9\u4e2a\u9879\u91cf\u5316\u4e86\u53d8\u6362 \\(f\\) \u7684\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u53d8\u6362\u5982\u4f55\u62c9\u4f38\u6216\u538b\u7f29\u7a7a\u95f4\u3002</p> </li> <li> <p>\u76f4\u89c2\u4e0a\uff0c\u5982\u679c\u53d8\u6362\u9700\u8981\u5bf9\u6570\u636e\u8fdb\u884c\u5927\u8303\u56f4\u7684\u626d\u66f2\u6216\u62c9\u4f38\u6765\u5339\u914d\u6570\u636e\u5206\u5e03\uff0c\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\u4f1a\u8f83\u5927\uff0c\u5bfc\u81f4\u8fd9\u4e2a\u9879\u7684\u503c\u589e\u52a0\uff0c\u4ece\u800c\u635f\u5931\u589e\u5927\u3002</p> </li> <li> <p>\u76ee\u6807\u662f\u8ba9\u53d8\u6362 \\(f\\) \u5c3d\u91cf\u7b80\u5355\uff0c\u540c\u65f6\u80fd\u6709\u6548\u5339\u914d\u6570\u636e\u5206\u5e03\u3002</p> </li> </ol>"},{"location":"book/chapter5_generation_theory/5.1MLE/#_5","title":"\u7528\u65e5\u5e38\u6bd4\u55bb\u76f4\u89c2\u7406\u89e3","text":"<p>\u53ef\u4ee5\u5c06 Loss \u7684\u4e24\u4e2a\u90e8\u5206\u7c7b\u6bd4\u4e3a\uff1a</p> <ol> <li> <p>\u9002\u914d\u6570\u636e\u7684\u8fc7\u7a0b \uff1a</p> </li> <li> <p>\u60f3\u8c61\u4f60\u6709\u4e00\u5757\u5e03\uff08\u7b80\u5355\u5206\u5e03\uff09\uff0c\u9700\u8981\u5c06\u5b83\u62c9\u4f38\u548c\u6298\u53e0\uff08\u53d8\u6362\uff09\u4ee5\u5b8c\u5168\u8986\u76d6\u4e00\u4e2a\u590d\u6742\u7684\u5730\u5f62\uff08\u6570\u636e\u5206\u5e03\uff09\u3002</p> </li> <li> <p>\u5e03\u7684\u6bcf\u4e2a\u90e8\u5206\u8d8a\u63a5\u8fd1\u76ee\u6807\u5730\u5f62\u7684\u5b9e\u9645\u5f62\u72b6\uff08\u5373\u6982\u7387\u5bc6\u5ea6\u9ad8\u7684\u533a\u57df\uff09\uff0c\u5c31\u8bf4\u660e\u4f60\u8d8a\u8d34\u5408\u76ee\u6807\uff0c\u7b2c\u4e00\u9879\u7684 Loss \u8d8a\u5c0f\u3002</p> </li> <li> <p>\u62c9\u4f38\u5e03\u7684\u590d\u6742\u6027 \uff1a</p> </li> <li> <p>\u5982\u679c\u9700\u8981\u5bf9\u5e03\u8fdb\u884c\u975e\u5e38\u590d\u6742\u7684\u53d8\u5f62\uff0c\u5e03\u4f1a\u53d8\u5f97\u66f4\u7d27\u6216\u66f4\u677e\uff08\u5bf9\u5e94\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\u7684\u53d8\u5316\uff09\uff0c\u8fd9\u4f1a\u589e\u52a0\u7b2c\u4e8c\u9879\u7684 Loss\u3002</p> </li> </ol> <p>\u6700\u7ec8\uff0cLoss \u5c31\u662f\u8fd9\u4e24\u8005\u7684\u52a0\u6743\u603b\u6210\u672c\uff1a\u65e2\u5e0c\u671b\u5e03\u80fd\u5f88\u597d\u5730\u8986\u76d6\u5730\u5f62\uff0c\u53c8\u5e0c\u671b\u53d8\u5f62\u7684\u8fc7\u7a0b\u4e0d\u8981\u8fc7\u4e8e\u590d\u6742\u3002</p> <p>\u5b9e\u9645\u5b9e\u73b0\u7684\u65f6\u5019\uff0cnormalize flow \u53ef\u4ee5\u8bbe\u8ba1\u4e3a\u53ef\u9006\u6620\u5c04\u7684\u590d\u5408\u3002</p>"},{"location":"book/chapter5_generation_theory/5.1MLE/#diffusion-model","title":"Diffusion Model","text":"<p>\u6211\u4eec\u4e5f\u53ef\u4ee5\u4ece\u6700\u5927\u4f3c\u7136\u7684\u89d2\u5ea6\u63a8\u5230\u51fa Diffusion model \u7684\u4f18\u5316\u76ee\u6807\u3002\u7c7b\u4f3c\u4e8eVAE\uff0c\u4e0d\u8fc7\u8fd9\u91cc\u7684\\(z\\) \u8981\u770b\u505a\u4e3a \\(X_{1:T}\\), \\(x\\) \u4e3a \\(X_0\\)\u3002\u540c\u6837\u5229\u7528\u53d8\u5206\u5f97\u5230ELBO.</p> <p>1. \u6700\u5927\u4f3c\u7136\u76ee\u6807</p> <p>\u6700\u5927\u5316\u6570\u636e\u5206\u5e03\u7684\u5bf9\u6570\u4f3c\u7136 \\(\\log p_\\theta(x_0)\\)\u3002\u6839\u636e\u53d8\u5206\u539f\u7406\uff0c\u5f15\u5165\u4e2d\u95f4\u53d8\u91cf \\(x_{1:T}\\)\uff0c\u5f97\u5230\uff1a</p> \\[  \\log p_\\theta(x_0) = \\log \\int p_\\theta(x_0, x_{1:T}) \\, dx_{1:T}. \\] <p>\u901a\u8fc7\u5f15\u5165\u5206\u5e03 \\(q(x_{1:T} \\mid x_0)\\)\uff0c\u5229\u7528\u5bf9\u6570\u5206\u89e3\uff1a</p> \\[  \\log p_\\theta(x_0) = \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\left[ \\log \\frac{p_\\theta(x_0, x_{1:T})}{q(x_{1:T} \\mid x_0)} \\right] + D_{\\text{KL}}(q(x_{1:T} \\mid x_0) \\| p_\\theta(x_{1:T} \\mid x_0)). \\] <p>\u7531\u4e8e KL \u6563\u5ea6\u975e\u8d1f\uff0c\u5f97\u5230\u53d8\u5206\u4e0b\u754c\uff08ELBO\uff09\uff1a</p> \\[  \\log p_\\theta(x_0) \\geq \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\left[ \\log \\frac{p_\\theta(x_0, x_{1:T})}{q(x_{1:T} \\mid x_0)} \\right]. \\] <p>2. \u8054\u5408\u5206\u5e03\u5206\u89e3</p> <p>\u6a21\u578b\u7684\u8054\u5408\u5206\u5e03 \\(p_\\theta(x_0, x_{1:T})\\) \u548c\u6269\u6563\u8fc7\u7a0b \\(q(x_{1:T} \\mid x_0)\\) \u5206\u522b\u8868\u793a\u4e3a\uff1a</p> \\[  p_\\theta(x_0, x_{1:T}) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1} \\mid x_t), \\] \\[  q(x_{1:T} \\mid x_0) = \\prod_{t=1}^T q(x_t \\mid x_{t-1}), \\] <p>\u4ee3\u5165 ELBO\uff1a</p> \\[  \\log p_\\theta(x_0) \\geq \\mathbb{E}*{q(x*{1:T} \\mid x_0)} \\left[ \\log \\frac{p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1} \\mid x_t)}{\\prod_{t=1}^T q(x_t \\mid x_{t-1})} \\right]. \\] <p>3. \u5206\u89e3\u548c\u7b80\u5316</p> <p>\u5c06\u5bf9\u6570\u5c55\u5f00\uff1a</p> \\[  \\log p_\\theta(x_0) \\geq \\mathbb{E}*{q(x*{1:T} \\mid x_0)} \\Bigg[ \\log p(x_T) + \\sum_{t=1}^T \\log p_\\theta(x_{t-1} \\mid x_t) - \\sum_{t=1}^T \\log q(x_t \\mid x_{t-1}) \\Bigg]. \\] <p>\u6ce8\u610f \\(q(x_t \\mid x_{t-1})\\) \u662f\u6269\u6563\u8fc7\u7a0b\u7684\u5df2\u77e5\u9ad8\u65af\u5206\u5e03\uff0c\u4e0b\u4e00\u6b65\u5c06\u91cd\u70b9\u5206\u6790\u3002</p> <p>4. \u9010\u6b65\u8c03\u6574\u5206\u5e03\u9879</p> <p>\u7531\u4e8e \\(q(x_{1:T} \\mid x_0)\\) \u662f\u5df2\u77e5\u7684\u52a0\u566a\u8fc7\u7a0b\uff0c\u6211\u4eec\u53ef\u4ee5\u5229\u7528\u6761\u4ef6\u5206\u5e03 \\(q(x_{t-1} \\mid x_t, x_0)\\) \u6765\u66ff\u4ee3\u76f4\u63a5\u7684 \\(q(x_t \\mid x_{t-1})\\)\u3002\u901a\u8fc7 \\(q(x_{t-1} \\mid x_t, x_0) \\cdot q(x_t \\mid x_0)\\) \u7684\u5173\u7cfb\uff1a</p> \\[  \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\Bigg[ \\sum_{t=1}^T \\log \\frac{p_\\theta(x_{t-1} \\mid x_t)}{q(x_t \\mid x_{t-1})} \\Bigg] = \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\Bigg[ \\log \\frac{p_\\theta(x_{t-1} \\mid x_t)}{q(x_{t-1} \\mid x_t, x_0)} \\Bigg]. \\] <p>\u4e8e\u662f\uff0c\u76ee\u6807\u5206\u89e3\u4e3a\u4ee5\u4e0b\u51e0\u90e8\u5206\uff1a</p> <ol> <li>\u91cd\u5efa\u8bef\u5dee\uff08Reconstruction Term\uff09\uff1a</li> </ol> \\[  \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\Big[ \\log p_\\theta(x_0 \\mid x_1) \\Big]. \\] <ol> <li>\u5148\u9a8c\u5339\u914d\u8bef\u5dee\uff08Prior Matching Term\uff09\uff1a</li> </ol> \\[  \\mathbb{E}_{q(x_T \\mid x_0)} \\Big[ \\log p(x_T) - \\log q(x_T \\mid x_0) \\Big] = -D_{\\text{KL}}(q(x_T \\mid x_0) \\| p(x_T)). \\] <ol> <li>\u53bb\u566a\u8bef\u5dee\uff08Denoising Matching Term\uff09\uff1a</li> </ol> \\[  \\sum_{t=2}^T \\mathbb{E}_{q(x_t \\mid x_0)} \\Big[ D_{\\text{KL}}(q(x_{t-1} \\mid x_t, x_0) \\| p_\\theta(x_{t-1} \\mid x_t)) \\Big]. \\] <p>5. \u6700\u7ec8\u635f\u5931\u51fd\u6570</p> <p>\u6700\u7ec8\u635f\u5931\u51fd\u6570\u4e3a\uff1a</p> \\[  \\mathcal{L} = \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\Big[ \\underbrace{-\\log p_\\theta(x_0 \\mid x_1)}_{\\text{\u91cd\u5efa\u8bef\u5dee}} + \\underbrace{D_{\\text{KL}}(q(x_T \\mid x_0) \\| p(x_T))}_{\\text{\u5148\u9a8c\u5339\u914d\u8bef\u5dee}} + \\underbrace{\\sum_{t=2}^T D_{\\text{KL}}(q(x_{t-1} \\mid x_t, x_0) \\| p_\\theta(x_{t-1} \\mid x_t))}_{\\text{\u53bb\u566a\u8bef\u5dee}} \\Big]. \\] <p>6. \u7b80\u5316\u4e0e\u4f18\u5316</p> <p>\u901a\u8fc7\u5047\u8bbe \\(p_\\theta(x_{t-1} \\mid x_t)\\) \u548c \\(q(x_{t-1} \\mid x_t, x_0)\\) \u90fd\u662f\u9ad8\u65af\u5206\u5e03\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u53bb\u566a\u8bef\u5dee\uff1a</p> \\[  \\mathcal{L}_{\\text{DDPM}} = \\mathbb{E}_{x_0, \\epsilon, t} \\Big[ \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\Big]. \\]"},{"location":"book/chapter5_generation_theory/5.1MLE/#_6","title":"\u81ea\u56de\u5f52\u6a21\u578b","text":"<ol> <li>\u56fe\u7247\u8868\u793a\u4e0e\u6700\u5927\u4f3c\u7136\u76ee\u6807</li> </ol> <p>\u5bf9\u4e8e\u4e00\u5f20\u56fe\u7247 \\(\\(\\mathbf{x}\\)\\)\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5176\u8868\u793a\u4e3a\u50cf\u7d20\u503c\u7684\u5e8f\u5217\uff1a</p> \\[  \\mathbf{x} = (x_1, x_2, \\ldots, x_T) \\] <p>\u5176\u4e2d \\(T\\) \u662f\u56fe\u7247\u50cf\u7d20\u7684\u603b\u6570\u3002\u751f\u6210\u56fe\u7247\u7684\u76ee\u6807\u662f\u6700\u5927\u5316\u56fe\u7247\u5728\u6a21\u578b\u4e0b\u7684\u6982\u7387\uff1a</p> \\[  P(\\mathbf{x}|\\theta) = P(x_1, x_2, \\ldots, x_T|\\theta) \\] <p>\u8fd9\u91cc\uff0c\\(\\theta\\) \u662f\u6a21\u578b\u7684\u53c2\u6570\u3002</p> <ol> <li>\u81ea\u56de\u5f52\u5206\u89e3 \u6839\u636e\u6982\u7387\u7684\u94fe\u5f0f\u89c4\u5219\uff0c\u56fe\u7247\u7684\u8054\u5408\u6982\u7387\u53ef\u4ee5\u5206\u89e3\u4e3a\u6bcf\u4e2a\u50cf\u7d20\u7684\u6761\u4ef6\u6982\u7387\u7684\u4e58\u79ef\uff1a</li> </ol> \\[  P(\\mathbf{x}|\\theta) = \\prod_{t=1}^T P(x_t | x_1, x_2, \\ldots, x_{t-1}, \\theta) \\] <p>\u8fd9\u8868\u793a\u5f53\u524d\u50cf\u7d20 \\(x_t\\) \u7684\u751f\u6210\u4f9d\u8d56\u4e8e\u4e4b\u524d\u751f\u6210\u7684\u50cf\u7d20\u503c\u3002</p> <ol> <li>\u5bf9\u6570\u4f3c\u7136\u51fd\u6570</li> </ol> <p>\u7531\u4e8e\u76f4\u63a5\u4f18\u5316\u6982\u7387\u5b58\u5728\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\uff0c\u6211\u4eec\u4f7f\u7528\u5bf9\u6570\u5f62\u5f0f\uff1a</p> \\[  \\log P(\\mathbf{x}|\\theta) = \\sum_{t=1}^T \\log P(x_t | x_1, x_2, \\ldots, x_{t-1}, \\theta) \\] <ol> <li>\u8d1f\u5bf9\u6570\u4f3c\u7136\u4f5c\u4e3a\u635f\u5931\u51fd\u6570</li> </ol> <p>\u4e3a\u4e86\u8fdb\u884c\u6700\u5c0f\u5316\u4f18\u5316\uff0c\u8f6c\u5316\u4e3a\u8d1f\u5bf9\u6570\u4f3c\u7136\u5f62\u5f0f\uff1a</p> \\[  \\mathcal{L}(\\theta) = - \\log P(\\mathbf{x}|\\theta) = - \\sum_{t=1}^T \\log P(x_t | x_1, x_2, \\ldots, x_{t-1}, \\theta) \\] <p>\u8fd9\u5c31\u662f\u81ea\u56de\u5f52\u751f\u6210\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\u3002</p> <ol> <li>\u50cf\u7d20\u5206\u5e03\u5efa\u6a21</li> </ol> <p>\u4e3a\u4e86\u8ba1\u7b97\u6761\u4ef6\u6982\u7387 \\(\\(P(x_t | x_1, x_2, \\ldots, x_{t-1}, \\theta)\\)\\)\uff0c\u9700\u8981\u5bf9\u50cf\u7d20\u503c\u7684\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\u3002\u5e38\u89c1\u7684\u9009\u62e9\u5305\u62ec\uff1a</p> <ol> <li> <p>\u79bb\u6563\u50cf\u7d20\u503c</p> <p>\u5982\u679c\u50cf\u7d20\u503c\u662f\u79bb\u6563\u7684\uff08\u5982 \\([0, 255]\\) \u7684\u6574\u6570\u503c)),\u6761\u4ef6\u6982\u7387 \\(P(x_t|\\cdot)\\) \u53ef\u4ee5\u901a\u8fc7\u5206\u7c7b\u5668\uff08\u5982 softmax \u8f93\u51fa\u5c42\uff09\u5efa\u6a21\uff1a</p> \\[ P(x_t | x_1, x_2, \\ldots, x_{t-1}, \\theta) = \\text{Softmax}(f_\\theta(x_1, x_2, \\ldots, x_{t-1})) \\] <p>\u6b64\u65f6\uff0c\u635f\u5931\u51fd\u6570\u53ef\u4ee5\u5177\u4f53\u5316\u4e3a\u4ea4\u53c9\u71b5\u635f\u5931\uff1a</p> \\[ \\mathcal{L}(\\theta) = - \\sum_{t=1}^T \\log P(x_t | x_1, x_2, \\ldots, x_{t-1}, \\theta) \\] </li> <li> <p>\u8fde\u7eed\u50cf\u7d20\u503c</p> <p>\u5982\u679c\u50cf\u7d20\u503c\u662f\u8fde\u7eed\u7684\uff08\u5982\u5f52\u4e00\u5316\u5230 \\([0, 1]\\) \u7684\u6d6e\u70b9\u6570)),\u901a\u5e38\u5047\u8bbe\u50cf\u7d20\u503c\u670d\u4ece\u67d0\u79cd\u6982\u7387\u5206\u5e03\uff08\u5982\u9ad8\u65af\u5206\u5e03\uff09\uff1a</p> \\[ P(x_t | x_1, x_2, \\ldots, x_{t-1}, \\theta) = \\mathcal{N}(x_t | \\mu_\\theta, \\sigma_\\theta) \\] <p>\u5176\u4e2d \\(\\mu_\\theta\\) \u548c \\(\\sigma_\\theta\\) \u662f\u6761\u4ef6\u5747\u503c\u548c\u6807\u51c6\u5dee\uff0c\u7531\u6a21\u578b\u9884\u6d4b\u5f97\u5230\u3002\u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u7b49\u4ef7\u4e8e\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\uff1a</p> \\[ \\mathcal{L}(\\theta) = \\frac{1}{2} \\sum_{t=1}^T \\left(x_t - \\mu_\\theta(x_1, x_2, \\ldots, x_{t-1})\\right)^2 \\] </li> <li> <p>\u751f\u6210\u56fe\u7247\u7684\u50cf\u7d20\u987a\u5e8f</p> </li> </ol> <p>\u4e3a\u4e86\u5b9a\u4e49\u5e8f\u5217\u751f\u6210\u987a\u5e8f\uff0c\u81ea\u56de\u5f52\u6a21\u578b\u9700\u8981\u660e\u786e\u50cf\u7d20\u7684\u751f\u6210\u65b9\u5f0f\u3002\u5e38\u89c1\u65b9\u6cd5\u5305\u62ec\uff1a - \u884c\u4f18\u5148\u751f\u6210 \uff1a\u9010\u884c\u4ece\u5de6\u5230\u53f3\u3001\u4ece\u4e0a\u5230\u4e0b\u751f\u6210\u50cf\u7d20\u3002 - \u5757\u4f18\u5148\u751f\u6210 \uff1a\u4ee5\u5757\u7684\u5f62\u5f0f\u751f\u6210\u50cf\u7d20\u3002 - \u81ea\u5b9a\u4e49\u987a\u5e8f \uff1a\u57fa\u4e8e\u7279\u5b9a\u7684\u6392\u5217\u89c4\u5219\u751f\u6210\u3002 \u987a\u5e8f\u7684\u9009\u62e9\u76f4\u63a5\u5f71\u54cd\u6761\u4ef6\u6982\u7387\u7684\u5efa\u6a21\u65b9\u5f0f\u3002</p> <ol> <li>\u603b\u7ed3\uff1a\u81ea\u56de\u5f52\u751f\u6210\u56fe\u7247\u6a21\u578b\u7684\u635f\u5931</li> </ol> <p>\u81ea\u56de\u5f52\u6a21\u578b\u7528\u4e8e\u56fe\u7247\u751f\u6210\u7684\u901a\u7528\u635f\u5931\u51fd\u6570\u4e3a\uff1a</p> \\[  \\mathcal{L}(\\theta) = - \\sum_{t=1}^T \\log P(x_t | x_1, x_2, \\ldots, x_{t-1}, \\theta) \\] <ul> <li> <p>\u5bf9\u4e8e\u79bb\u6563\u50cf\u7d20\uff0c\u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u3002</p> </li> <li> <p>\u5bf9\u4e8e\u8fde\u7eed\u50cf\u7d20\uff0c\u901a\u5e38\u4f7f\u7528\u5747\u65b9\u8bef\u5dee\u6216\u8d1f\u5bf9\u6570\u4f3c\u7136\u3002</p> </li> </ul> <p>\u901a\u8fc7\u5bf9\u6bcf\u4e2a\u50cf\u7d20\u7684\u6761\u4ef6\u6982\u7387\u8fdb\u884c\u4f18\u5316\uff0c\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u751f\u6210\u56fe\u7247\u7684\u590d\u6742\u5206\u5e03\u3002</p>"},{"location":"book/chapter5_generation_theory/5.1MLE/#refenrence","title":"Refenrence","text":"<ul> <li>normalize flow</li> <li>Diffusion \u7efc\u8ff0\u9605\u8bfb\u7b14\u8bb0</li> </ul>"},{"location":"book/chapter5_generation_theory/manifold_hypothesis/","title":"The Manifold Hypothesis: Why High-Dimensional Data Isn't as Complex as It Seems","text":""},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#introduction","title":"Introduction","text":"<p>The manifold hypothesis is a cornerstone concept in machine learning, particularly for understanding and generating complex data like images, text, and sensor signals. It posits that real-world high-dimensional data (e.g., a 256x256 RGB image with 196,608 pixels) doesn\u2019t randomly fill its ambient space. Instead, it concentrates on low-dimensional manifolds\u2014geometric structures governed by far fewer underlying factors (e.g., pose, lighting, identity). This blog explores the implications, challenges, and applications of this hypothesis in modern AI.</p>"},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#core-concepts","title":"Core Concepts","text":""},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#1-intrinsic-vs-extrinsic-dimensionality","title":"1. Intrinsic vs. Extrinsic Dimensionality","text":"<ul> <li>Extrinsic Dimensionality: The raw dimension of the data space (e.g., 196,608 for a 256x256 image).</li> <li>Intrinsic Dimensionality: The true dimension of the manifold capturing meaningful variations (often ~10-100 for images).    Example: A facial image dataset\u2019s intrinsic factors might include facial expression, angle, and lighting\u2014not individual pixels.</li> </ul>"},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#2-curse-of-dimensionality","title":"2. Curse of Dimensionality","text":"<p>High-dimensional spaces are sparse, making tasks like sampling and interpolation inefficient. The manifold hypothesis sidesteps this by focusing on the data-rich, low-dimensional subspace.</p>"},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#3-nonlinear-structure","title":"3. Nonlinear Structure","text":"<p>Manifolds are rarely linear. They can be twisted, folded, or disconnected (e.g., distinct classes in images), requiring models to learn complex mappings between latent and ambient spaces.</p>"},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#implications-for-machine-learning","title":"Implications for Machine Learning","text":""},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#1-generative-models","title":"1. Generative Models","text":"<p>Models like GANs, VAEs, and diffusion models implicitly approximate the data manifold by mapping a low-dimensional latent space to the high-dimensional data space. - GANs: Generate images by sampling latent vectors and projecting them onto the manifold. - Diffusion Models: Gradually perturb data with noise to bridge the manifold and ambient space, enabling stable training.</p>"},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#2-score-based-models-challenges","title":"2. Score-Based Models &amp; Challenges","text":"<p>Score-based models (e.g., diffusion models) estimate gradients (\\(\\nabla_x \\log p_{\\text{data}}(x)\\)) to generate data. However, the manifold hypothesis introduces two key issues: - Undefined Scores: Gradients are computed in the ambient space but are undefined on low-dimensional manifolds. - Inconsistent Estimation: Score matching objectives require data to span the full ambient space, failing when confined to a manifold.</p> <p>Solutions: - Noise Perturbation: Adding small noise (\\(\\mathcal{N}(0, 0.0001)\\)) \"thickens\" the manifold, stabilizing training (see Figure 1).</p>"},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#3-dimensionality-reduction-representation-learning","title":"3. Dimensionality Reduction &amp; Representation Learning","text":"<ul> <li>Autoencoders and t-SNE compress data into manifold-aligned latent spaces.</li> <li>Disentanglement: Unsupervised methods isolate latent factors (e.g., shape vs. texture) to control generation.</li> </ul>"},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#challenges-trade-offs","title":"Challenges &amp; Trade-offs","text":""},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#1-complex-manifold-topology","title":"1. Complex Manifold Topology","text":"<ul> <li>Disconnected manifolds (e.g., MNIST digits) or \"holes\" complicate modeling.</li> <li>Example: A model trained on cats and dogs may struggle to interpolate between classes.</li> </ul>"},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#2-noise-perturbation-trade-offs","title":"2. Noise Perturbation Trade-offs","text":"<ul> <li>Too little noise: Fails to resolve manifold inconsistencies.</li> <li>Too much noise: Corrupts data structure, harming generation quality.</li> </ul>"},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#3-approximation-errors","title":"3. Approximation Errors","text":"<p>Poorly learned manifolds lead to artifacts (e.g., GAN-generated faces with distorted eyes).</p>"},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#applications-beyond-images","title":"Applications Beyond Images","text":""},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#1-natural-language-processing-nlp","title":"1. Natural Language Processing (NLP)","text":"<p>Word embeddings (e.g., Word2Vec) project language onto semantic manifolds, where similar words cluster.</p>"},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#2-sensor-data","title":"2. Sensor Data","text":"<p>EEG signals and other time-series data lie on low-dimensional manifolds tied to physiological states.</p>"},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#3-robotics","title":"3. Robotics","text":"<p>Control policies for joint angles or motion trajectories operate on manifolds.</p>"},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#future-directions","title":"Future Directions","text":"<ol> <li>Manifold-Aware Architectures: Developing models that explicitly respect manifold geometry.</li> <li>Theoretical Guarantees: Formalizing consistency conditions for score-based methods on manifolds.</li> <li>Cross-Domain Manifold Learning: Unifying manifolds across modalities (e.g., image-text pairs).</li> </ol>"},{"location":"book/chapter5_generation_theory/manifold_hypothesis/#conclusion","title":"Conclusion","text":"<p>The manifold hypothesis is more than a theoretical curiosity\u2014it\u2019s a practical framework for tackling high-dimensional data. By exploiting low-dimensional structure, models achieve efficiency, realism, and interpretability. Yet, challenges like nonlinearity, topology, and noise trade-offs remind us that the \"simple\" low-dimensional story is anything but trivial. As generative AI advances, understanding manifolds will remain central to bridging the gap between raw data and meaningful intelligence.</p>"},{"location":"book/chapter6_ebm/cd/","title":"\u5bf9\u6bd4\u6563\u5ea6\uff08CD\uff09\u65b9\u6cd5\u8be6\u89e3\uff1a\u4ece\u7406\u8bba\u63a8\u5bfc\u5230\u76f4\u89c2\u7406\u89e3","text":""},{"location":"book/chapter6_ebm/cd/#_1","title":"\u5f15\u8a00","text":"<p>\u5728\u8bad\u7ec3\u80fd\u91cf\u6a21\u578b\uff08\u5982\u53d7\u9650\u73bb\u5c14\u5179\u66fc\u673a\uff0cRBM\uff09\u65f6\uff0c\u914d\u5206\u51fd\u6570\u7684\u8ba1\u7b97\u4e00\u76f4\u662f\u4e00\u4e2a\u6838\u5fc3\u96be\u9898\u3002\u4f20\u7edf\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u9700\u8981\u8ba1\u7b97\u914d\u5206\u51fd\u6570\u7684\u68af\u5ea6\uff0c\u4f46\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u8fd9\u4e00\u8fc7\u7a0b\u51e0\u4e4e\u4e0d\u53ef\u884c\u3002 2002\u5e74\uff0cGeoffrey Hinton\u63d0\u51fa \u5bf9\u6bd4\u6563\u5ea6\uff08Contrastive Divergence, CD \u65b9\u6cd5\uff0c\u901a\u8fc7\u5de7\u5999\u7684\u8fd1\u4f3c\u7b56\u7565\uff0c\u7ed5\u8fc7\u4e86\u914d\u5206\u51fd\u6570\u7684\u663e\u5f0f\u8ba1\u7b97\uff0c\u6210\u4e3a\u8bad\u7ec3\u80fd\u91cf\u6a21\u578b\u7684\u91cc\u7a0b\u7891\u5f0f\u5de5\u4f5c\u3002 \u672c\u6587\u5c06\u6df1\u5165\u89e3\u6790CD\u7684\u6570\u5b66\u539f\u7406\u3001\u76f4\u89c2\u89e3\u91ca\u53ca\u5176\u5b9e\u9645\u5e94\u7528\u3002</p>"},{"location":"book/chapter6_ebm/cd/#1","title":"1.\u95ee\u9898\u80cc\u666f\uff1a\u80fd\u91cf\u6a21\u578b\u4e0e\u914d\u5206\u51fd\u6570","text":"<p>\u80fd\u91cf\u6a21\u578b\u901a\u8fc7\u80fd\u91cf\u51fd\u6570 $ E_\\theta(x) $ \u5b9a\u4e49\u6982\u7387\u5206\u5e03\uff1a</p> \\[ p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z(\\theta)}, \\quad Z(\\theta) = \\int e^{-E_\\theta(x)} dx, \\] <p>\u5176\u4e2d $ Z(\\theta) $ \u662f\u914d\u5206\u51fd\u6570\u3002\u8bad\u7ec3\u76ee\u6807\u662f\u6700\u5927\u5316\u6570\u636e\u7684\u5bf9\u6570\u4f3c\u7136\uff1a</p> \\[ \\mathcal{L}(\\theta) = E_{x\\sim p_{\\text{data}}(x)}[\\log p_\\theta(x_i)] \\] <p>\u914d\u5206\u51fd\u6570\u7684\u68af\u5ea6\u53ef\u901a\u8fc7\u5bf9\u80fd\u91cf\u51fd\u6570\u6c42\u5bfc\u5f97\u5230\uff1a</p> \\[ \\begin{aligned} \\nabla_\\theta \\log Z(\\theta) &amp;= \\frac{1}{Z(\\theta)} \\nabla_\\theta Z(\\theta) \\\\ &amp;= \\frac{1}{Z(\\theta)} \\int \\nabla_\\theta e^{-E_\\theta(x)} dx \\\\ &amp;= \\frac{1}{Z(\\theta)} \\int -e^{-E_\\theta(x)} \\nabla_\\theta E_\\theta(x) dx \\\\ &amp;= -\\mathbb{E}_{p_\\theta(x)} \\left[ \\nabla_\\theta E_\\theta(x) \\right]. \\end{aligned} \\] <p>\u5c06\u5bf9\u6570\u4f3c\u7136\u7684\u68af\u5ea6\u5206\u89e3\u4e3a\u6570\u636e\u9879\u548c\u6a21\u578b\u9879\uff1a $$ \\begin{aligned} \\nabla_\\theta \\mathcal{L}(\\theta) &amp;= \\mathbb{E}{p{\\text{data}}(x)} \\left[ \\nabla_\\theta \\log p_\\theta(x) \\right] \\ &amp;= \\mathbb{E}{p{\\text{data}}(x)} \\left[ -\\nabla_\\theta E_\\theta(x) - \\nabla_\\theta \\log Z(\\theta) \\right] \\ &amp;= -\\mathbb{E}{p{\\text{data}}(x)} \\left[ \\nabla_\\theta E_\\theta(x) \\right] + \\mathbb{E}{p\\theta(x)} \\left[ \\nabla_\\theta E_\\theta(x) \\right]. \\end{aligned} $$ \u5176\u4e2d\uff1a - \u7b2c\u4e00\u9879\uff1a\u6570\u636e\u5206\u5e03\u7684\u671f\u671b\uff0c\u76f4\u63a5\u8ba1\u7b97\u8bad\u7ec3\u6837\u672c\u7684\u68af\u5ea6\u3002 - \u7b2c\u4e8c\u9879\uff1a\u6a21\u578b\u5206\u5e03\u7684\u671f\u671b\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700MCMC\u91c7\u6837\uff0cCD\u901a\u8fc7\u77ed\u94fe\u91c7\u6837\u8fd1\u4f3c\u3002</p> <p>\u68af\u5ea6\u8ba1\u7b97\u96be\u9898\uff1a \u5bf9\u6570\u4f3c\u7136\u7684\u68af\u5ea6\u4e3a\uff1a $$ \\nabla_\\theta \\mathcal{L}(\\theta) = -\\mathbb{E}{p{\\text{data}}} [\\nabla_\\theta E_\\theta(x)] + \\mathbb{E}{p\\theta(x)} [\\nabla_\\theta E_\\theta(x)]. $$ \u5176\u4e2d\u7b2c\u4e8c\u9879 $ \\mathbb{E}{p\\theta(x)}[\\cdot] $ \u9700\u8981\u4ece\u6a21\u578b\u5206\u5e03\u4e2d\u91c7\u6837\uff0c\u4f46\u4f20\u7edfMCMC\u65b9\u6cd5\u56e0\u9ad8\u7ef4\u7a7a\u95f4\u6536\u655b\u6162\u800c\u4e0d\u53ef\u884c\u3002</p>"},{"location":"book/chapter6_ebm/cd/#2-cd","title":"2. CD\u7684\u6838\u5fc3\u601d\u60f3","text":""},{"location":"book/chapter6_ebm/cd/#21","title":"2.1. \u76f4\u89c2\u89e3\u91ca","text":"<p>CD\u7684\u6838\u5fc3\u601d\u60f3\u662f\u7528\u5c11\u91cfMCMC\u6b65\u9aa4\u751f\u6210\u8fd1\u4f3c\u6837\u672c\uff0c\u66ff\u4ee3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u5b8c\u5168\u6536\u655b\u94fe\uff1a - \u4ece\u6570\u636e\u5206\u5e03\u542f\u52a8\u94fe\uff1a\u7528\u8bad\u7ec3\u6837\u672c $ x_{\\text{data}} $ \u521d\u59cb\u5316MCMC\u94fe\uff0c\u800c\u975e\u968f\u673a\u566a\u58f0\u3002 - \u77ed\u94fe\u91c7\u6837\uff1a\u4ec5\u8fd0\u884c $ k $ \u6b65\uff08\u901a\u5e38 $ k=1 $\uff09MCMC\uff08\u5982Gibbs\u91c7\u6837\uff09\u751f\u6210\u8d1f\u6837\u672c $ x_{\\text{CD_k}} $\u3002 - \u68af\u5ea6\u8fd1\u4f3c\uff1a\u7528 $ x_{\\text{CD_k}} $ \u8fd1\u4f3c\u6a21\u578b\u5206\u5e03\u7684\u671f\u671b\uff0c\u8ba1\u7b97\u68af\u5ea6\u66f4\u65b0\u53c2\u6570\u3002</p> <p>\u4e3a\u4ec0\u4e48\u6709\u6548\uff1f - \u6570\u636e\u5206\u5e03\u9760\u8fd1\u6a21\u578b\u5206\u5e03\uff0c\u77ed\u94fe\u5373\u53ef\u903c\u8fd1\u76ee\u6807\u5206\u5e03\u3002 - \u907f\u514d\u4e86\u957f\u94feMCMC\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3002</p>"},{"location":"book/chapter6_ebm/cd/#3","title":"3. \u6570\u5b66\u63a8\u5bfc","text":""},{"location":"book/chapter6_ebm/cd/#31","title":"3.1  \u76ee\u6807\u51fd\u6570\u4e0e\u68af\u5ea6","text":"<p>\u6700\u5927\u5316\u5bf9\u6570\u4f3c\u7136\u7b49\u4ef7\u4e8e\u6700\u5c0f\u5316KL\u6563\u5ea6\uff1a</p> \\[ D_{\\text{KL}}(p_{\\text{data}} \\| p_\\theta) = \\mathbb{E}_{p_{\\text{data}}} [\\log p_{\\text{data}}(x) - \\log p_\\theta(x)]. \\] <p>\u5176\u68af\u5ea6\u4e3a\uff1a</p> \\[ \\nabla_\\theta D_{\\text{KL}} = -\\mathbb{E}_{p_{\\text{data}}} [\\nabla_\\theta \\log p_\\theta(x)] + \\mathbb{E}_{p_\\theta(x)} [\\nabla_\\theta \\log p_\\theta(x)]. \\]"},{"location":"book/chapter6_ebm/cd/#32-cd","title":"3.2. CD\u7684\u6838\u5fc3\u7b56\u7565","text":"<p>\u76f4\u63a5\u7528\u77ed\u94feMCMC\u91c7\u6837\u8fd1\u4f3c\u6a21\u578b\u5206\u5e03\u7684\u671f\u671b\uff1a - \u6b65\u9aa4\uff1a   1. \u4ece\u8bad\u7ec3\u6570\u636e\u6837\u672c \\(x_{\\text{data}}\\) \u542f\u52a8\u9a6c\u5c14\u53ef\u592b\u94fe\u3002   2. \u8fd0\u884c \\(k\\) \u6b65MCMC\uff08\u5982Gibbs\u91c7\u6837\uff09\u751f\u6210\u6837\u672c \\(x_{\\text{CD\\_k}}\\)\u3002   3. \u7528\u8fd9\u4e9b\u6837\u672c\u7684\u5747\u503c\u8fd1\u4f3c \\(\\mathbb{E}_{p_\\theta(x)}[\\cdot]\\)\uff0c\u5373\uff1a</p> <pre><code> $$\n \\mathbb{E}_{p_\\theta(x)} [\\nabla_\\theta E_\\theta(x)] \\approx \\frac{1}{B} \\sum_{i=1}^B \\nabla_\\theta E_\\theta(x_{\\text{CD\\_k}}^{(i)}).\n $$\n</code></pre>"},{"location":"book/chapter6_ebm/cd/#33","title":"3.3. \u53c2\u6570\u66f4\u65b0\u89c4\u5219\u7684\u7b80\u5316","text":"<p>\u68af\u5ea6\u516c\u5f0f\u7b80\u5316\u4e3a\uff1a</p> \\[ \\nabla_\\theta \\mathcal{L}(\\theta) \\approx -\\mathbb{E}_{p_{\\text{data}}} [\\nabla_\\theta E_\\theta(x)] + \\frac{1}{B} \\sum_{i=1}^B \\nabla_\\theta E_\\theta(x_{\\text{CD\\_k}}^{(i)}). \\] <p>\u53c2\u6570\u66f4\u65b0\u65b9\u5411\u4e3a\uff1a</p> \\[ \\Delta \\theta \\propto -\\nabla_\\theta E_\\theta(x_{\\text{data}}) + \\nabla_\\theta E_\\theta(x_{\\text{CD\\_k}}). \\]"},{"location":"book/chapter6_ebm/cd/#34","title":"3.4. \u7b49\u4ef7\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49","text":"<p>CD\u7684\u7b49\u4ef7\u635f\u5931\u51fd\u6570\u53ef\u8868\u793a\u4e3a\uff1a $$ \\mathcal{L}{\\text{CD}} = \\mathbb{E}{p_{\\text{data}}} [E_\\theta(x)] - \\mathbb{E}{p{\\theta}^{(k)}} [E_\\theta(x)], $$ \u5176\u4e2d\uff1a - \u7b2c\u4e00\u9879\uff1a\u771f\u5b9e\u6570\u636e\u6837\u672c \\( x \\sim p_{\\text{data}} \\) \u7684\u5e73\u5747\u80fd\u91cf\u3002 - \u7b2c\u4e8c\u9879\uff1a\u4ece\u6570\u636e\u5206\u5e03\u542f\u52a8 \\( k \\) \u6b65MCMC\uff08\u5982Gibbs\u91c7\u6837\uff09\u751f\u6210\u7684\u6837\u672c \\( x_{\\text{CD\\_k}} \\sim p_{\\theta}^{(k)} \\) \u7684\u5e73\u5747\u80fd\u91cf\u3002 - \u4f18\u5316\u76ee\u6807\uff1a\u6700\u5c0f\u5316 \\( \\mathcal{L}_{\\text{CD}} \\)\uff0c\u5373\u964d\u4f4e\u771f\u5b9e\u6570\u636e\u7684\u80fd\u91cf\uff0c\u540c\u65f6\u63d0\u9ad8\u751f\u6210\u6570\u636e\u7684\u80fd\u91cf\u3002 - \u4f5c\u7528\uff1a\u964d\u4f4e\u771f\u5b9e\u6837\u672c\u7684\u80fd\u91cf\uff0c\u4f7f\u5176\u66f4\u53ef\u80fd\u88ab\u6a21\u578b\u751f\u6210\u3002\u63d0\u9ad8\u751f\u6210\u6837\u672c\u7684\u80fd\u91cf\uff0c\u4f7f\u5176\u8fdc\u79bb\u5f53\u524d\u6a21\u578b\u5206\u5e03\u3002</p> <p></p>"},{"location":"book/chapter6_ebm/cd/#35","title":"3.5. \u77ed\u94fe\u91c7\u6837\u7684\u5408\u7406\u6027","text":"<ul> <li>\u70ed\u542f\u52a8\uff1a\u4ece\u6570\u636e\u70b9\u542f\u52a8MCMC\u94fe\uff0c\u8d77\u70b9\u9760\u8fd1\u9ad8\u6982\u7387\u533a\u57df\uff0c\u5c11\u91cf\u6b65\u9aa4\u5373\u53ef\u903c\u8fd1\u6a21\u578b\u5206\u5e03\u3002</li> <li>\u5c40\u90e8\u63a2\u7d22\uff1a\u77ed\u94fe\u4e3b\u8981\u8c03\u6574\u6837\u672c\u7684\u5c40\u90e8\u7ed3\u6784\uff08\u5982\u7eb9\u7406\u7ec6\u8282\uff09\uff0c\u800c\u975e\u5168\u5c40\u6a21\u5f0f\u3002</li> </ul>"},{"location":"book/chapter6_ebm/cd/#36-mcmc","title":"3.6. \u4e0e\u4f20\u7edfMCMC\u7684\u5bf9\u6bd4","text":"\u65b9\u6cd5 \u521d\u59cb\u72b6\u6001 \u94fe\u957f \u8ba1\u7b97\u6210\u672c \u4f20\u7edfMCMC \u968f\u673a\u566a\u58f0 \u957f\uff08\u6536\u655b\uff09 \u9ad8 CD \u8bad\u7ec3\u6570\u636e \u77ed\uff08$ k=1 $\uff09 \u4f4e"},{"location":"book/chapter6_ebm/cd/#4","title":"4. \u5e94\u7528\u4e0e\u53d8\u79cd","text":""},{"location":"book/chapter6_ebm/cd/#41-rbm","title":"4.1. \u5728RBM\u4e2d\u7684\u5b9e\u73b0","text":"<ul> <li>Gibbs\u91c7\u6837\u6b65\u9aa4\uff1a</li> <li>\u6b63\u5411\u4f20\u64ad\uff1a\u8ba1\u7b97\u9690\u5c42\u6982\u7387 $ p(h \\mid v_{\\text{data}}) $\uff0c\u91c7\u6837 $ h_0 $\u3002</li> <li>\u53cd\u5411\u91cd\u6784\uff1a\u8ba1\u7b97\u53ef\u89c1\u5c42\u6982\u7387 $ p(v \\mid h_0) $\uff0c\u91c7\u6837 $ v_1 $\u3002</li> <li>\u53c2\u6570\u66f4\u65b0\uff1a   $$   \\Delta w_{ij} \\propto v_{\\text{data},i} h_{0,j} - v_{1,i} h_{1,j}.   $$</li> </ul>"},{"location":"book/chapter6_ebm/cd/#42","title":"4.2 \u6539\u8fdb\u65b9\u6cd5","text":"<ul> <li>Persistent CD (PCD)\uff1a\u8de8\u6279\u6b21\u4fdd\u7559MCMC\u94fe\u72b6\u6001\uff0c\u63d0\u5347\u91c7\u6837\u6548\u7387\uff08Tijmen Tieleman, 2008\uff09\u3002</li> <li>Fast CD\uff1a\u7ed3\u5408\u52a8\u91cf\u3001\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u52a0\u901f\u8bad\u7ec3\u3002</li> </ul>"},{"location":"book/chapter6_ebm/cd/#5","title":"5. \u4f18\u7f3a\u70b9\u5206\u6790","text":""},{"location":"book/chapter6_ebm/cd/#_2","title":"\u4f18\u70b9","text":"<ul> <li>\u9ad8\u6548\uff1a\u907f\u514d\u957f\u94feMCMC\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\u3002</li> <li>\u5b9e\u7528\uff1a\u5728RBM\u7b49\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u52a8\u4e86\u6df1\u5ea6\u5b66\u4e60\u590d\u5174\u3002</li> </ul>"},{"location":"book/chapter6_ebm/cd/#_3","title":"\u7f3a\u70b9","text":"<ul> <li>\u6709\u504f\u4f30\u8ba1\uff1a\u77ed\u94fe\u672a\u6536\u655b\u5230\u5e73\u7a33\u5206\u5e03\uff0c\u68af\u5ea6\u4e0d\u51c6\u786e\u3002</li> <li>\u6a21\u5f0f\u574d\u584c\u98ce\u9669\uff1a\u53ef\u80fd\u9057\u6f0f\u4f4e\u6982\u7387\u533a\u57df\u3002</li> </ul>"},{"location":"book/chapter6_ebm/cd/#6","title":"6. \u603b\u7ed3","text":"<p>\u5bf9\u6bd4\u6563\u5ea6\uff08CD\uff09\u901a\u8fc7\u6709\u9650\u6b65MCMC\u91c7\u6837\u548c\u6570\u636e\u5206\u5e03\u70ed\u542f\u52a8\u7684\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u80fd\u91cf\u6a21\u578b\u8bad\u7ec3\u7684\u914d\u5206\u51fd\u6570\u96be\u9898\u3002\u5c3d\u7ba1\u5b58\u5728\u7406\u8bba\u504f\u5dee\uff0c\u5176\u5b9e\u7528\u6027\u548c\u9ad8\u6548\u6027\u4f7f\u5176\u6210\u4e3a\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u7684\u57fa\u77f3\u4e4b\u4e00\u3002\u4eceRBM\u5230\u73b0\u4ee3\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0cCD\u7684\u601d\u60f3\u4ecd\u5728\u6301\u7eed\u5f71\u54cd\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u53d1\u5c55\u3002</p> <p>\u53c2\u8003\u6587\u732e\uff1a Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8), 1771\u20131800.</p>"},{"location":"book/chapter6_ebm/introduction/","title":"\u80fd\u91cf\u6a21\u578b\uff1a\u7406\u8bba\u4e0e\u5e94\u7528\u5168\u666f\u89e3\u8bfb","text":""},{"location":"book/chapter6_ebm/introduction/#_2","title":"\u5f15\u8a00","text":"<p>\u80fd\u91cf\u6a21\u578b\uff08Energy-Based Models, EBMs\uff09\u662f\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e00\u7c7b\u91cd\u8981\u751f\u6210\u6a21\u578b\uff0c\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u80fd\u91cf\u51fd\u6570\u9690\u5f0f\u5b9a\u4e49\u6570\u636e\u5206\u5e03\u3002\u4e0e\u663e\u5f0f\u5efa\u6a21\u6982\u7387\u5bc6\u5ea6\u7684\u6a21\u578b\uff08\u5982\u6807\u51c6\u5316\u6d41\u3001VAE\uff09\u4e0d\u540c\uff0c\u80fd\u91cf\u6a21\u578b\u901a\u8fc7\u201c\u80fd\u91cf\u503c\u201d\u8861\u91cf\u6837\u672c\u7684\u5408\u7406\u6027\uff0c\u4f4e\u80fd\u91cf\u5bf9\u5e94\u9ad8\u6982\u7387\u533a\u57df\u3002\u8fd9\u79cd\u7075\u6d3b\u6027\u4f7f\u5176\u5728\u56fe\u50cf\u751f\u6210\u3001\u591a\u6a21\u6001\u5b66\u4e60\u3001\u79d1\u5b66\u8ba1\u7b97\u7b49\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u72ec\u7279\u4f18\u52bf\u3002 \u672c\u6587\u5c06\u7cfb\u7edf\u89e3\u6790\u80fd\u91cf\u6a21\u578b\u7684\u6570\u5b66\u57fa\u7840\u3001\u6838\u5fc3\u6d41\u6d3e\u3001\u6269\u5c55\u65b9\u6cd5\u53ca\u5176\u4e0e\u4e3b\u6d41\u751f\u6210\u6a21\u578b\uff08\u5982GAN\u3001\u6269\u6563\u6a21\u578b\uff09\u7684\u8054\u7cfb\uff0c\u5e76\u63a2\u8ba8\u5176\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002</p>"},{"location":"book/chapter6_ebm/introduction/#1","title":"1. \u80fd\u91cf\u6a21\u578b\u7684\u57fa\u7840\u7406\u8bba","text":""},{"location":"book/chapter6_ebm/introduction/#11","title":"1.1. \u5b9a\u4e49\u4e0e\u6570\u5b66\u5f62\u5f0f","text":"<p>\u80fd\u91cf\u6a21\u578b\u901a\u8fc7\u80fd\u91cf\u51fd\u6570 \\(E_\\theta(x)\\) \u9690\u5f0f\u5b9a\u4e49\u6982\u7387\u5206\u5e03\uff1a</p> \\[ P_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z(\\theta)}, \\quad Z(\\theta) = \\int e^{-E_\\theta(x)} dx \\] <ul> <li>\u80fd\u91cf\u51fd\u6570 \\(E_\\theta(x)\\)\uff1a\u8861\u91cf\u6837\u672c \\(x\\) \u7684\u201c\u4e0d\u5408\u7406\u6027\u201d\uff0c\u901a\u5e38\u7531\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316\u3002</li> <li>\u5f52\u4e00\u5316\u56e0\u5b50 \\(Z(\\theta)\\)\uff1a\u786e\u4fdd\u6982\u7387\u79ef\u5206\u4e3a1\uff0c\u4f46\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u96be\u4ee5\u76f4\u63a5\u8ba1\u7b97\u3002</li> </ul> <p>\u7c7b\u6bd4\u7269\u7406\u7cfb\u7edf - \u7269\u7406\u52bf\u80fd\uff1a\u5728\u7269\u7406\u5b66\u4e2d\uff0c\u52bf\u80fd\u8d8a\u4f4e\u7684\u72b6\u6001\u8d8a\u7a33\u5b9a\uff08\u5982\u5c0f\u7403\u5728\u8c37\u5e95\uff09\u3002 - \u80fd\u91cf\u6a21\u578b\uff1a\u80fd\u91cf\u51fd\u6570E(x) \u626e\u6f14\u7c7b\u4f3c\u52bf\u80fd\u7684\u89d2\u8272\uff0c\u4f4e\u80fd\u91cf\u533a\u57df\u5bf9\u5e94\u6570\u636e\u5206\u5e03\u7684\u9ad8\u6982\u7387\u533a\u57df\uff08\u5373\u771f\u5b9e\u6570\u636e\u96c6\u4e2d\u51fa\u73b0\u53ef\u80fd\u6027\u5927\u7684\u6837\u672c\uff09\u3002</p> <p>\u80fd\u91cf\u51fd\u6570\u672c\u8d28\u662f\u4e00\u4e2a\u8bc4\u5206\u51fd\u6570\uff1a - \u5bf9\u5408\u7406\u7684\u6837\u672c\uff08\u5982\u8bad\u7ec3\u6570\u636e\uff09\u8d4b\u4e88\u4f4e\u80fd\u91cf\u503c; - \u5bf9\u4e0d\u5408\u7406\u7684\u6837\u672c\uff08\u5982\u566a\u58f0\u3001\u5f02\u5e38\u503c\uff09\u8d4b\u4e88\u9ad8\u80fd\u91cf\u503c\u3002</p> <p>\u793a\u4f8b\uff1a</p> <ul> <li>\u5728\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u6e05\u6670\u7684\u56fe\u7247\u80fd\u91cf\u4f4e\uff0c\u6a21\u7cca\u6216\u6df7\u4e71\u7684\u56fe\u7247\u80fd\u91cf\u9ad8;</li> <li>\u5728\u6587\u672c\u751f\u6210\u4e2d\uff0c\u8bed\u6cd5\u6b63\u786e\u7684\u53e5\u5b50\u80fd\u91cf\u4f4e\uff0c\u8bed\u4e49\u77db\u76fe\u7684\u53e5\u5b50\u80fd\u91cf\u9ad8\u3002</li> </ul> <p>\u8fd9\u91cc\u5927\u5bb6\u53ef\u80fd\u4f1a\u6709\u56f0\u60d1\uff0c\u867d\u7136\u5b9a\u4e49\u4e86\u4e00\u4e2a\u80fd\u91cf\uff0c\u4f46\u662f\u8fd9\u4e2a\u80fd\u91cf\u5230\u5e95\u600e\u4e48\u4f7f\u7528\uff1f\u4f7f\u7528\u80fd\u91cf\u6a21\u578b\u6709\u4e24\u4e2a\u6838\u5fc3\u8981\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u4e00\u4e2a\u662f\u5b66\u4e60\u51fa\u80fd\u91cf\u51fd\u6570\u548c\u76ee\u6807\u6570\u636e\u5206\u5e03\u4e00\u81f4\uff0c\u53e6\u5916\u4e00\u4e2a\u662f\u4ece\u80fd\u91cf\u51fd\u6570\u4e2d\u91c7\u6837\u51fa\u6570\u636e\uff0c\u8fd9\u662f\u80fd\u91cf\u6a21\u578b\u7684\u6838\u5fc3\u6311\u6218\u3002\u540e\u7eed\u6211\u4eec\u4f1a\u8bb2\u5230\u600e\u4e48\u5b66\u4e60\u80fd\u91cf\u51fd\u6570\uff0c\u53c8\u600e\u4e48\u4ece\u80fd\u91cf\u51fd\u6570\u4e2d\u91c7\u6837\u3002\u76f4\u89c2\u4e0a\u8bb2\uff0c\u5047\u8bbe\u6211\u4eec\u6709\u4e86\u4e00\u4e2a\u80fd\u91cf\u51fd\u6570\uff0c\u90a3\u91c7\u6837\u610f\u5473\u7740\u6211\u4eec\u6309\u7167\u80fd\u91cf\u51fd\u6570\u7ed9\u51fa\u7684\u5206\u5e03\u6982\u7387\u8fdb\u884c\u91c7\u6837\u3002</p>"},{"location":"book/chapter6_ebm/introduction/#12","title":"1.2. \u6838\u5fc3\u5047\u8bbe\u4e0e\u6761\u4ef6","text":"<ul> <li>\u4f4e\u80fd\u91cf\u5bf9\u5e94\u9ad8\u6982\u7387\uff1a\u771f\u5b9e\u6570\u636e\u6837\u672c\u4f4d\u4e8e\u80fd\u91cf\u51fd\u6570\u7684\u4f4e\u80fd\u533a\u57df\u3002</li> <li>\u6570\u5b66\u6761\u4ef6\uff1a</li> <li>\u53ef\u79ef\u6027\uff1a\u4fdd\u8bc1 \\(Z(\\theta)\\) \u5b58\u5728\uff08\u79ef\u5206\u6536\u655b\uff09\u3002</li> <li>\u53ef\u5fae\u5206\u6027\uff1a\u80fd\u91cf\u51fd\u6570\u9700\u5bf9\u8f93\u5165 \\(x\\) \u548c\u53c2\u6570 \\(\\theta\\) \u53ef\u5bfc\uff0c\u4ee5\u652f\u6301\u68af\u5ea6\u4f18\u5316\u3002</li> <li>\u8bad\u7ec3\u5047\u8bbe\uff1a</li> <li>\u5bf9\u6bd4\u5b66\u4e60\uff1a\u901a\u8fc7\u533a\u5206\u771f\u5b9e\u6837\u672c\uff08\u4f4e\u80fd\u91cf\uff09\u4e0e\u566a\u58f0\u6837\u672c\uff08\u9ad8\u80fd\u91cf\uff09\u4f18\u5316\u80fd\u91cf\u51fd\u6570\u3002</li> <li>\u91c7\u6837\u53ef\u884c\u6027\uff1a\u4f9d\u8d56\u9ad8\u6548\u91c7\u6837\u65b9\u6cd5\uff08\u5982MCMC\uff09\u751f\u6210\u8d1f\u6837\u672c\u3002</li> </ul>"},{"location":"book/chapter6_ebm/introduction/#2","title":"2. \u80fd\u91cf\u6a21\u578b\u7684\u6838\u5fc3\u6311\u6218","text":""},{"location":"book/chapter6_ebm/introduction/#21-p_textdatax","title":"2.1. \u4f30\u8ba1\u771f\u5b9e\u6570\u636e\u5206\u5e03 \\(p_{\\text{data}}(x)\\)","text":"<p>\u76f4\u63a5\u8ba1\u7b97\u5f52\u4e00\u5316\u56e0\u5b50 \\(Z(\\theta)\\) \u4e0d\u53ef\u884c\uff0c\u9700\u7ed5\u8fc7\u663e\u5f0f\u79ef\u5206\uff1a - \u5bf9\u6bd4\u6563\u5ea6\uff08CD\uff09\uff1a\u7528\u5c11\u91cfMCMC\u6b65\u9aa4\u751f\u6210\u8d1f\u6837\u672c\uff0c\u5bf9\u6bd4\u4f18\u5316\u80fd\u91cf\u51fd\u6570\uff08\u5982\u53d7\u9650\u73bb\u5c14\u5179\u66fc\u673a\uff09\u3002 - \u5206\u6570\u5339\u914d\uff08Score Matching\uff09\uff1a\u76f4\u63a5\u5339\u914d\u6570\u636e\u5206\u5e03\u7684\u68af\u5ea6 \\(\\nabla_x \\log p_{\\text{data}}(x)\\)\uff08\u5373\u80fd\u91cf\u51fd\u6570\u7684\u8d1f\u68af\u5ea6\uff09\u3002 - \u5bf9\u6297\u8bad\u7ec3\uff1a\u5c06\u751f\u6210\u5668\u4f5c\u4e3a\u91c7\u6837\u5668\uff0c\u5224\u522b\u5668\u4f5c\u4e3a\u80fd\u91cf\u51fd\u6570\uff08\u5982EBGAN\uff09\u3002</p>"},{"location":"book/chapter6_ebm/introduction/#22","title":"2.2. \u9ad8\u6548\u91c7\u6837","text":"<p>\u4ece \\(P_\\theta(x) \\propto e^{-E_\\theta(x)}\\) \u4e2d\u91c7\u6837\u9700\u63a2\u7d22\u4f4e\u80fd\u91cf\u533a\u57df\uff1a - \u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\uff08MCMC\uff09\uff1a   - Langevin\u52a8\u529b\u5b66\uff1a\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u4e0e\u566a\u58f0\u6270\u52a8\u8fed\u4ee3\u91c7\u6837\uff1a</p> <pre><code>$$\nx_{t+1} = x_t - \\eta \\nabla_x E_\\theta(x_t) + \\sqrt{2\\eta} \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, I)\n$$\n</code></pre> <ul> <li>Gibbs\u91c7\u6837\uff1a\u9002\u7528\u4e8e\u5177\u6709\u5c40\u90e8\u4f9d\u8d56\u6027\u7684\u6a21\u578b\uff08\u5982\u73bb\u5c14\u5179\u66fc\u673a\uff09\u3002</li> <li>\u9690\u5f0f\u751f\u6210\u5668\uff1a\u5982GAN\u7684\u751f\u6210\u5668\u76f4\u63a5\u5b66\u4e60\u4ece\u7b80\u5355\u5206\u5e03\u5230\u6570\u636e\u5206\u5e03\u7684\u6620\u5c04\u3002</li> <li>\u6269\u6563\u6a21\u578b\uff1a\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7ed3\u5408\u80fd\u91cf\u68af\u5ea6\u5f15\u5bfc\u91c7\u6837\u3002</li> </ul> <p>\u91c7\u6837\u65f6\u4e00\u822c\u90fd\u9700\u8981\u80fd\u91cf\u51fd\u6570\u7684\u68af\u5ea6\uff0c\u8fd9\u4e2a\u68af\u5ea6\u6211\u4eec\u4e5f\u53eb\u505a\"score function\", \u662f\u4e00\u4e2a\u5411\u91cf\u573a\uff0c\u8868\u793a\u80fd\u91cf\u53d8\u5316\u7684\u65b9\u5411\u548c\u5f3a\u5ea6\u3002</p> <ul> <li>\u9884\u6d4b\u80fd\u91cf\u51fd\u6570\u7684\u6a21\u578b\uff0c \u6a21\u578b\u8f93\u51fa\u4e00\u4e2a\u6807\u91cf\u503c, \u6211\u4eec\u53eb \"EBM\", energy-based model</li> <li>\u76f4\u63a5\u9884\u6d4bscore function \u7684\u6a21\u578b, \u6a21\u578b\u8f93\u51fa\u4e3a\u4e00\u4e2a\u5411\u91cf\u573a, \u6211\u4eec\u53eb\u505a \"SBM\", score based model</li> </ul>"},{"location":"book/chapter6_ebm/introduction/#3","title":"3. \u80fd\u91cf\u6a21\u578b\u7684\u4e3b\u6d41\u6d41\u6d3e","text":""},{"location":"book/chapter6_ebm/introduction/#31-contrastive-divergence","title":"3.1. Contrastive Divergence","text":"<ul> <li> <p>\u6838\u5fc3\uff1a\u6b63\u5e38\u8fed\u4ee3\u6a21\u578b\uff0c\u9700\u8981\u8ba1\u7b97\\(\\theta\\) \u7684\u68af\u5ea6\uff0c\u90a3\u6211\u4eec\u662f\u4e0d\u662f\u53ef\u4ee5\u76f4\u63a5\u8ba1\u7b97\u51fa \\(p_\\theta(x)\\) \u7684\u68af\u5ea6, \u7136\u540e\u6309\u7167\u68af\u5ea6\u65b9\u5411\u53bb\u66f4\u65b0\u68af\u5ea6\u3002 \u6839\u636e\u8fd9\u4e2a\u65b9\u6cd5\u6211\u4eec\u5c31\u5f97\u5230\u4e86\u5bf9\u6bd4\u6563\u5ea6\uff08CD\uff09\u65b9\u6cd5\u3002</p> </li> <li> <p>\u4ee3\u8868\u6a21\u578b\uff1a\u53d7\u9650\u73bb\u5c14\u5179\u66fc\u673a\uff08RBM\uff09\u3001Deep Belief Networks\u3002</p> </li> <li>\u7279\u70b9\uff1a\u8bad\u7ec3\u7a33\u5b9a\uff0c\u4f46MCMC\u91c7\u6837\u6548\u7387\u4f4e\u3002</li> </ul>"},{"location":"book/chapter6_ebm/introduction/#32","title":"3.2. \u5206\u6570\u5339\u914d","text":"<ul> <li>\u6838\u5fc3\uff1a \u76f4\u63a5\u8ba1\u7b97\u4e24\u4e2a\u5206\u5e03\u4e4b\u95f4\u7684\u8ddd\u79bb\u4f1a\u6bd4\u8f83\u96be\uff0c\u4f46\u662f\u8ba1\u7b97\u4e24\u4e2a\u5206\u5e03\u7684\u68af\u5ea6\u7684\u5dee\u5f02\u4f1a\u5bb9\u6613\u4e00\u4e9b\u3002\u540c\u65f6\u5229\u7528\u5206\u90e8\u79ef\u5206\uff0c\u53ef\u4ee5\u7ed5\u8fc7\u539f\u59cb\u5206\u5e03\u7684\u68af\u5ea6\u8ba1\u7b97\u3002\u8fd9\u5c31\u662f\u6240\u8c13\u7684 score matching \u65b9\u6cd5\u3002\u8fd9\u91cc\u7684\u2018score\u2019 \u5c31\u662f\u68af\u5ea6\u3002</li> </ul> \\[  J(\\theta) = \\frac{1}{2} \\mathbb{E}_{p(x)} \\left[ \\| \\nabla_x \\log q_\\theta(x) - \\nabla_x \\log p(x) \\|^2 \\right]. \\] <p>\u5b83\u7b49\u4ef7\u4e8e\u4f18\u5316</p> \\[  \\min_\\theta\\mathbb{E}_{p(x)} \\left[ \\text{trace} \\left( \\nabla_x^2 \\log q_\\theta(x) \\right) + \\frac{1}{2} \\| \\nabla_x \\log q_\\theta(x) \\|^2 \\right]. \\] <p>\u56e0\u4e3a\u4e8c\u9636\u5bfc\u7684\u8ba1\u7b97\u901a\u5e38\u6bd4\u8f83\u590d\u6742\uff0c\u56e0\u6b64\u5728\u8fd9\u4e2a\u65b9\u5411\u4e0a\u4f1a\u6709\u4e0d\u540c\u7684\u5b9e\u8df5\u65b9\u6cd5\uff0c\u5e38\u89c1\u7684\u6709sliced score matching \u548c denosing score matching</p> <ul> <li>\u4ee3\u8868\u6a21\u578b\uff1a\u5e38\u89c1\u7684score matching \u6709 sliced score matching\u548cdenosing score matching</li> <li>\u7279\u70b9\uff1a\u751f\u6210\u8d28\u91cf\u9ad8\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u5927\u3002</li> </ul>"},{"location":"book/chapter6_ebm/introduction/#33","title":"3.3. \u5bf9\u6297\u8bad\u7ec3\u6d3e","text":"<ul> <li>\u6838\u5fc3\uff1a\u5224\u522b\u5668\u4f5c\u4e3a\u80fd\u91cf\u51fd\u6570\uff0c\u751f\u6210\u5668\u4f5c\u4e3a\u91c7\u6837\u5668\u3002</li> <li>\u4ee3\u8868\u6a21\u578b\uff1aEBGAN\u3001BEGAN\u3002</li> <li>\u7279\u70b9\uff1a\u751f\u6210\u9ad8\u6548\uff0c\u4f46\u6613\u6a21\u5f0f\u5d29\u6e83\u3002</li> </ul>"},{"location":"book/chapter6_ebm/introduction/#34","title":"3.4. \u7ed3\u6784\u5316\u80fd\u91cf\u6a21\u578b","text":"<ul> <li>\u6838\u5fc3\uff1a\u5f15\u5165\u9886\u57df\u77e5\u8bc6\uff08\u5982\u56fe\u7ed3\u6784\u3001\u7269\u7406\u7ea6\u675f\uff09\u8bbe\u8ba1\u80fd\u91cf\u51fd\u6570\u3002</li> <li>\u4ee3\u8868\u6a21\u578b\uff1aGraphEBM\uff08\u5206\u5b50\u751f\u6210\uff09\u3001CRF\uff08\u6761\u4ef6\u968f\u673a\u573a\uff09\u3002</li> <li>\u7279\u70b9\uff1a\u53ef\u89e3\u91ca\u6027\u5f3a\uff0c\u6cdb\u5316\u6027\u5f31\u3002</li> </ul>"},{"location":"book/chapter6_ebm/introduction/#4","title":"4.\u80fd\u91cf\u6a21\u578b\u7684\u6269\u5c55\u65b9\u6cd5","text":""},{"location":"book/chapter6_ebm/introduction/#41","title":"4.1. \u6761\u4ef6\u80fd\u91cf\u6a21\u578b","text":"<p>\u901a\u8fc7\u6761\u4ef6\u53d8\u91cf \\(c\\)\uff08\u5982\u6587\u672c\u3001\u6807\u7b7e\uff09\u5efa\u6a21\u6761\u4ef6\u5206\u5e03 \\(p(x \\mid c)\\)\uff1a - \u80fd\u91cf\u51fd\u6570\uff1a\\(E_\\theta(x, c)\\)\uff0c\u6982\u7387\u5206\u5e03\u4e3a</p> <p>$$   p(x \\mid c) = \\frac{e^{-E_\\theta(x, c)}}{Z(c)}, \\quad Z(c) = \\int e^{-E_\\theta(x, c)} dx   $$</p> <ul> <li>\u5e94\u7528\u573a\u666f\uff1a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3001\u53ef\u63a7\u751f\u6210\uff08\u5982DALL\u00b7E 2\uff09\u3002</li> </ul>"},{"location":"book/chapter6_ebm/introduction/#42","title":"4.2. \u9690\u53d8\u91cf\u80fd\u91cf\u6a21\u578b","text":"<p>\u5f15\u5165\u9690\u53d8\u91cf \\(z\\) \u5efa\u6a21\u8054\u5408\u5206\u5e03 \\(p(x, z)\\)\uff0c\u63d0\u5347\u8868\u8fbe\u80fd\u529b\uff1a - \u53d8\u5206\u80fd\u91cf\u6a21\u578b\uff08VAE-EBM\uff09\uff1a\u7ed3\u5408VAE\u7684\u53d8\u5206\u63a8\u65ad\u4e0e\u80fd\u91cf\u51fd\u6570\u7ea6\u675f\u3002</p> <p>$$   \\log p(x) \\geq \\mathbb{E}{q(z|x)} \\left[ -E\\theta(x, z) - \\log q(z|x) \\right] - \\log Z   $$</p> <ul> <li>\u5bf9\u6297\u5f0f\u9690\u53d8\u91cfEBMs\uff1a\u751f\u6210\u5668 \\(G(z)\\) \u751f\u6210\u6837\u672c\uff0c\u80fd\u91cf\u51fd\u6570 \\(E_\\theta(x)\\) \u5224\u522b\u5408\u7406\u6027\u3002</li> </ul>"},{"location":"book/chapter6_ebm/introduction/#43","title":"4.3. \u80fd\u91cf\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b","text":"<p>\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u5f15\u5165\u80fd\u91cf\u51fd\u6570 \\(E_\\theta(x, c)\\)\uff0c\u589e\u5f3a\u751f\u6210\u53ef\u63a7\u6027\uff1a - \u91c7\u6837\u516c\u5f0f\uff1a</p> <p>$$   x_{t-1} = \\text{Denoise}(x_t) - \\eta \\nabla_x E_\\theta(x_t, c)   $$</p> <ul> <li>\u5e94\u7528\uff1a\u5206\u5b50\u751f\u6210\u3001\u827a\u672f\u54c1\u98ce\u683c\u8fc1\u79fb\u3002</li> </ul>"},{"location":"book/chapter6_ebm/introduction/#5","title":"5. \u80fd\u91cf\u6a21\u578b\u4e0e\u5176\u4ed6\u751f\u6210\u6a21\u578b\u7684\u5173\u7cfb","text":""},{"location":"book/chapter6_ebm/introduction/#51-vae","title":"5.1. VAE\uff1a\u9690\u53d8\u91cf\u80fd\u91cf\u6a21\u578b\u7684\u53d8\u5206\u5b9e\u73b0","text":"<p>VAE\u901a\u8fc7\u9690\u53d8\u91cf \\(z\\) \u5efa\u6a21\u8054\u5408\u5206\u5e03 \\(p(x, z)\\)\uff0c\u5176ELBO\u76ee\u6807\u7b49\u4ef7\u4e8e\u80fd\u91cf\u51fd\u6570\u7684\u53d8\u5206\u4f18\u5316\uff1a</p> \\[ \\text{ELBO} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - \\text{KL}(q(z|x) \\| p(z)) \\]"},{"location":"book/chapter6_ebm/introduction/#52-gan","title":"5.2. GAN\uff1a\u5bf9\u6297\u5f0f\u80fd\u91cf\u6a21\u578b","text":"<ul> <li>EBGAN\uff1a\u5224\u522b\u5668 \\(D(x)\\) \u4f5c\u4e3a\u80fd\u91cf\u51fd\u6570\uff0c\u751f\u6210\u5668 \\(G(z)\\) \u6700\u5c0f\u5316 \\(E(G(z))\\)\u3002</li> <li>\u80fd\u91cf\u89c6\u89d2\uff1aGAN\u7684\u5bf9\u6297\u8bad\u7ec3\u9690\u5f0f\u4f18\u5316\u80fd\u91cf\u5206\u5e03\u3002</li> </ul>"},{"location":"book/chapter6_ebm/introduction/#53","title":"5.3. \u6269\u6563\u6a21\u578b\uff1a\u52a8\u6001\u80fd\u91cf\u6a21\u578b","text":"<ul> <li>\u5206\u6570\u5339\u914d\u7b49\u4ef7\u6027\uff1a\u6269\u6563\u6a21\u578b\u5b66\u4e60\u5206\u6570\u51fd\u6570 \\(\\nabla_x \\log p(x)\\)\uff0c\u5373\u80fd\u91cf\u68af\u5ea6 \\(-\\nabla_x E(x)\\)\u3002</li> <li>Langevin\u52a8\u529b\u5b66\uff1a\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u4e0e\u80fd\u91cf\u6a21\u578b\u7684MCMC\u65b9\u6cd5\u4e00\u81f4\u3002</li> </ul>"},{"location":"book/chapter6_ebm/introduction/#54","title":"5.4. \u6807\u51c6\u5316\u6d41\uff1a\u663e\u5f0f\u5f52\u4e00\u5316\u7684\u7279\u4f8b","text":"<p>\u6807\u51c6\u5316\u6d41\u901a\u8fc7\u53ef\u9006\u53d8\u6362\u663e\u5f0f\u4fdd\u8bc1 \\(Z=1\\)\uff0c\u5176\u80fd\u91cf\u51fd\u6570\u4e3a \\(E(x) = -\\log p(x)\\)\uff0c\u4f46\u751f\u6210\u673a\u5236\uff08\u786e\u5b9a\u6027\u6620\u5c04\uff09\u4e0e\u4f20\u7edfEBMs\u4e0d\u540c\u3002</p>"},{"location":"book/chapter6_ebm/introduction/#6","title":"6.\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411","text":""},{"location":"book/chapter6_ebm/introduction/#61","title":"6.1. \u6838\u5fc3\u6311\u6218","text":"<ul> <li>\u8bad\u7ec3\u7a33\u5b9a\u6027\uff1a\u9690\u53d8\u91cf\u4e0e\u6761\u4ef6\u53d8\u91cf\u7684\u8054\u5408\u4f18\u5316\u6613\u5bfc\u81f4\u6a21\u5f0f\u5d29\u6e83\u3002</li> <li>\u91c7\u6837\u6548\u7387\uff1a\u9ad8\u7ef4\u6570\u636e\u4e0bMCMC\u6536\u655b\u7f13\u6162\u3002</li> <li>\u53ef\u89e3\u91ca\u6027\uff1a\u9690\u53d8\u91cf\u7684\u7269\u7406\u610f\u4e49\u4e0d\u660e\u786e\u3002</li> </ul>"},{"location":"book/chapter6_ebm/introduction/#62","title":"6.2. \u672a\u6765\u8d8b\u52bf","text":"<ul> <li>\u9ad8\u6548\u91c7\u6837\u7b97\u6cd5\uff1a\u6269\u6563\u84b8\u998f\u3001\u5feb\u901fMCMC\u53d8\u4f53\u3002</li> <li>\u591a\u6a21\u6001\u7edf\u4e00\u5efa\u6a21\uff1a\u8de8\u6587\u672c\u3001\u56fe\u50cf\u3001\u8bed\u97f3\u7684\u80fd\u91cf\u51fd\u6570\u8bbe\u8ba1\u3002</li> <li>\u4e0e\u5927\u578b\u6a21\u578b\u7ed3\u5408\uff1a\u5229\u7528LLMs\u63d0\u4f9b\u6761\u4ef6\u4fe1\u53f7\u6216\u9690\u53d8\u91cf\u5148\u9a8c\u3002</li> <li>\u79d1\u5b66\u8ba1\u7b97\u6df1\u5316\uff1a\u5206\u5b50\u52a8\u529b\u5b66\u3001\u6750\u6599\u8bbe\u8ba1\u4e2d\u7684\u80fd\u91cf\u6a21\u578b\u5e94\u7528\u3002</li> </ul>"},{"location":"book/chapter6_ebm/sampling/","title":"Sampling from a Distribution","text":"<p>Understanding the process of sampling is crucial in probability and statistics. Sampling is not merely picking random points in space; it involves generating samples that statistically resemble a given probability distribution. This is essential for tasks such as Monte Carlo simulations, statistical modeling, and machine learning.</p>"},{"location":"book/chapter6_ebm/sampling/#key-concepts-in-sampling","title":"Key Concepts in Sampling","text":"<ol> <li> <p>Representation of the Distribution: The samples should reflect the underlying probability distribution. For example, if a distribution has a high density in a specific region, more samples should appear in that region.</p> </li> <li> <p>Convergence to the Distribution: As the number of samples increases, the empirical distribution (e.g., a histogram) should converge to the theoretical probability distribution.</p> </li> <li> <p>Applications: Accurate sampling is vital for simulations and models where results depend on how well samples represent the true distribution.</p> </li> </ol> <p></p> <p>That is to say, we want to give higher priority to sample that have higher probability.</p> <p>Next, if given a density function, how to sample it. Unlike the VAE, Gan which directed generated an example. In the energy based model, we only have the probability density function (precisely, we have the unnormalized probability density function), how can we sample from it?</p> <p>Generally, sample from a probability distribution can be done in this way</p> <ol> <li>sample from a uniform distribution</li> <li>map the sample to the inverse of the CDF function</li> </ol> <p>than we obtained the samples that follow the given probability distribution. Here CDF is the cumulative distribution function, that is</p> \\[ CDF(x) = \\int_{-\\infty}^x p(z) \\, dz \\]"},{"location":"book/chapter6_ebm/sampling/#sample-from-a-simple-gaussian-distribution","title":"sample from a simple gaussian distribution","text":"<p>In python, sample the gussian distribution maybe simple as the following.</p> <pre><code>import torch\nimport torch.distributions as dist\n\n# Sample from a standard normal distribution\nsample = dist.Normal(0, 1).sample()\nprint(sample)\n</code></pre>"},{"location":"book/chapter6_ebm/sampling/#transformations-from-uniform-distributions","title":"Transformations from Uniform Distributions","text":"<p>While Gaussian distributions can be sampled directly, they can also be derived from uniform distributions using transformations like the Box-Muller method:</p> <pre><code>import numpy as np\n\n# Box-Muller transform: sampling normal distribution from uniform\nu1 = np.random.uniform(0, 1)\nu2 = np.random.uniform(0, 1)\nz1 = np.sqrt(-2 * np.log(u1)) * np.cos(2 * np.pi * u2)\nz2 = np.sqrt(-2 * np.log(u1)) * np.sin(2 * np.pi * u2)\nprint(f\"Samples: {z1}, {z2}\")\n</code></pre> <p>This method is efficient because it avoids the need for the inverse CDF, which does not have a closed form for Gaussian distributions.</p>"},{"location":"book/chapter6_ebm/sampling/#sampling-from-complex-distributions","title":"Sampling from Complex Distributions","text":"<p>For complex distributions, especially with unnormalized probability density functions (as in Energy-Based Models), sophisticated methods are needed:</p> <ol> <li>Markov Chain Monte Carlo (MCMC): A broad class of algorithms for sampling from complex distributions.</li> <li>Langevin Dynamics: Combines gradient descent with noise to explore probability spaces.</li> <li>Hamiltonian Monte Carlo (HMC): Uses physical dynamics to efficiently sample from high-dimensional distributions.</li> </ol>"},{"location":"book/chapter6_ebm/sampling/#langevin-dynamics","title":"Langevin Dynamics","text":"<p>Langevin dynamics is an iterative process that uses the following equation to sample from a known distribution \\(p(\\mathbf{x})\\):</p> \\[ \\mathbf{x}_{t+1} = \\mathbf{x}_t + \\tau \\nabla_x \\log p(\\mathbf{x}_t) + \\sqrt{2\\tau}\\mathbf{z}, \\quad \\mathbf{z} \\sim \\mathcal{N}(0, \\mathbf{I}), \\] <p>where \\(\\tau\\) is the step size, and \\(\\mathbf{x}_0\\) is initialized with white noise.</p> <pre><code>import torch\nimport torch.nn as nn\n\n# Langevin dynamics sampling function\ndef langevin_dynamics(energy_function, initial_samples, n_steps=1000, step_size=0.1):\n    x = initial_samples.clone().requires_grad_(True)\n    for i in range(n_steps):\n        energy = energy_function(x)\n        grad = torch.autograd.grad(energy.sum(), x)[0]\n        noise = torch.randn_like(x) * np.sqrt(2 * step_size)\n        x.data += -step_size * grad + noise\n    return x.detach()\n\n# Example energy function\ndef example_energy_function(x):\n    return 0.5 * ((x - 2)**2) + 0.5 * ((x + 2)**2)\n\n# Initialize samples\ninitial_x = torch.zeros(1000, 1)\n# Sample using Langevin dynamics\nsamples = langevin_dynamics(example_energy_function, initial_x)\n</code></pre>"},{"location":"book/chapter6_ebm/sampling/#intuition-behind-langevin-dynamics","title":"Intuition Behind Langevin Dynamics","text":"<ol> <li>Drift Term $ \\nabla_x \\log p(\\mathbf{x}) $: Acts as a force guiding particles toward high-probability regions.</li> <li>Noise Term: Introduces randomness, allowing exploration and preventing particles from getting stuck.</li> <li>Balance Between Drift and Noise: Ensures thorough exploration of the distribution space.</li> <li>Convergence: Over time, the particle distribution converges to the target distribution.</li> </ol> <p>In the following sections, we will discuss the Langevin dynamics in continous form and the proof that is converged to the unerline distribution.</p>"},{"location":"book/chapter6_ebm/sampling/#simulation-example","title":"Simulation Example","text":"<p>The following Python code simulates Langevin dynamics for a Gaussian mixture:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Mixture parameters\npi1, mu1, sigma1 = 0.6, 2.0, 0.5\npi2, mu2, sigma2 = 0.4, -2.0, 0.2\n\n# PDF and gradient functions\ndef gaussian_pdf(x, mu, sigma):\n    return (1.0 / (np.sqrt(2.0 * np.pi) * sigma) * np.exp(-0.5 * ((x - mu)/sigma)**2))\n\ndef mixture_pdf(x):\n    return pi1 * gaussian_pdf(x, mu1, sigma1) + pi2 * gaussian_pdf(x, mu2, sigma2)\n\ndef grad_log_mixture_pdf(x):\n    n1 = gaussian_pdf(x, mu1, sigma1)\n    n2 = gaussian_pdf(x, mu2, sigma2)\n    numerator = (pi1 * n1 * (-(x - mu1) / sigma1**2) + pi2 * n2 * (-(x - mu2) / sigma2**2))\n    denominator = pi1 * n1 + pi2 * n2 + 1e-16\n    return numerator / denominator\n\n# Langevin dynamics parameters\nM = 10_000\nnp.random.seed(1234)\nx_min, x_max = -3.0, 3.0\nx = np.random.uniform(x_min, x_max, size=M)\neta = 0.01\nn_steps = 100\nplot_times = [0, 1, 10, 100]\nsample_snapshots = {0: x.copy()}\n\n# Perform Langevin dynamics\nfor t in range(1, n_steps+1):\n    grad = grad_log_mixture_pdf(x)\n    x = x + eta*grad + np.sqrt(2*eta)*np.random.randn(M)\n    if t in plot_times:\n        sample_snapshots[t] = x.copy()\n\n# Plot results\nplt.figure(figsize=(12, 3))\nfor i, t in enumerate(plot_times):\n    plt.subplot(1, len(plot_times), i+1)\n    plt.hist(sample_snapshots[t], bins=50, density=True, alpha=0.7, color='orange')\n    grid = np.linspace(-4, 5, 400)\n    pdf_vals = mixture_pdf(grid)\n    plt.plot(grid, pdf_vals, 'r-', lw=2)\n    plt.title(f\"t = {t}\")\n    plt.ylim(0, 0.9)\nplt.suptitle(\"Langevin dynamics sampling from a 1D Gaussian mixture\")\nplt.tight_layout()\nplt.show()\n</code></pre> <ul> <li>Code <code>experiment/langevin_dynamics_simulation.ipynb</code></li> </ul> <p>This simulation starts from a uniform distribution and converges to a Gaussian mixture, illustrating the effectiveness of Langevin dynamics in sampling from complex distributions.</p>"},{"location":"book/chapter6_ebm/sampling/#markov-chain-monte-carlo-mcmc","title":"Markov Chain Monte Carlo (MCMC)","text":"<p>MCMC is a broad class of algorithms used to draw samples from complex probability distributions, especially when direct sampling or classical numerical integration is difficult. The main idea behind MCMC is:</p> <ol> <li>We want samples from a target distribution Suppose we have a probability distribution \\(\\pi(x)\\) (often given up to a normalization constant, e.g., \\(\\pi(x)\\propto e^{-U(x)}\\)), and we want to estimate expectations like</li> </ol> \\[  \\mathbb{E}_{x\\sim \\pi}[f(x)] \\;=\\; \\int f(x)\\,\\pi(x)\\,dx. \\] <p>In many applications (e.g., Bayesian inference), \\(\\pi\\) may be high\u2010dimensional or have no closed\u2010form normalizing constant, making direct sampling infeasible.</p> <ol> <li> <p>Construct a Markov Chain whose stationary distribution is \\(\\pi\\) MCMC methods build a Markov chain \\(X_0, X_1, X_2,\\dots\\) with a transition rule \\(X_{t+1}\\sim T(\\cdot\\mid X_t)\\). The key is to design \\(T\\) so that if \\(X_t\\) is distributed according to \\(\\pi\\), then \\(X_{t+1}\\) is also distributed according to \\(\\pi\\). Under suitable conditions (ergodicity), the chain then converges to \\(\\pi\\) from a wide range of initial states, and the samples \\(X_0, X_1, \\dots\\) \u201cmix\u201d throughout the support of \\(\\pi\\).</p> </li> <li> <p>Samples from the chain approximate samples from \\(\\pi\\) If the Markov chain is ergodic and aperiodic, then for large \\(t\\), the distribution of \\(X_t\\) is close to \\(\\pi\\). We can compute empirical averages using$  \\frac{1}{N}\\sum_{t=1}^N f(X_t) $ to estimate \\(\\mathbb{E}_{\\pi}[f]\\). The law of large numbers for Markov chains implies that, as \\(N\\to\\infty\\), these empirical averages converge to the true expectation (under mild regularity conditions).</p> </li> </ol> <p>Popular MCMC approaches include:</p> <ul> <li> <p>Metropolis\u2013Hastings (MH) : Propose a new sample from a proposal distribution \\(q(\\cdot\\mid X_t)\\) and accept or reject it based on a Metropolis acceptance probability that ensures \\(\\pi\\) is the stationary distribution.</p> </li> <li> <p>Gibbs sampling : Update each component in turn from its conditional distribution, often used when conditionals of \\(\\pi\\) are simpler than the joint.</p> </li> </ul>"},{"location":"book/chapter6_ebm/sampling/#hamiltonian-monte-carlo-hmc","title":"Hamiltonian Monte Carlo (HMC)","text":"<p>Hamiltonian Monte Carlo (HMC)  is a specialized MCMC method designed to tackle high\u2010dimensional sampling problems more efficiently than basic Metropolis\u2013Hastings or Gibbs sampling, especially when \\(\\pi(x)\\propto e^{-U(x)}\\) for some smooth potential \\(U(x)\\). Its key ingredients:</p> <ol> <li>Incorporate \u201cphysical\u201d dynamics HMC treats the target variable \\(x\\) as a position in a physical system and introduces an auxiliary momentum variable \\(p\\). Together, \\((x,p)\\) evolve according to (fictitious) Hamiltonian dynamics governed by a Hamiltonian function</li> </ol> \\[H(x,p) = U(x) + \\frac{1}{2}p^\\top M^{-1} p\\] <p>where \\(M\\) is a mass matrix (often the identity).</p> <ol> <li>Hamiltonian flow Starting from \\((x,p)\\), HMC simulates the continuous\u2010time Hamiltonian equations:</li> </ol> \\[  \\begin{cases} \\dot{x} \\;=\\; M^{-1} p,\\\\ \\dot{p} \\;=\\; -\\,\\nabla U(x). \\end{cases} \\] <p>These flow equations conserve the Hamiltonian \\(H(x,p)\\). In practice, one discretizes this flow via a symplectic integrator (e.g., leapfrog method), which approximates the true continuous trajectory but still preserves many beneficial geometry properties.</p> <ol> <li>Metropolis correction After simulating the Hamiltonian system for a certain number of leapfrog steps, HMC performs a Metropolis acceptance/rejection step:</li> <li> <p>Propose a new state \\((x^\\star,p^\\star)\\) by integrating from \\((x,p)\\).</p> </li> <li> <p>Accept or reject based on the Metropolis probability involving the change in Hamiltonian:</p> </li> </ol> \\[  \\alpha \\;=\\; \\min\\Bigl(1,\\;\\exp\\bigl[-(H(x^\\star,p^\\star)-H(x,p))\\bigr]\\Bigr). \\] <p>This ensures the Markov chain has \\(\\pi(x)\\cdot\\mathcal{N}(p\\mid 0,M)\\) as its invariant distribution in the extended space.</p> <ol> <li>Efficient exploration Because Hamiltonian trajectories can travel long distances in the state space without random walk behavior, HMC reduces the random walk inefficiency often seen in simpler MCMC methods. This often leads to better mixing and more decorrelated samples, especially in high dimensions.</li> </ol> <p>Summary of HMC steps</p> <ol> <li> <p>Sample momentum \\(p\\sim \\mathcal{N}(0,M)\\) to get \\((x,p)\\).</p> </li> <li> <p>Simulate Hamiltonian flow  with a symplectic integrator (e.g., leapfrog) for a chosen number of steps \\(L\\) and step size \\(\\epsilon\\). This yields a proposal \\((x^\\star, p^\\star)\\).</p> </li> <li> <p>Accept/Reject \\((x^\\star,p^\\star)\\) using the Metropolis probability \\(\\alpha\\). If accepted, set \\(X_{t+1}=x^\\star\\); else remain at \\(X_{t+1}=x\\).</p> </li> <li> <p>Repeat  for many iterations.</p> </li> </ol>"},{"location":"book/chapter6_ebm/sampling/#references-further-reading","title":"References &amp; Further Reading","text":"<ul> <li>MCMC in general :</li> <li> <p>Chib, S. and Greenberg, E. (1995). Understanding the Metropolis\u2013Hastings Algorithm. The American Statistician.</p> </li> <li> <p>Gilks, W. R., Richardson, S., &amp; Spiegelhalter, D. J. (1995). Markov Chain Monte Carlo in Practice. Chapman &amp; Hall.</p> </li> <li> <p>Hamiltonian Monte Carlo :</p> </li> <li> <p>Duane, S., Kennedy, A. D., Pendleton, B. J., &amp; Roweth, D. (1987). Hybrid Monte Carlo. Physics Letters B.</p> </li> <li> <p>Neal, R. M. (2011). MCMC Using Hamiltonian Dynamics. In Handbook of Markov Chain Monte Carlo (eds S. Brooks, et al.).</p> </li> <li> <p>Betancourt, M. (2018). A Conceptual Introduction to Hamiltonian Monte Carlo.</p> </li> </ul> <p>In short, MCMC is the backbone of sampling from complicated distributions when direct sampling is infeasible. Hamiltonian Monte Carlo refines this idea by incorporating physical dynamics to move quickly through the space, often yielding more efficient sampling in high\u2010dimensional problems.</p>"},{"location":"book/chapter6_ebm/sampling/#langevin-dynamics-sampling","title":"Langevin Dynamics Sampling","text":""},{"location":"book/chapter6_ebm/sampling/#annealed-langevin-dynamics-sampling","title":"Annealed Langevin Dynamics Sampling","text":""},{"location":"book/chapter6_ebm/score_function/","title":"Score Function","text":"<p>In the context of Langevin dynamics and the underlying Fokker-Planck equation, a key component is the gradient of the log-likelihood function \\(\\nabla_x \\log p(x)\\). This gradient is formally known as the Stein's score function, denoted by:</p> \\[ s_\\theta(x) \\overset{\\text{def}}{=} \\nabla_x \\log p_\\theta(x). \\] <p>It's important to distinguish Stein's score function from the ordinary score function, defined as:</p> \\[ s_x(\\theta) \\overset{\\text{def}}{=} \\nabla_\\theta \\log p_\\theta(x). \\] <p>The ordinary score function is the gradient with respect to the parameter \\(\\theta\\) of the log-likelihood, commonly used in maximum likelihood estimation. In contrast, Stein's score function is the gradient with respect to the data point \\(x\\), utilized in Langevin dynamics. Despite the naming conventions, in diffusion literature, Stein's score function is often simply referred to as the score function, and we will adopt this terminology here.</p> <p>In the context of contrastive divergence (CD), the ordinary score function is used to directly calculate the derivative of \\(p_\\theta\\) and update the network parameters.</p> <p>In score matching methods, Stein's score function is employed, where the loss is defined as the divergence between the gradient of the learned score function and the true gradient of the log-likelihood function.</p> <p>Suppose \\(p_\\theta\\) is a mixture of two Gaussian distributions in one and two dimensions. We can visualize the score function of \\(p_\\theta\\) in the following figures:</p> <p></p> <p></p> <p>In two dimensions, the score function can be visualized as a vector field of the log-likelihood function. The arrow direction represents the gradient, and the arrow length indicates the gradient's magnitude. By employing Langevin dynamics, we can randomly choose a point and follow the arrows to sample from the log-likelihood function, ultimately tracing the trajectory of Langevin dynamics. In physics, the score function is analogous to \"drift,\" indicating how diffusion particles should flow towards the lowest energy state.</p>"},{"location":"book/chapter6_ebm/score_function/#reference","title":"Reference","text":"<ul> <li>code about the score function of gaussian mixture: experiment/score_function_gaussian.ipynb</li> </ul>"},{"location":"book/chapter6_ebm/score_matching/","title":"Score matching","text":"<p>Score Matching \u662f\u4e00\u79cd\u4f30\u8ba1\u6982\u7387\u5206\u5e03\u7684\u5bc6\u5ea6\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u6700\u65e9\u7531 Aapo Hyv\u00e4rinen \u5728 2005 \u5e74\u63d0\u51fa\u3002\u5b83\u7684\u4e3b\u8981\u76ee\u6807\u662f\u901a\u8fc7\u76f4\u63a5\u62df\u5408\u6570\u636e\u7684\u6982\u7387\u5bc6\u5ea6\u68af\u5ea6\uff08\u4e5f\u79f0\u4e3a score function\uff09\u6765\u907f\u514d\u5bf9\u5f52\u4e00\u5316\u56e0\u5b50\uff08partition function\uff09\u7684\u663e\u5f0f\u8ba1\u7b97\u3002\u4ee5\u4e0b\u662f\u5bf9 Score Matching \u65b9\u6cd5\u7684\u603b\u7ed3\uff1a</p> <p>\u6838\u5fc3\u601d\u60f3 - \u8bbe \\(p(x)\\) \u662f\u6570\u636e\u7684\u771f\u5b9e\u5206\u5e03\uff0c\u6a21\u578b\u5206\u5e03\u4e3a \\(q_\\theta(x)\\)\u3002</p> <ul> <li> <p>Score Matching \u7684\u76ee\u6807\u662f\u4f7f\u6a21\u578b\u7684 score function \\(\\nabla_x \\log q_\\theta(x)\\) \u5c3d\u53ef\u80fd\u63a5\u8fd1\u6570\u636e\u5206\u5e03\u7684 score function \\(\\nabla_x \\log p(x)\\)</p> </li> <li> <p>\u6362\u53e5\u8bdd\u8bf4\uff0c\u4f18\u5316\u76ee\u6807\u662f\u6700\u5c0f\u5316\u4ee5\u4e0b\u76ee\u6807\u51fd\u6570\uff1a</p> </li> </ul> \\[  J(\\theta) = \\frac{1}{2} \\mathbb{E}_{p(x)} \\left[ \\| \\nabla_x \\log q_\\theta(x) - \\nabla_x \\log p(x) \\|^2 \\right]. \\] <p>\u603b\u4f53\u6765\u8bf4\uff0c\u5728EBM\u548cSBM (energy based model and score based model) \u4e2d,score matching \u90fd\u53ef\u4ee5\u4f7f\u7528\u3002\u5728EBM\u4e2d\uff0c\u51fd\u6570\u662f\u4e00\u4e2a\u6807\u91cf\uff0c\u8868\u793a\u80fd\u91cf\uff0c\u80fd\u91cf\u4f4e\u7684\u5730\u65b9\u8868\u793a\u6982\u7387\u9ad8\uff0c\u7a33\u5b9a\u533a\u57df\u3002\u5728SBM\u4e2d\uff0c\u51fd\u6570\u662f\u4e00\u4e2a\u5411\u91cf\uff0c\u8868\u793a score function\uff0cscore \u8868\u793a\u80fd\u91cf\u6216\u8005\u5bc6\u5ea6\u53d8\u5316\u7684\u5267\u70c8\u7a0b\u5ea6\u548c\u65b9\u5411\uff0c\u968f\u7740score \u7684\u8d1f\u65b9\u5411\uff0c\u4f1a\u5230\u8fbe\u80fd\u91cf\u4f4e\u7684\u533a\u57df\u3002score matching \u5728\u8fd9\u4e24\u79cd\u6a21\u578b\u4e2d\u90fd\u6709\u81ea\u7136\u7684\u6269\u5c55\uff0c\u5373\u53ef\u4ee5\u62df\u5408\u975e\u5e38\u590d\u6742\u7684density function\u6216\u8005\u5176score function\u3002</p> <p>\u5bf9\u4e8escore matching, \u5b83\u4e5f\u6709\u4e0d\u540c\u7684\u5b9e\u73b0\u65b9\u6cd5\u3002</p> <ul> <li>explicite score matching</li> <li>sliced score matching</li> <li>denosing score matching</li> </ul>"},{"location":"book/chapter6_ebm/score_matching/#explicit-score-matching","title":"Explicit Score-Matching","text":""},{"location":"book/chapter6_ebm/score_matching/#esm","title":"\u663e\u5f0f\u5206\u6570\u5339\u914d\uff08ESM\uff09\u7684\u76f4\u89c2\u601d\u60f3","text":"<p>ESM \u7684\u6838\u5fc3\u76ee\u6807\u662f\u8bad\u7ec3\u4e00\u4e2a\u6a21\u578b $ s_\\theta(x) $\uff0c\u4f7f\u5176\u9884\u6d4b\u7684\u68af\u5ea6\u5c3d\u53ef\u80fd\u63a5\u8fd1\u771f\u5b9e\u7684 score function\u3002\u4e3a\u6b64\uff0cESM \u76f4\u63a5\u5b9a\u4e49\u4e00\u4e2a\u635f\u5931\u51fd\u6570\uff1a</p> \\[ \\mathcal{L}(\\theta) = \\mathbb{E}_{x \\sim p(x)} \\left[ \\| s_\\theta(x) - \\nabla_x \\log p(x) \\|^2 \\right] \\] <p>\u4f46\u95ee\u9898\u5728\u4e8e\uff1a\u771f\u5b9e\u5206\u5e03 $ p(x) $ \u672a\u77e5\uff0c\u56e0\u6b64\u65e0\u6cd5\u76f4\u63a5\u8ba1\u7b97\u771f\u5b9e\u7684\u68af\u5ea6\u3002 \u89e3\u51b3\u65b9\u6848\uff1a\u5148\u7528 \u6838\u5bc6\u5ea6\u4f30\u8ba1\uff08KDE\uff09 \u8fd1\u4f3c $ p(x) $\uff0c\u518d\u7528 KDE \u7684\u7ed3\u679c\u8bad\u7ec3\u6a21\u578b\u3002</p>"},{"location":"book/chapter6_ebm/score_matching/#step-1kde","title":"Step 1\uff1a\u7528\u6838\u5bc6\u5ea6\u4f30\u8ba1\uff08KDE\uff09\u8fd1\u4f3c\u6570\u636e\u5206\u5e03","text":"<p>KDE \u662f\u4e00\u79cd\u975e\u53c2\u6570\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u70b9\u5468\u56f4\u7684\u201c\u5e73\u6ed1\u5c0f\u5c71\u4e18\u201d\uff08\u6838\u51fd\u6570\uff09\u53e0\u52a0\u6765\u4f30\u8ba1\u5206\u5e03\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e\u6570\u636e\u70b9 $ {x_1, x_2, ..., x_N} $\uff0cKDE \u7684\u516c\u5f0f\u4e3a\uff1a</p> \\[ \\hat{p}(x) = \\frac{1}{N} \\sum_{i=1}^N K_h(x - x_i) \\] <p>\u5176\u4e2d $ K_h $ \u662f\u6838\u51fd\u6570\uff08\u5982\u9ad8\u65af\u6838\uff09\uff0c$ h $ \u662f\u63a7\u5236\u5e73\u6ed1\u5ea6\u7684\u5e26\u5bbd\u53c2\u6570\u3002</p> <p>\u9ad8\u65af\u6838\u7684\u76f4\u89c2\u89e3\u91ca\uff1a\u6bcf\u4e2a\u6570\u636e\u70b9 $ x_i $ \u5468\u56f4\u751f\u6210\u4e00\u4e2a\u949f\u5f62\u66f2\u7ebf\uff0c\u6240\u6709\u66f2\u7ebf\u53e0\u52a0\u5f62\u6210\u6574\u4f53\u5206\u5e03\u3002</p>"},{"location":"book/chapter6_ebm/score_matching/#step-2-kde","title":"Step 2\uff1a\u8ba1\u7b97 KDE \u7684\u68af\u5ea6","text":"<p>\u4e3a\u4e86\u5f97\u5230 score function\uff0c\u9700\u8ba1\u7b97 $ \\nabla_x \\log \\hat{p}(x) $\u3002\u4ee5\u9ad8\u65af\u6838\u4e3a\u4f8b\uff1a 1. \u8ba1\u7b97\u68af\u5ea6\u7684\u5206\u5b50\u90e8\u5206\uff1a</p> <p>$$    \\nabla_x \\hat{p}(x) = \\frac{1}{N} \\sum_{i=1}^N K_h(x - x_i) \\cdot \\left(-\\frac{x - x_i}{h^2}\\right)    $$</p> <ol> <li>\u8ba1\u7b97\u68af\u5ea6\uff1a</li> </ol> <p>$$    \\nabla_x \\log \\hat{p}(x) = \\frac{\\nabla_x \\hat{p}(x)}{\\hat{p}(x)}    $$</p> <p>\u7269\u7406\u610f\u4e49\uff1a\u68af\u5ea6\u65b9\u5411\u6307\u5411\u5468\u56f4\u6570\u636e\u70b9\u7684\u52a0\u6743\u5e73\u5747\u4f4d\u7f6e\u3002</p>"},{"location":"book/chapter6_ebm/score_matching/#step-3","title":"Step 3\uff1a\u8bad\u7ec3\u6a21\u578b\u62df\u5408\u68af\u5ea6","text":"<p>\u5b9a\u4e49\u4e00\u4e2a\u6a21\u578b\uff08\u5982\u795e\u7ecf\u7f51\u7edc\uff09 $ s_\\theta(x) $\uff0c\u8f93\u5165\u6570\u636e\u70b9 $ x $\uff0c\u8f93\u51fa\u9884\u6d4b\u7684\u68af\u5ea6\u3002\u635f\u5931\u51fd\u6570\u4e3a\uff1a</p> \\[ \\mathcal{L}(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\| s_\\theta(x_i) - \\nabla_x \\log \\hat{p}(x_i) \\|^2 \\] <p>\u8bad\u7ec3\u8fc7\u7a0b\uff1a 1. \u5bf9\u6bcf\u4e2a\u6570\u636e\u70b9 $ x_i $\uff0c\u7528 KDE \u8ba1\u7b97\u5176\u68af\u5ea6 $ \\nabla_x \\log \\hat{p}(x_i) $\u3002 2. \u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\u4f18\u5316\u6a21\u578b\u53c2\u6570 $ \\theta $\uff0c\u4f7f\u9884\u6d4b\u503c\u903c\u8fd1 KDE \u7684\u68af\u5ea6\u3002</p>"},{"location":"book/chapter6_ebm/score_matching/#_1","title":"\u4e3a\u4ec0\u4e48\u9700\u8981\u6539\u8fdb\uff1f","text":"<p>\u5c3d\u7ba1\u65b9\u6cd5\u76f4\u89c2\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a 1. \u7ef4\u5ea6\u707e\u96be\uff1aKDE \u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u6548\u679c\u5dee\uff08\u8ba1\u7b97\u91cf\u5927\u4e14\u4f30\u8ba1\u4e0d\u51c6\uff09\u3002 2. \u8ba1\u7b97\u6548\u7387\uff1a\u6bcf\u4e2a\u70b9\u7684\u68af\u5ea6\u8ba1\u7b97\u9700\u904d\u5386\u6240\u6709\u6570\u636e\uff0c\u590d\u6742\u5ea6\u4e3a $ O(N) $\u3002</p>"},{"location":"book/chapter6_ebm/score_matching/#_2","title":"\u6539\u8fdb\u65b9\u6848","text":"<ol> <li> <p>\u53bb\u566a\u5206\u6570\u5339\u914d\uff08DSM\uff09\uff1a    \u76f4\u63a5\u5411\u6570\u636e\u6dfb\u52a0\u566a\u58f0\uff08\u5982\u9ad8\u65af\u566a\u58f0\uff09\uff0c\u5229\u7528\u566a\u58f0\u5206\u5e03\u7684\u5df2\u77e5\u68af\u5ea6\u95f4\u63a5\u8bad\u7ec3\u6a21\u578b\uff0c\u907f\u514d\u663e\u5f0f\u8ba1\u7b97 $ p(x) $\u3002</p> </li> <li> <p>\u964d\u7ef4\u9884\u5904\u7406\uff1a    \u5bf9\u9ad8\u7ef4\u6570\u636e\u4f7f\u7528 PCA \u6216\u81ea\u7f16\u7801\u5668\u964d\u7ef4\uff0c\u518d\u5728\u4f4e\u7ef4\u7a7a\u95f4\u5e94\u7528 KDE\u3002</p> </li> <li> <p>Mini-batch \u4f18\u5316\uff1a    \u6bcf\u6b21\u968f\u673a\u91c7\u6837\u90e8\u5206\u6570\u636e\u8ba1\u7b97\u68af\u5ea6\uff0c\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002</p> </li> </ol>"},{"location":"book/chapter6_ebm/score_matching/#_3","title":"\u603b\u7ed3\u4e0e\u9002\u7528\u573a\u666f","text":"<ul> <li>\u9002\u7528\u573a\u666f\uff1a\u4f4e\u7ef4\u6570\u636e\u5206\u5e03\u4f30\u8ba1\u3001\u751f\u6210\u6a21\u578b\u9884\u8bad\u7ec3\u3001\u5c0f\u89c4\u6a21\u6570\u636e\u5206\u6790\u3002</li> <li>\u4f18\u52bf\uff1a\u65e0\u9700\u5047\u8bbe\u6570\u636e\u5206\u5e03\u5f62\u5f0f\uff0c\u76f4\u63a5\u901a\u8fc7\u6570\u636e\u5b66\u4e60\u68af\u5ea6\u3002</li> <li>\u5c40\u9650\u6027\uff1a\u9ad8\u7ef4\u6570\u636e\u6548\u679c\u53d7\u9650\uff0c\u9700\u7ed3\u5408 DSM \u6216\u964d\u7ef4\u6280\u672f\u3002</li> </ul> <p>\u901a\u8fc7 KDE \u4e0e\u663e\u5f0f\u5206\u6570\u5339\u914d\u7684\u7ed3\u5408\uff0c\u6211\u4eec\u80fd\u591f\u4ece\u6709\u9650\u7684\u6570\u636e\u4e2d\u201c\u611f\u77e5\u201d\u6982\u7387\u5206\u5e03\u7684\u53d8\u5316\u65b9\u5411\uff0c\u4e3a\u540e\u7eed\u751f\u6210\u6a21\u578b\u6216\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u5960\u5b9a\u57fa\u7840\u3002\u5c3d\u7ba1\u5b58\u5728\u6311\u6218\uff0c\u8fd9\u4e00\u65b9\u6cd5\u5728\u4f4e\u7ef4\u573a\u666f\u4e2d\u4ecd\u662f\u4e00\u4e2a\u7b80\u6d01\u800c\u5f3a\u5927\u7684\u5de5\u5177\u3002</p>"},{"location":"book/chapter6_ebm/score_matching/#_4","title":"\u5b9e\u9a8c","text":"<p>\u6211\u4eec\u7528\u4e00\u4e2a\u4e8c\u7ef4\u7684\u6df7\u5408\u9ad8\u65af\u5206\u5e03\u8fdb\u884c\u5b9e\u9a8c\u3002</p> <pre><code>def generate_data(num_samples=1000):\n    # \u751f\u6210\u4e24\u4e2a\u9ad8\u65af\u5206\u5e03\u7684\u6df7\u5408\u6570\u636e\n    mean1 = [2, 2]\n    cov1 = [[1, 0.5], [0.5, 1]]\n    data1 = np.random.multivariate_normal(mean1, cov1, num_samples//2)\n\n    mean2 = [-2, -2]\n    cov2 = [[1, -0.5], [-0.5, 1]]\n    data2 = np.random.multivariate_normal(mean2, cov2, num_samples//2)\n\n    return np.vstack([data1, data2])\n</code></pre> <p>\u6839\u636e\u4ee5\u4e0a\u7684\u6df7\u5408\u9ad8\u65af\u5206\u5e03\u751f\u6210\u7684\u6570\u636e\u3002  \u7136\u540e\u6211\u4eec\u8fdb\u884cKDE\uff0c\u7136\u540e\u5229\u7528KDE\u53bb\u4f30\u8ba1\u68af\u5ea6\u3002\u6700\u540e\u5f97\u5230\u9884\u6d4b\u7ed3\u679c\u548c\u771f\u5b9e\u68af\u5ea6\u7684\u53ef\u89c6\u5316\u3002</p> <p></p> <p>\u53ef\u4ee5\u770b\u5230\u6700\u7ec8\u5728\u9ad8\u6982\u7387\u533a\u57df\uff0c\u5206\u5e03\u662f\u6bd4\u8f83\u76f8\u4f3c\u7684\uff0c\u4f46\u662f\u5728\u4f4e\u6982\u7387\u533a\u57df\uff0c\u5206\u5e03\u76f8\u5dee\u8f83\u5927\u3002\u8fd9\u4e2a\u539f\u56e0\u4e3b\u8981\u662f\u56e0\u4e3a\u4f4e\u6982\u7387\u533a\u57df\u7684\u6570\u636e\u91cf\u8f83\u5c11\uff0cKDE \u4f30\u8ba1\u7684\u68af\u5ea6\u53ef\u80fd\u4f1a\u53d7\u5230\u5927\u91cf\u566a\u58f0\u7684\u5f71\u54cd\u3002 \u540c\u65f6\u6a21\u578b\u5b66\u51fa\u6765\u7684\u5206\u5e03\u6bd4KDE\u548c\u539f\u59cb\u5206\u5e03\u66f4\u52a0\u63a5\u8fd1\u3002</p>"},{"location":"book/chapter6_ebm/score_matching/#implicit-score-matching","title":"Implicit Score Matching","text":""},{"location":"book/chapter6_ebm/score_matching/#fisher","title":"\u4eceFisher\u6563\u5ea6\u5230\u9690\u5f0f\u5206\u6570\u5339\u914d\u7684\u63a8\u5bfc","text":""},{"location":"book/chapter6_ebm/score_matching/#1-fisher","title":"1. \u5b9a\u4e49Fisher\u6563\u5ea6","text":"<p>Fisher\u6563\u5ea6\u8861\u91cf\u4e24\u4e2a\u6982\u7387\u5206\u5e03 $ p(\\mathbf{x}) $ \u548c $ q(\\mathbf{x}; \\theta) $ \u7684\u5206\u6570\u5dee\u5f02\uff1a</p> \\[ D_F(p \\parallel q) = \\mathbb{E}_{p(\\mathbf{x})} \\left[ \\| \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x}) - \\nabla_{\\mathbf{x}} \\log q(\\mathbf{x}; \\theta) \\|^2 \\right]. \\]"},{"location":"book/chapter6_ebm/score_matching/#2","title":"2. \u5c55\u5f00\u5e73\u65b9\u9879","text":"<p>\u5c06\u5e73\u65b9\u9879\u5c55\u5f00\u4e3a\u4e09\u90e8\u5206\uff1a</p> \\[ D_F = \\underbrace{\\mathbb{E}_{p} \\left[ \\| \\nabla \\log p \\|^2 \\right]}_{T_1} - 2 \\underbrace{\\mathbb{E}_{p} \\left[ (\\nabla \\log p)^\\top (\\nabla \\log q) \\right]}_{T_2} + \\underbrace{\\mathbb{E}_{p} \\left[ \\| \\nabla \\log q \\|^2 \\right]}_{T_3}. \\]"},{"location":"book/chapter6_ebm/score_matching/#3-t_2","title":"3. \u5904\u7406\u4ea4\u53c9\u9879 $ T_2 $","text":"<p>\u4ea4\u53c9\u9879 $ T_2 = -2 \\mathbb{E}_{p} \\left[ (\\nabla \\log p)^\\top (\\nabla \\log q) \\right] $ \u5305\u542b\u672a\u77e5\u7684 $ \\nabla \\log p $\uff0c\u9700\u901a\u8fc7\u5206\u90e8\u79ef\u5206\u6d88\u9664\u4f9d\u8d56\u3002</p>"},{"location":"book/chapter6_ebm/score_matching/#_5","title":"\u5206\u91cf\u4e3a\u6807\u91cf\u7684\u63a8\u5bfc","text":"<p>\u5bf9\u6bcf\u4e2a\u7ef4\u5ea6 $ x_i $ \u5355\u72ec\u5904\u7406\uff1a</p> \\[ T_2 = -2 \\sum_{i=1}^d \\mathbb{E}_{p} \\left[ \\partial_i \\log p \\cdot \\partial_i \\log q \\right]. \\] <p>\u5e94\u7528\u5206\u90e8\u79ef\u5206\u516c\u5f0f\uff1a</p> \\[ \\int p(\\mathbf{x}) \\partial_i \\log p \\cdot \\partial_i \\log q \\, d\\mathbf{x} = -\\int p(\\mathbf{x}) \\partial_i^2 \\log q \\, d\\mathbf{x}. \\] <p>\u5173\u952e\u5047\u8bbe\uff1a\u6982\u7387\u5bc6\u5ea6\u5728\u8fb9\u754c\u5904\u8870\u51cf\u81f3\u96f6\uff0c\u5373\uff1a</p> \\[ \\left. p(\\mathbf{x}) \\partial_i \\log q \\right|_{x_i \\to \\pm\\infty} = 0. \\]"},{"location":"book/chapter6_ebm/score_matching/#_6","title":"\u5408\u5e76\u6240\u6709\u7ef4\u5ea6","text":"<p>\u5bf9\u6bcf\u4e2a\u5206\u91cf\u79ef\u5206\u540e\u6c42\u548c\uff1a</p> \\[ T_2 = 2 \\sum_{i=1}^d \\mathbb{E}_{p} \\left[ \\partial_i^2 \\log q \\right] = 2 \\mathbb{E}_{p} \\left[ \\text{tr}(\\nabla_{\\mathbf{x}}^2 \\log q) \\right], \\] <p>\u5176\u4e2d $ \\text{tr}(\\nabla_{\\mathbf{x}}^2 \\log q) $ \u8868\u793aHessian\u77e9\u9635\u7684\u8ff9\u3002</p>"},{"location":"book/chapter6_ebm/score_matching/#4-fisher","title":"4. \u91cd\u7ec4Fisher\u6563\u5ea6","text":"<p>\u5c06 $ T_1, T_2, T_3 $ \u4ee3\u5165\u539f\u5f0f\uff1a</p> \\[ D_F = \\mathbb{E}_{p} \\left[ \\| \\nabla \\log p \\|^2 \\right] + \\mathbb{E}_{p} \\left[ \\| \\nabla \\log q \\|^2 + 2 \\, \\text{tr}(\\nabla^2 \\log q) \\right]. \\] <p>\u5ffd\u7565\u5e38\u6570\u9879 $ \\mathbb{E}_{p} \\left[ | \\nabla \\log p |^2 \\right] $\uff0c\u4f18\u5316\u76ee\u6807\u7b80\u5316\u4e3a\uff1a</p> \\[ \\min_{\\theta} \\mathbb{E}_{p} \\left[ \\| \\nabla \\log q \\|^2 + 2 \\, \\text{tr}(\\nabla^2 \\log q) \\right]. \\]"},{"location":"book/chapter6_ebm/score_matching/#5","title":"5. \u6807\u51c6\u5316\u76ee\u6807\u51fd\u6570","text":"<p>\u5f15\u5165\u7f29\u653e\u56e0\u5b50 $ \\frac{1}{2} $\uff0c\u5f97\u5230\u9690\u5f0f\u5206\u6570\u5339\u914d\u76ee\u6807\u51fd\u6570\uff1a</p> \\[ J(\\theta) = \\mathbb{E}_{p(\\mathbf{x})} \\left[ \\frac{1}{2} \\| \\nabla_{\\mathbf{x}} \\log q(\\mathbf{x}; \\theta) \\|^2 + \\text{tr}(\\nabla_{\\mathbf{x}}^2 \\log q(\\mathbf{x}; \\theta)) \\right]. \\]"},{"location":"book/chapter6_ebm/score_matching/#6","title":"6. \u9690\u5f0f\u6027\u5206\u6790","text":"<ul> <li>\u663e\u5f0f\u5339\u914d\u9879\uff1a$ \\frac{1}{2} | \\nabla \\log q |^2 $ \u76f4\u63a5\u7ea6\u675f\u5206\u6570\u6a21\u957f\u3002</li> <li>\u9690\u5f0f\u6b63\u5219\u9879\uff1a$ \\text{tr}(\\nabla^2 \\log q) $ \u901a\u8fc7\u4e8c\u9636\u5bfc\u6570\u9690\u5f0f\u7ea6\u675f\u5206\u6570\u65b9\u5411\uff0c\u907f\u514d\u4f9d\u8d56 $ \\nabla \\log p $\u3002</li> </ul>"},{"location":"book/chapter6_ebm/score_matching/#7","title":"7. \u6700\u7ec8\u5f62\u5f0f","text":"<p>\u9690\u5f0f\u5206\u6570\u5339\u914d\u7684\u76ee\u6807\u51fd\u6570\u4e3a\uff1a</p> \\[ \\boxed{J(\\theta) = \\mathbb{E}_{p(\\mathbf{x})} \\left[ \\frac{1}{2} \\| \\nabla_{\\mathbf{x}} \\log q(\\mathbf{x}; \\theta) \\|^2 + \\text{tr}(\\nabla_{\\mathbf{x}}^2 \\log q(\\mathbf{x}; \\theta)) \\right]}. \\] <p>\u5f53\u7136\u5982\u679c\u6211\u4eec\u53ea\u5173\u6ce8score function \u672c\u8eab\uff0c\u800c\u4e0d\u9700\u8981\u77e5\u9053\u539f\u59cb\u7684density function \u6216\u8005\u8bf4energy function, \u90a3\u4e48\u95ee\u9898\u5c31\u7b80\u5316\u4e86\uff0c\u5c31\u4e0d\u6d89\u53ca\u5230\u4e8c\u9636\u5bfc\u4e86\u3002</p> <p>Let \\(s_\\theta(x) = \\nabla_{\\mathbf{x}} \\log q(\\mathbf{x}; \\theta)\\) be score function.</p> <p>The implicit score matching loss can be approximated by Monte Carlo:</p> \\[ J_{\\text{ISM}}(\\theta) \\approx \\frac{1}{M} \\sum_{m=1}^{M} \\sum_{i} \\left( \\partial_i \\mathbf{s}_{\\theta}(\\mathbf{x}^{(m)}) + \\frac{1}{2} \\| \\mathbf{s}_{\\theta}(\\mathbf{x}^{(m)}) \\|^2 \\right), \\] <p>where \\(\\partial_i \\mathbf{s}_{\\theta}(\\mathbf{x}^{(m)}) = \\frac{\\partial}{\\partial x_i} [\\mathbf{s}_{\\theta}(\\mathbf{x})]_i = \\frac{\\partial^2}{\\partial x_i^2} \\log p(\\mathbf{x})\\). If the model for the score function is realized by a deep neural network, the trace operator can be difficult to compute, hence making the implicit score matching not scalable [40].</p> <p>** Refenrence** - Aapo Hyv\u00a8arinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research (JMLR), 6(24):695\u2013709, 2005. https://jmlr.org/papers/volume6/ hyvarinen05a/hyvarinen05a.pdf</p>"},{"location":"book/chapter6_ebm/score_matching/#experiment","title":"experiment","text":"<p>We use a mixture of guassian distribution for testing. But the results is not good.</p> <p> </p> <p>It turns out that the derivative maganitude is near 0, but the trace is almost - 0.0004.</p> <ul> <li>optimizatioin 1 add regularization of \\(E_x p_\\theta(x)\\). Still not work, almost constant </li> <li>optimization 2 output energy function not the \\(p_\\theta\\), that is without a exponention.</li> </ul>"},{"location":"book/chapter6_ebm/score_matching/#siced-score-matching","title":"Siced score matching","text":"<ul> <li>optimization3 Finally, it is because that the network used ReLU, which is not enough for modeling the complex density function. Here we change to Swish function instead. Now the iteration is much more stable and output the correct estimation of the ground truth density function (un-normalized)</li> </ul>"},{"location":"book/chapter6_ebm/score_matching/#_7","title":"\u6838\u5fc3\u601d\u60f3","text":"<p>Sliced Score Matching\uff08\u5207\u7247\u5206\u6570\u5339\u914d\uff09 \u662f\u4e00\u79cd\u7528\u4e8e\u9ad8\u6548\u4f30\u8ba1\u6570\u636e\u5206\u5e03\u68af\u5ea6\uff08score function\uff09\u7684\u65b9\u6cd5\uff0c\u4e3b\u8981\u9488\u5bf9\u9ad8\u7ef4\u6570\u636e\u573a\u666f\u8bbe\u8ba1\u3002\u5176\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u901a\u8fc7\u968f\u673a\u6295\u5f71\u6280\u672f\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u907f\u514d\u76f4\u63a5\u8ba1\u7b97\u9ad8\u7ef4Jacobian\u77e9\u9635\u7684\u8ff9\uff08Trace\uff09\uff0c\u4ece\u800c\u89e3\u51b3\u4f20\u7edfImplicit Score Matching\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002</p> <p>\u76f4\u89c2\u7684\u8bf4\uff0c\u5c31\u662f\u5728\u4efb\u4f55\u7684\u6295\u5f71\u7a7a\u95f4\uff0c\u4e24\u4e2afunction \u80fd\u591f\u8fd1\u4f3c\uff0c\u90a3\u4e48\u539f\u672c\u7684\u4e24\u4e2afunction \u4e5f\u5fc5\u7136\u8fd1\u4f3c\u3002\u8fd9\u662f\u4e00\u79cd\u6bd4\u8f83\u91cd\u8981\u7684\u601d\u60f3\uff0c\u53ef\u4ee5\u5728\u5176\u4ed6\u7684\u95ee\u9898\u4e2d\u501f\u9274\u3002\u540c\u6837\u7684\u8fd8\u6709\uff0c\u5982\u679c\u4e24\u4e2afunction \u7684\u68af\u5ea6\u5904\u5904\u76f8\u7b49\uff0c\u90a3\u4e48\u8fd9\u4e24\u4e2afunction \u4e5f\u76f8\u7b49\uff08\u5dee\u4e00\u4e2aconstant), \u8fd9\u4e9b\u90fd\u662f\u4e00\u79cd\u8f6c\u5316\u95ee\u9898\u7684\u6280\u5de7\uff0c\u503c\u5f97\u5b66\u4e60\u3002</p>"},{"location":"book/chapter6_ebm/score_matching/#_8","title":"\u6570\u5b66\u5b9a\u4e49","text":"<p>1. Fisher \u6563\u5ea6\u7684\u5b9a\u4e49 Fisher \u6563\u5ea6\u8861\u91cf\u771f\u5b9e\u5206\u5e03 $ p(x) $ \u4e0e\u6a21\u578b\u5206\u5e03 $ q_\\theta(x) $ \u7684 score function \u4e4b\u95f4\u7684\u5dee\u5f02\uff1a $$ \\mathcal{D}{\\text{Fisher}}(p | q\\theta) = \\mathbb{E}{p(x)} \\left[ | \\nabla_x \\log p(x) - \\nabla_x \\log q\\theta(x) |^2 \\right]. $$ \u6700\u5c0f\u5316 Fisher \u6563\u5ea6\u7b49\u4ef7\u4e8e\u8ba9\u6a21\u578b score $ \\nabla_x \\log q_\\theta(x) $ \u903c\u8fd1\u771f\u5b9e score $ \\nabla_x \\log p(x) $\u3002</p> <p>\u901a\u8fc7 Stein \u6052\u7b49\u5f0f\uff0cISM \u5c06 Fisher \u6563\u5ea6\u8f6c\u5316\u4e3a\u4ee5\u4e0b\u76ee\u6807\u51fd\u6570\uff1a $$ J_{\\text{ISM}}(\\theta) = \\mathbb{E}{p(x)} \\left[ \\operatorname{Tr}(\\nabla_x \\mathbf{s}\\theta(x)) + \\frac{1}{2} | \\mathbf{s}\\theta(x) |^2 \\right], $$ \u5176\u4e2d $ \\mathbf{s}\\theta(x) $ \u662f\u6a21\u578b\u9884\u6d4b\u7684 score function\u3002 \u5173\u952e\u7b49\u5f0f\uff1a\u5f53 $ \\mathbf{s}\\theta(x) = \\nabla_x \\log p(x) $ \u65f6\uff0cISM \u635f\u5931\u8fbe\u5230\u6700\u5c0f\u503c\uff0c\u6b64\u65f6 $ \\mathcal{D}{\\text{Fisher}} = 0 $\u3002</p> <p>3. Sliced Score Matching \u7684\u52a8\u673a ISM \u7684\u74f6\u9888\u5728\u4e8e\u8ba1\u7b97 Jacobian \u77e9\u9635\u7684\u8ff9 $ \\operatorname{Tr}(\\nabla_x \\mathbf{s}_\\theta(x)) $\uff0c\u5176\u590d\u6742\u5ea6\u4e3a $ O(d^2) \\(\uff08\\) d $ \u4e3a\u6570\u636e\u7ef4\u5ea6\uff09\u3002 \u6838\u5fc3\u601d\u60f3\uff1a\u5229\u7528\u968f\u673a\u6295\u5f71\u6280\u672f\u5c06\u8ff9\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u81f3 $ O(d) $\u3002</p> <p>4. \u968f\u673a\u6295\u5f71\u8fd1\u4f3c\u8ff9</p> <p>\u6839\u636e Hutchinson \u8ff9\u4f30\u8ba1\u5668\uff0c\u4efb\u610f\u77e9\u9635 $ A \\in \\mathbb{R}^{d \\times d} $ \u7684\u8ff9\u53ef\u8868\u793a\u4e3a\uff1a</p> \\[ \\operatorname{Tr}(A) = \\mathbb{E}_{\\mathbf{v} \\sim \\mathcal{N}(0, I)} \\left[ \\mathbf{v}^\\top A \\mathbf{v} \\right], \\] <p>\u5176\u4e2d $ \\mathbf{v} $ \u4e3a\u6807\u51c6\u6b63\u6001\u5206\u5e03\u7684\u968f\u673a\u5411\u91cf\u3002 \u5c06\u6b64\u6280\u672f\u5e94\u7528\u4e8e ISM \u7684\u8ff9\u9879\uff1a</p> \\[ \\operatorname{Tr}(\\nabla_x \\mathbf{s}_\\theta(x)) = \\mathbb{E}_{\\mathbf{v}} \\left[ \\mathbf{v}^\\top \\nabla_x \\mathbf{s}_\\theta(x) \\mathbf{v} \\right]. \\] <p>5. Sliced Score Matching \u7684\u76ee\u6807\u51fd\u6570</p> <p>\u5c06 ISM \u7684\u8ff9\u9879\u66ff\u6362\u4e3a\u968f\u673a\u6295\u5f71\u8fd1\u4f3c\uff0c\u5f97\u5230 SSM \u7684\u76ee\u6807\u51fd\u6570\uff1a</p> \\[ J_{\\text{SSM}}(\\theta) = \\mathbb{E}_{p(x)} \\mathbb{E}_{\\mathbf{v}} \\left[ \\mathbf{v}^\\top \\nabla_x \\mathbf{s}_\\theta(x) \\mathbf{v} + \\frac{1}{2} \\| \\mathbf{s}_\\theta(x) \\|^2 \\right]. \\] <p>\u8499\u7279\u5361\u6d1b\u8fd1\u4f3c\uff1a\u901a\u8fc7\u91c7\u6837\u5c11\u91cf\u6295\u5f71\u65b9\u5411 $ {\\mathbf{v}k}{k=1}^K $ \u4f30\u8ba1\u671f\u671b\u503c\uff1a</p> \\[ J_{\\text{SSM}}(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\left[ \\frac{1}{K} \\sum_{k=1}^K \\mathbf{v}_k^\\top \\nabla_x \\mathbf{s}_\\theta(x_i) \\mathbf{v}_k + \\frac{1}{2} \\| \\mathbf{s}_\\theta(x_i) \\|^2 \\right]. \\] <p>6. \u7b49\u4ef7\u6027\u8bc1\u660e</p> <p>\u5f53\u6295\u5f71\u65b9\u5411\u6570\u91cf $ K \\to \\infty $ \u65f6\uff0cSSM \u4e0e ISM \u7684\u76ee\u6807\u51fd\u6570\u7b49\u4ef7\uff1a $$ \\lim_{K \\to \\infty} J_{\\text{SSM}}(\\theta) = J_{\\text{ISM}}(\\theta). $$ \u56e0\u6b64\uff0cSSM \u662f ISM \u7684\u9ad8\u6548\u8fd1\u4f3c\uff0c\u4e14\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7b49\u4ef7\u4e8e\u6700\u5c0f\u5316 Fisher \u6563\u5ea6\u3002</p> <p>7. \u603b\u7ed3</p> <ul> <li>\u4ece Fisher \u6563\u5ea6\u5230 SSM \u7684\u63a8\u5bfc\u8def\u5f84\uff1a   $$   \\mathcal{D}{\\text{Fisher}} \\xrightarrow{\\text{Stein \u6052\u7b49\u5f0f}} J{\\text{ISM}} \\xrightarrow{\\text{\u968f\u673a\u6295\u5f71\u8fd1\u4f3c}} J_{\\text{SSM}}.   $$</li> <li>SSM \u7684\u4f18\u52bf\uff1a   \u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece $ O(d^2) $ \u964d\u4f4e\u81f3 $ O(Kd) $\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\uff08\u5982\u56fe\u50cf\u3001\u6587\u672c\uff09\u3002</li> <li> <p>\u9002\u7528\u573a\u666f\uff1a   \u751f\u6210\u6a21\u578b\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u3001\u65e0\u9700\u663e\u5f0f\u6982\u7387\u5bc6\u5ea6\u7684\u68af\u5ea6\u4f30\u8ba1\u3002</p> </li> <li> <p>\u57fa\u7840\u516c\u5f0f    \u4e0e\u4f20\u7edfImplicit Score Matching\u7684\u635f\u5931\u51fd\u6570\u7c7b\u4f3c\uff0c\u4f46\u5f15\u5165\u968f\u673a\u6295\u5f71\u5411\u91cf $ \\mathbf{v} $\uff1a    $$    J_{\\text{SSM}}(\\theta) = \\mathbb{E}{p(\\mathbf{x})} \\mathbb{E}{\\mathbf{v} \\sim \\mathcal{N}(0,I)} \\left[ \\mathbf{v}^\\top \\nabla_{\\mathbf{x}} \\mathbf{s}\\theta(\\mathbf{x}) \\mathbf{v} + \\frac{1}{2} | \\mathbf{s}\\theta(\\mathbf{x}) |^2 \\right],    $$    \u5176\u4e2d\uff1a</p> </li> <li>$ \\mathbf{v} $ \u662f\u670d\u4ece\u6807\u51c6\u6b63\u6001\u5206\u5e03\u7684\u968f\u673a\u5411\u91cf\u3002</li> <li> <p>$ \\mathbf{v}^\\top \\nabla_{\\mathbf{x}} \\mathbf{s}_\\theta(\\mathbf{x}) \\mathbf{v} $ \u8868\u793aJacobian\u77e9\u9635\u5728\u968f\u673a\u65b9\u5411 $ \\mathbf{v} $ \u4e0a\u7684\u6295\u5f71\u3002</p> </li> <li> <p>\u5173\u952e\u7b80\u5316    \u901a\u8fc7\u968f\u673a\u6295\u5f71\uff0c\u5c06\u8ba1\u7b97\u5168Jacobian\u77e9\u9635\u7684\u8ff9 $ \\operatorname{Tr}(\\nabla_{\\mathbf{x}}\\mathbf{s}_\\theta(\\mathbf{x})) $ \u8f6c\u5316\u4e3a\uff1a</p> </li> </ul> <p>$$    \\operatorname{Tr}(\\nabla_{\\mathbf{x}}\\mathbf{s}\\theta(\\mathbf{x})) = \\mathbb{E}{\\mathbf{v}} \\left[ \\mathbf{v}^\\top \\nabla_{\\mathbf{x}} \\mathbf{s}_\\theta(\\mathbf{x}) \\mathbf{v} \\right].    $$</p> <p>\u8fd9\u4e00\u6280\u5de7\u5c06\u590d\u6742\u5ea6\u4ece $ O(d^2) $\uff08\u76f4\u63a5\u8ba1\u7b97Jacobian\uff09\u964d\u4f4e\u5230 $ O(d) $\uff08\u4ec5\u9700\u8ba1\u7b97\u5411\u91cf-\u77e9\u9635-\u5411\u91cf\u4e58\u79ef\uff09\u3002</p>"},{"location":"book/chapter6_ebm/score_matching/#_9","title":"\u4f18\u52bf\u4e0e\u610f\u4e49","text":"<ol> <li>\u8ba1\u7b97\u9ad8\u6548\u6027</li> <li>\u907f\u514d\u663e\u5f0f\u8ba1\u7b97\u9ad8\u7ef4Jacobian\u77e9\u9635\uff0c\u5c24\u5176\u9002\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7b49\u53c2\u6570\u5316\u6a21\u578b\u3002</li> <li> <p>\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u91c7\u6837\u5c11\u91cf\u6295\u5f71\u65b9\u5411 $ \\mathbf{v} $\uff0c\u5373\u53ef\u8fd1\u4f3c\u771f\u5b9e\u8ff9\u503c\u3002</p> </li> <li> <p>\u7406\u8bba\u4fdd\u8bc1</p> </li> <li>\u5728\u6295\u5f71\u65b9\u5411\u8db3\u591f\u591a\u65f6\uff0c\u635f\u5931\u51fd\u6570\u4e0eImplicit Score Matching\u7b49\u4ef7\u3002</li> <li> <p>\u4fdd\u7559\u4e86\u68af\u5ea6\u5339\u914d\u7684\u4e00\u81f4\u6027\uff0c\u5373\u6700\u5c0f\u5316\u635f\u5931\u7b49\u4ef7\u4e8e\u5339\u914d\u771f\u5b9escore function\u3002</p> </li> <li> <p>\u9002\u7528\u573a\u666f</p> </li> <li>\u9ad8\u7ef4\u6570\u636e\uff08\u5982\u56fe\u50cf\u3001\u6587\u672c\uff09\u3002</li> <li>\u751f\u6210\u6a21\u578b\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u3001\u5bc6\u5ea6\u4f30\u8ba1\u4efb\u52a1\u3002</li> </ol>"},{"location":"book/chapter6_ebm/score_matching/#_10","title":"\u793a\u4f8b\u8bf4\u660e","text":"<p>\u5047\u8bbe\u6570\u636e\u7ef4\u5ea6 $ d=1000 $\uff0c\u76f4\u63a5\u8ba1\u7b97Jacobian\u77e9\u9635\u7684\u8ff9\u9700\u8981 $ 10^6 $ \u6b21\u64cd\u4f5c\uff0c\u800cSliced Score Matching\u4ec5\u9700\uff1a 1. \u91c7\u6837\u968f\u673a\u5411\u91cf $ \\mathbf{v} \\in \\mathbb{R}^{1000} $\u3002 2. \u8ba1\u7b97 $ \\mathbf{v}^\\top \\nabla_{\\mathbf{x}} \\mathbf{s}_\\theta(\\mathbf{x}) \\mathbf{v} $\uff0c\u4ec5\u9700 $ O(1000) $ \u6b21\u64cd\u4f5c\u3002 3. \u5bf9\u591a\u4e2a $ \\mathbf{v} $ \u53d6\u5e73\u5747\uff0c\u8fd1\u4f3c\u8ff9\u503c\u3002</p>"},{"location":"book/chapter6_ebm/score_matching/#_11","title":"\u5bf9\u6bd4\u5176\u4ed6\u65b9\u6cd5","text":"\u65b9\u6cd5 \u8ba1\u7b97\u590d\u6742\u5ea6 \u4e3b\u8981\u6311\u6218 Explicit Score Matching $ O(d) $ \u4f9d\u8d56\u771f\u5b9escore function\uff0c\u901a\u5e38\u672a\u77e5 Implicit Score Matching $ O(d^2) $ \u8ba1\u7b97Jacobian\u8ff9\u7684\u9ad8\u590d\u6742\u5ea6 Sliced Score Matching $ O(d) $ \u9700\u5e73\u8861\u6295\u5f71\u65b9\u5411\u6570\u91cf\u4e0e\u4f30\u8ba1\u7cbe\u5ea6"},{"location":"book/chapter6_ebm/score_matching/#_12","title":"\u4ee3\u7801\u5b9e\u73b0\uff08\u4f2a\u4ee3\u7801\uff09","text":"<pre><code>import torch\n\ndef sliced_score_matching_loss(model, data, num_projections=10):\n    \"\"\"\n    model: \u9884\u6d4bscore function\u7684\u795e\u7ecf\u7f51\u7edc\n    data: \u8f93\u5165\u6570\u636e\u6837\u672c\n    num_projections: \u968f\u673a\u6295\u5f71\u65b9\u5411\u7684\u6570\u91cf\n    \"\"\"\n    scores = model(data)  # \u6a21\u578b\u9884\u6d4b\u7684score [batch_size, d]\n    loss = 0.5 * torch.mean(torch.sum(scores**2, dim=1))  # 1/2 ||s_\u03b8(x)||^2\n\n    for _ in range(num_projections):\n        v = torch.randn_like(data)  # \u968f\u673a\u6295\u5f71\u5411\u91cf [batch_size, d]\n        v.requires_grad_(True)\n\n        # \u8ba1\u7b97 v^T \u2207s_\u03b8(x) v\n        v_scores = torch.sum(v * scores, dim=1)  # [batch_size]\n        jvp = torch.autograd.grad(\n            outputs=v_scores, inputs=data,\n            grad_outputs=torch.ones_like(v_scores),\n            create_graph=True\n        )[0]  # \u2207(v^T s_\u03b8(x)) = v^T \u2207s_\u03b8(x)\n        trace_estimate = torch.sum(v * jvp, dim=1)  # v^T \u2207s_\u03b8(x) v\n\n        loss += torch.mean(trace_estimate) / num_projections\n\n    return loss\n</code></pre>"},{"location":"book/chapter6_ebm/score_matching/#experiment_1","title":"experiment","text":"<p>We also tried with the mixture og gaussian distribution, it performs well.</p> <p></p> <p>See notebook on  experiment/implicit_score_matching.ipynb.</p>"},{"location":"book/chapter6_ebm/score_matching/#denoising-score-matching","title":"Denoising Score Matching","text":"<p>Generative models aim to learn the underlying distribution of data, allowing us to generate new samples. Denoising Score Matching (DSM) is a powerful technique in this domain, leveraging noise to simplify training. In this blog, we'll break down DSM's intuition, math, training, and sampling, and extend it to multi-scale and continuous noise settings. We\u2019ll emphasize why each step matters and how the pieces connect.</p>"},{"location":"book/chapter6_ebm/score_matching/#1-intuition-why-add-noise","title":"1. Intuition: Why Add Noise?","text":""},{"location":"book/chapter6_ebm/score_matching/#what-is-a-score","title":"What is a \"Score\"?","text":"<p>The score of a probability distribution $ p(x) $ is its gradient of the log-density: $$ \\nabla_x \\log p(x) $$ Imagine you\u2019re hiking on a landscape where valleys represent regions of high data density (e.g., realistic images). The score tells you the direction to move uphill toward higher density (i.e., toward realistic data).</p>"},{"location":"book/chapter6_ebm/score_matching/#why-traditional-score-matching-fails","title":"Why Traditional Score Matching Fails","text":"<p>For high-dimensional data (e.g., images), most of the space is empty\u2014data lies on a thin \"manifold.\" Traditional score matching struggles because: 1. Computational cost: Estimating gradients in high dimensions is expensive. 2. Sparse signals: The score is undefined or noisy in empty regions far from the data manifold.</p>"},{"location":"book/chapter6_ebm/score_matching/#the-noise-solution","title":"The Noise Solution","text":"<p>By adding Gaussian noise to data points, we \"smooth\" the distribution, filling empty regions with a blurry haze of noisy data. This makes the score easier to estimate everywhere. Think of it like turning a spiky mountain range into rolling hills\u2014easier to navigate!</p> <p>Key Insight: Instead of learning $ \\nabla_x \\log p(x) $ directly (hard!), learn to denoise perturbed data. The denoising direction aligns with the score of the noise-augmented distribution.</p>"},{"location":"book/chapter6_ebm/score_matching/#2-derivation-connecting-noise-to-the-score","title":"2. Derivation: Connecting Noise to the Score","text":""},{"location":"book/chapter6_ebm/score_matching/#step-1-define-the-noisy-distribution","title":"Step 1: Define the Noisy Distribution","text":"<p>Corrupt a data point $ x $ with Gaussian noise:</p> \\[ \\tilde{x} = x + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I) \\] <p>The conditional distribution is:</p> \\[ q(\\tilde{x}|x) = \\mathcal{N}(x, \\sigma^2 I) \\] <p>thus,</p> \\[ q_\\sigma(\\tilde{\\mathbf{x}}) = \\int q(\\tilde{\\mathbf{x}} \\mid \\mathbf{x}) p_{\\text{data}}(\\mathbf{x}) \\, d\\mathbf{x}, \\] <p>where $ q(\\tilde{\\mathbf{x}} \\mid \\mathbf{x}) = \\mathcal{N}(\\tilde{\\mathbf{x}}; \\mathbf{x}, \\sigma^2 \\mathbf{I}) $ is a Gaussian distribution centered at $ \\mathbf{x} $ with variance $ \\sigma^2 $. This represents a convolution of $ p_{\\text{data}}(\\mathbf{x}) $ with a Gaussian kernel.</p>"},{"location":"book/chapter6_ebm/score_matching/#1-small-noise-sigma-to-0","title":"1. Small Noise $ \\sigma \\to 0 $","text":"<ul> <li>Behavior of $ q(\\tilde{\\mathbf{x}} \\mid \\mathbf{x}) $:</li> </ul> <p>The Gaussian becomes a Dirac delta function:</p> <p>$$   q_\\sigma(\\tilde{\\mathbf{x}} \\mid \\mathbf{x}) \\to \\delta(\\tilde{\\mathbf{x}} - \\mathbf{x}).   $$</p> <ul> <li>Effect on $q(\\tilde{\\mathbf{x}}) $:</li> </ul> <p>The integral simplifies to $p_{\\text{data}}(\\tilde{\\mathbf{x}}) $, preserving the original distribution:</p> <p>$$   q_\\sigma(\\tilde{\\mathbf{x}}) \\approx p_{\\text{data}}(\\tilde{\\mathbf{x}}).   $$ - Interpretation: Minimal blurring; the perturbed distribution matches the original data distribution.</p>"},{"location":"book/chapter6_ebm/score_matching/#2-moderate-noise-sigma-0","title":"2. Moderate Noise $ \\sigma &gt; 0 $","text":"<ul> <li> <p>Behavior of $q(\\tilde{\\mathbf{x}} \\mid \\mathbf{x}) $:   The Gaussian acts as a smoothing kernel with width proportional to $ \\sigma $.</p> </li> <li> <p>Effect on $q(\\tilde{\\mathbf{x}}) $:</p> </li> </ul> <p>The convolution introduces controlled blurring, creating a smoothed version of $ p_{\\text{data}}(\\mathbf{x}) $. Fine details are averaged, but the global structure remains recognizable.</p> <ul> <li>Interpretation: Useful for regularization or generating \"softened\" data samples.</li> </ul>"},{"location":"book/chapter6_ebm/score_matching/#3-large-noise-sigma-to-infty","title":"3. Large Noise $\\sigma \\to \\infty $**","text":"<ul> <li> <p>Behavior of $ q_\\sigma(\\tilde{\\mathbf{x}} \\mid \\mathbf{x}) $:   The Gaussian becomes extremely wide and flat, approximating a uniform distribution over the domain.</p> </li> <li> <p>Effect on $ q_\\sigma(\\tilde{\\mathbf{x}}) $:</p> </li> </ul> <p>The integral averages $ p_{\\text{data}}(\\mathbf{x}) $ over a large region, erasing fine structure. If $ p_{\\text{data}}(\\mathbf{x}) $ is bounded, $ q(\\tilde{\\mathbf{x}}) $ approaches a uniform distribution; otherwise, it becomes a broad Gaussian.</p> <ul> <li>Interpretation: Severe distortion; the original distribution is lost.</li> </ul>"},{"location":"book/chapter6_ebm/score_matching/#step-2-score-of-the-noisy-distribution","title":"Step 2: Score of the Noisy Distribution","text":"<p>The score of $ q(\\tilde{x}|x) $ is:</p> \\[ \\nabla_{\\tilde{x}} \\log q(\\tilde{x}|x) = \\frac{x - \\tilde{x}}{\\sigma^2} \\] <p>Why? For a Gaussian $ \\mathcal{N}(x, \\sigma^2 I) $, the gradient of the log-density with respect to $ \\tilde{x} $ points toward the mean $ x $. The term $ (x - \\tilde{x})/\\sigma^2 $ is the \"denoising direction\" that corrects $ \\tilde{x} $ back to $ x $.</p>"},{"location":"book/chapter6_ebm/score_matching/#step-3-the-dsm-objective","title":"Step 3: The DSM Objective","text":"<p>Train a model $ s_\\theta(\\tilde{x}) $ to match this score:</p> \\[ J(\\theta) = \\mathbb{E}_{q(\\tilde{x},x)}\\left[ \\| s_\\theta(\\tilde{x}) - \\frac{x - \\tilde{x}}{\\sigma^2} \\|^2 \\right] \\] <p>Why This Works:</p> <p>Minimizing this loss forces $ s_\\theta(\\tilde{x}) $ to approximate $ \\nabla_{\\tilde{x}} \\log q(\\tilde{x}) $, the score of the marginal noisy distribution $ q(\\tilde{x}) = \\int q(\\tilde{x}|x)p_{\\text{data}}(x)dx $,  which has been proved in the above section. As illustrated above, this is equivalent to learning the score of the true data distribution $ p_{\\text{data}}(x) $ as $ \\sigma \\to 0 $.</p>"},{"location":"book/chapter6_ebm/score_matching/#3-training-process-step-by-step","title":"3. Training Process: Step-by-Step","text":""},{"location":"book/chapter6_ebm/score_matching/#step-1-add-noise-to-data","title":"Step 1: Add Noise to Data","text":"<p>For each clean data point $ x $, generate a noisy version:</p> \\[ \\tilde{x} = x + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I) \\] <p>Why Add Noise? - Creates \"easier\" training examples by spreading data into empty regions. - Teaches the model to handle perturbations, improving robustness.</p>"},{"location":"book/chapter6_ebm/score_matching/#step-2-compute-the-loss","title":"Step 2: Compute the Loss","text":"<p>The DSM loss simplifies to:</p> \\[ \\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^N \\| s_\\theta(\\tilde{x}_i) - \\frac{x_i - \\tilde{x}_i}{\\sigma^2} \\|^2 \\] <p>Interpretation:</p> <p>The model learns to predict the vector $ (x_i - \\tilde{x}_i)/\\sigma^2 $, which points from the noisy sample $ \\tilde{x}_i $ back to the clean $ x_i $. This is equivalent to estimating the score of the noisy distribution.</p>"},{"location":"book/chapter6_ebm/score_matching/#step-3-gradient-descent","title":"Step 3: Gradient Descent","text":"<p>Update model parameters $ \\theta $ to minimize $ \\mathcal{L} $.</p> <p></p> <p>Practical Tip:</p> <p>Scale the loss by $ \\sigma^2 $ to balance learning across noise levels (critical for multi-scale training).</p> <p>\u591a\u5c3a\u5ea6 sigmas (\u03c3 schedules)</p> <ul> <li> <p>\u4e3a\u4e86\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u53ef\u4ee5\u4e3a\u566a\u58f0\u8bbe\u7f6e\u4e00\u7cfb\u5217\u503c\uff0c\u4ece\u8f83\u5c0f\u7684\u566a\u58f0\u5230\u8f83\u5927\u7684\u566a\u58f0\uff0c\u5f62\u6210\u4e00\u4e2a\u566a\u58f0\u5c3a\u5ea6 \u3002</p> </li> <li> <p>\u516c\u5f0f : \u901a\u5e38\u6784\u9020\u4e00\u4e2a\u5bf9\u6570\u95f4\u9694\u7684\u5e8f\u5217\uff0c\u4f8b\u5982\uff1a</p> </li> </ul> \\[ \\sigma_i = \\sigma_{\\text{min}} \\times \\left(\\frac{\\sigma_{\\text{max}}}{\\sigma_{\\text{min}}}\\right)^{i / (N-1)} \\] <pre><code>- `\u03c3_min`: \u6700\u5c0f\u566a\u58f0\u5f3a\u5ea6\u3002\n\n- `\u03c3_max`: \u6700\u5927\u566a\u58f0\u5f3a\u5ea6\u3002\n\n- `N`: \u566a\u58f0\u5c3a\u5ea6\u7684\u6570\u91cf\u3002\n</code></pre> <ul> <li> <p>\u5e38\u7528\u8bbe\u7f6e :</p> <ul> <li> <p>\u5bf9\u4e8e\u56fe\u50cf\u6570\u636e\uff0c<code>\u03c3_min</code> \u901a\u5e38\u662f <code>0.01</code> \u6216 <code>0.05</code>\u3002</p> </li> <li> <p><code>\u03c3_max</code> \u901a\u5e38\u53d6 <code>0.5</code> \u6216 <code>1.0</code>\u3002</p> </li> <li> <p><code>N</code> \u7684\u5178\u578b\u503c\u4e3a <code>10</code> \u5230 <code>50</code>\u3002</p> </li> </ul> </li> <li> <p>\u4f18\u70b9 : \u591a\u5c3a\u5ea6\u566a\u58f0\u66f4\u9002\u5408\u590d\u6742\u5206\u5e03\uff0c\u53ef\u4ee5\u5efa\u6a21\u4ece\u4f4e\u566a\u58f0\u5230\u9ad8\u566a\u58f0\u7684\u5404\u79cd\u60c5\u51b5\u3002</p> </li> <li> <p>\u7f3a\u70b9 : \u8bad\u7ec3\u548c\u5b9e\u73b0\u66f4\u590d\u6742\uff0c\u9700\u8981\u52a8\u6001\u8c03\u6574\u566a\u58f0\u6c34\u5e73\u3002</p> </li> </ul>"},{"location":"book/chapter6_ebm/score_matching/#4-sampling-with-langevin-dynamics","title":"4. Sampling with Langevin Dynamics","text":"<p>Once trained, we use Langevin dynamics to generate samples by \"walking\" along the learned score.</p>"},{"location":"book/chapter6_ebm/score_matching/#the-update-rule","title":"The Update Rule:","text":"\\[ x_{t+1} = x_t + \\epsilon \\cdot s_\\theta(x_t) + \\sqrt{2\\epsilon} \\cdot z_t, \\quad z_t \\sim \\mathcal{N}(0, I) \\] <p>Breaking It Down:</p> <ol> <li>Score term $ \\epsilon \\cdot s_\\theta(x_t) $: Guides $ x_t $ toward high-density regions (denoising).</li> <li>Noise term $ \\sqrt{2\\epsilon} \\cdot z_t $: Adds randomness to escape local minima and explore the distribution.</li> </ol> <p>Why This Works: Langevin dynamics is a Markov Chain Monte Carlo (MCMC) method that uses the score to perform gradient ascent on $ \\log p(x) $. The noise ensures ergodicity, allowing the chain to converge to the true distribution.</p>"},{"location":"book/chapter6_ebm/score_matching/#analogy","title":"Analogy:","text":"<p>Imagine rolling a marble on a bumpy surface (the data landscape). The score tilts the surface to guide the marble toward valleys (data points), while the noise gives it occasional kicks to explore new areas.</p>"},{"location":"book/chapter6_ebm/score_matching/#5-multi-scale-noise-training","title":"5. Multi-Scale Noise Training","text":""},{"location":"book/chapter6_ebm/score_matching/#why-multiple-noise-scales","title":"Why Multiple Noise Scales?","text":"<p>Key Challenges in Training SBMs/EBMs</p> <ul> <li>Undefined Score Function Off the Data Manifold : Under the manifold hypothesis, data resides on a low-dimensional manifold embedded in a high-dimensional ambient space. The score function $ \\nabla_x \\log p(x) $, which requires gradients to be defined everywhere in the ambient space, becomes ill-defined outside the manifold.</li> </ul> <p>Problem: Score estimation fails in regions irrelevant to the data, destabilizing training and generation.</p> <p>Implication: Score estimation fails in regions irrelevant to the data, destabilizing training and generation.</p> <ul> <li>Sparse Data in Low-Density Regions</li> </ul> <p>Problem: Real-world datasets often lack sufficient samples in low-density areas (e.g., transitions between classes or rare features). This sparsity makes it difficult to reliably estimate the score function in these regions.</p> <p>Implication: Poor score approximation leads to artifacts, mode collapse, or unrealistic interpolations.</p> <ul> <li>Degradation of Mixing Distribution Coefficients</li> </ul> <p>Problem: In near-zero density regions (e.g., far from the manifold), the coefficients (weights) of the mixing distribution\u2014used to model complex data\u2014vanish or become negligible.</p> <p>Implication: The model loses expressive power in these regions, exacerbating mode collapse and limiting diversity in generated samples.</p> <p>We use multi-scale noise pertubation could help address these challenges.</p> <p>Real-world data (e.g., images) has structure at multiple resolutions: - Low noise (small $ \\sigma $): Captures fine details (e.g., textures). - High noise (large $ \\sigma $): Captures coarse structure (e.g., shapes).</p> <p>Training with a single $ \\sigma $ limits the model\u2019s ability to generalize across scales.</p>"},{"location":"book/chapter6_ebm/score_matching/#training-process","title":"Training Process:","text":"<ol> <li>Noise Sampling: For each batch, randomly pick $ \\sigma_i $ from a set $ {\\sigma_1, ..., \\sigma_L} $.</li> <li>Loss Adjustment: Scale the loss by $ \\sigma_i^2 $ to prevent larger $ \\sigma $ from dominating:</li> </ol> <p>$$ \\mathcal{L} = \\frac{1}{L} \\sum_{i=1}^L \\mathbb{E}\\left[ \\sigma_i^2 | s_\\theta(\\tilde{x}, \\sigma_i) - \\frac{x - \\tilde{x}}{\\sigma_i^2} |^2 \\right] $$</p>"},{"location":"book/chapter6_ebm/score_matching/#sampling","title":"Sampling:","text":"<p>Use a decreasing sequence $ \\sigma_1 &gt; \\sigma_2 &gt; ... &gt; \\sigma_L $ during Langevin dynamics:</p> <ol> <li>Start with high noise to capture coarse structure.</li> <li>Gradually reduce noise to refine details.</li> </ol> <p>Analogy: Like sketching a painting\u2014first outline shapes (high noise), then add details (low noise).</p>"},{"location":"book/chapter6_ebm/score_matching/#6-continuous-noise-levels","title":"6. Continuous Noise Levels","text":""},{"location":"book/chapter6_ebm/score_matching/#why-go-continuous","title":"Why Go Continuous?","text":"<p>Discrete noise scales are rigid and computationally costly for large $ L $. A continuous approach: - Smoothly interpolates between noise levels. - Connects to differential equations for efficient sampling.</p>"},{"location":"book/chapter6_ebm/score_matching/#training","title":"Training:","text":"<ol> <li>Noise Sampling: Sample $ t \\sim \\mathcal{U}(0,1) $, compute $ \\sigma(t) $ (e.g., $ \\sigma(t) = \\sigma_{\\text{min}} + t(\\sigma_{\\text{max}} - \\sigma_{\\text{min}}) $).</li> <li>Condition the Model: Feed $ t $ to $ s_\\theta $ via time embeddings (e.g., sinusoidal features).</li> </ol>"},{"location":"book/chapter6_ebm/score_matching/#sampling-with-stochastic-differential-equations-sdes","title":"Sampling with Stochastic Differential Equations (SDEs)","text":"<p>The continuous noise process can be described as an SDE:</p> \\[ dx = s_\\theta(x, t) dt + \\sqrt{2\\sigma(t)} dw \\] <p>Intuition: This generalizes Langevin dynamics to infinitesimal steps. The term $ s_\\theta(x,t)dt $ is the deterministic drift (denoising), and $ \\sqrt{2\\sigma(t)}dw $ is stochastic diffusion (noise).</p> <p>Solving the SDE: Use numerical solvers like Euler-Maruyama:</p> \\[ x_{t+1} = x_t + s_\\theta(x_t, t) \\Delta t + \\sqrt{2\\sigma(t) \\Delta t} \\, z_t \\] <p>This is equivalent to Langevin dynamics with time-dependent noise.</p>"},{"location":"book/chapter6_ebm/score_matching/#7-why-does-dsm-work-a-unified-view","title":"7. Why Does DSM Work? A Unified View","text":"<ol> <li>Avoids Density Estimation: Models gradients instead of $ p(x) $, bypassing normalization constraints.</li> <li>Exploits Noise: Smoothens the data manifold, making score estimation tractable.</li> <li>Connects to Diffusion Models: DSM is the backbone of diffusion models (e.g., DDPM, Score-SDE), where noise addition/removal is formalized across timesteps.</li> </ol> <p>Comparison to GANs/VAEs: - No adversarial training: More stable than GANs. - No latent space bottlenecks: Richer expressivity than VAEs.</p>"},{"location":"book/chapter6_ebm/score_matching/#conclusion","title":"Conclusion","text":"<p>Denoising Score Matching elegantly bridges noise and geometry to learn data distributions. By progressively corrupting and denoising data, it captures multi-scale structure and enables efficient sampling via SDEs. This framework powers cutting-edge generative models, offering flexibility, stability, and scalability.</p> <p>Further Reading: - Score Matching (Hyv\u00e4rinen, 2005) - Denoising Score Matching (Vincent, 2011) - Score-Based SDEs (Song et al., 2021) - Generative Modeling by Estimating Gradients of the Data Distribution (YangSong, 2019)</p>"}]}